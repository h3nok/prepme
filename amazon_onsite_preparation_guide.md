# Amazon Applied Scientist Onâ€‘Site InterviewÂ Preparation Guide

---

## Executive Summary

Amazonâ€™s Applied Scientist loop blends rigorous machineâ€‘learning system design, practical coding drills, and deep alignment with the companyâ€™s 16 Leadership Principles (LPs). Expect fiveÂ oneâ€‘hour sessionsâ€”two coding, one or two ML systemâ€‘design/technical depth, and one or two LPâ€‘heavy rounds that interleave behavioural and technical probes. In most loops behavioural content consumes **40â€‘50â€¯%** of each interview, with the Barâ€‘Raiser pushing aggressively on customer impact and metricsÂ ([igotanoffer.com](https://igotanoffer.com/en/advice/amazon-applied-scientist-interview?utm_source=chatgpt.com))Â ([reddit.com](https://www.reddit.com/r/leetcode/comments/1jwond7/amazon_applied_scientist_interview_experience/?utm_source=chatgpt.com)).  This guide arms you with endâ€‘toâ€‘end preparation material: detailed loop anatomy, exemplar LP stories, fullyâ€‘worked coding solutions (PyTorchâ€‘autograd allowed only), ML breadth flash cards with answers, and two readyâ€‘toâ€‘speak systemâ€‘design templates (Fraud Detection and ComputerÂ Vision).  A 14â€‘day power study plan and curated resource list round things out.

---

## 1Â Â Interview Loop Anatomy

| Round                        | Typical Focus                                                       | Success Signals                                                      |
| ---------------------------- | ------------------------------------------------------------------- | -------------------------------------------------------------------- |
| **CodingÂ #1**                | Classic DS&A plus clean Python                                      | O(1) space, clear narration, edgeâ€‘cases covered                      |
| **CodingÂ #2 (MLÂ coding)**    | Implement an optimizer, loss, or small model from scratch           | Correct math, unitâ€‘tested code, timeâ€‘boxedÂ <Â 25Â min                  |
| **MLÂ System Design / Depth** | Endâ€‘toâ€‘end design for prodâ€‘scale model (fraud, recommender, CV/NLP) | Clear KPI framing, multiple candidate architectures, monitoring plan |
| **Research / MLÂ Breadth**    | Rapidâ€‘fire fundamentals + practical tradeâ€‘offs                      | Concise derivations, knowing *when* to apply which model             |
| **Barâ€‘Raiser (LPâ€¯focus)**    | Aggressive LP probing woven into tech discussion                    | Quantified impact, reflection, learningsÂ                             |

Interviews are calibrated against Amazonâ€™sÂ LPsÂ ([amazon.jobs](https://www.amazon.jobs/content/our-workplace/leadership-principles?utm_source=chatgpt.com)), and candidates who cannot anchor technical decisions in customer or business metrics rarely clear the barâ€‘raiserÂ ([igotanoffer.com](https://igotanoffer.com/blogs/tech/amazon-bar-raiser-interview?utm_source=chatgpt.com)).

---

## 2Â Â Leadership Principles Cheatâ€‘Sheet

Below are compact STAR prompts you can rehearse.  For each LP prepare **three** distinct stories; rotate fresh examples if an interviewer reâ€‘asks.

| LP                       | STAR Hook                                                                        | Metric                                                                                                                                                                              |
| ------------------------ | -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Customer Obsession       | Detected silent dataâ€‘drift that inflated falseÂ positives; shipped hotâ€‘fix inÂ 4â€¯h | âˆ’20â€¯% FP, +\$1.2â€¯M revenueÂ protect                                                                                                                                                  |
| InventÂ &Â Simplify        | Reâ€‘engineered rules engine into single GNN microâ€‘service                         | 10Ã— latency cut                                                                                                                                                                     |
| DiveÂ Deep                | Latency spike traced to coldÂ start in SageMaker; implemented warmÂ pool           | 55â€¯msÂ â†’Â 22â€¯ms p95Â ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/rad-ai-reduces-real-time-inference-latency-by-50-using-amazon-sagemaker/?utm_source=chatgpt.com)) |
| AreÂ Right,Â AÂ Lot         | Predicted recall drop due to holiday skew; A/B confirmed                         | +8â€¯pp recall                                                                                                                                                                        |
| InsistÂ onÂ HighÂ Standards | Reâ€‘wrote feature pipeline tests; 0 failed deploys in Q4                          | 15Â â†’Â 0 regression bugs                                                                                                                                                              |
| EarnÂ Trust               | Published explanation dashboard for Risk team                                    | NPSÂ â†‘Â +28                                                                                                                                                                           |
| LearnÂ &Â BeÂ Curious       | Completed AWS CertifiedÂ MLOps inÂ 6Â weeks                                         | new infra PoC                                                                                                                                                                       |

Use precise numbers; barâ€‘raisers value quantified outcomesÂ ([igotanoffer.com](https://igotanoffer.com/blogs/tech/amazon-bar-raiser-interview?utm_source=chatgpt.com)).

---

## 3Â Â MLÂ Codingâ€”Fullyâ€‘Worked Solutions

### 3.0Â Â CodingÂ #1Â â€” DS&AÂ Classics

Amazonâ€™s first coding round typically focuses on **general dataâ€‘structures & algorithms problems** (arrays, linked lists, hash maps, trees/graphs, intervals, DP).  Solutions are expected to be *spaceâ€‘optimal* (often O(1)) and narrated clearly.  Review the patterns and sample implementations below before moving to the MLâ€‘heavy round.

#### 3.0.1Â Â Recurring Patterns  ([Medium](https://medium.com/swlh/how-to-study-for-data-structures-and-algorithms-interviews-at-faang-65043e00b5df))

- Twoâ€‘Pointers / Slidingâ€‘Window â€” arrays & strings.
- HashÂ Map for O(1) lookâ€‘ups.
- Stack for balancedâ€‘parentheses / monotonicâ€‘stack.
- BinaryÂ Search on sorted or answerâ€‘space.
- DFS/BFS for trees, graphs, matrices.
- Dynamic Programming (1â€‘D / 2â€‘D table, rolling O(1) space).

#### 3.0.2Â Â Highâ€‘Yield Questions  (curated from Glassdoor/LeetCode top Amazon listÂ ([LeetCode](https://leetcode.com/problem-list/7p5x763/))Â ([DesignGurus](https://www.designgurus.io/blog/amazon-14-question)))

| # | Problem                        | Pattern           | Edge Cases                       | Goal Space                                                                         | PythonÂ Sketch          |
| - | ------------------------------ | ----------------- | -------------------------------- | ---------------------------------------------------------------------------------- | ---------------------- |
| 1 | TwoÂ Sum                        | HashÂ Map          | negative nums; duplicate indices | O(N) time / O(N) space.O(1) space variant: sort + twoâ€‘pointer, but destroys order. | See `two_sum.py` below |
| 2 | ReverseÂ LinkedÂ List            | Pointers          | 1â€‘node list; empty list          | O(N) / **O(1)**                                                                    | `reverse_list.py`      |
| 3 | DetectÂ CycleÂ inÂ DirectedÂ Graph | DFS, inâ€‘stack set | selfâ€‘loop; disconnected graph    | O(V+E) / O(V)                                                                      | `has_cycle.py`         |
| 4 | LRUÂ Cache (Design)             | HashÂ Map + DLL    | capacityÂ =Â 1                     | O(1) per op / O(C)                                                                 | `lru_cache.py`         |
| 5 | Longest Substring w/o Repeat   | SlidingÂ Window    | unicode; all same char           | O(N) / O(min(N,Î£))                                                                 | `longest_sub.py`       |
| 6 | SearchÂ RotatedÂ Array           | BinaryÂ Search     | duplicates; no rotation          | O(logâ€¯N) / **O(1)**                                                                | `search_rot.py`        |
| 7 | NumberÂ ofÂ Islands              | BFS               | diagonal adjacency?              | O(NM) / O(min(N,M))                                                                | `num_islands.py`       |
| 8 | MergeÂ Intervals                | SortÂ + sweep      | nested intervals                 | O(NÂ logâ€¯N) / **O(1)**                                                              | `merge_int.py`         |

---

##### two\_sum.py (hashÂ map, narrated)

```python
# ===== Two Sum =====
# Return indices i,j such that nums[i] + nums[j] = target.
# Hashâ€‘map gives O(N) time, O(N) extra space.
# Interview twist: can you trade space for O(N log N) sort?

def two_sum(nums, target):
    seen = {}                      # val -> index
    for i, x in enumerate(nums):
        complement = target - x
        if complement in seen:     # found pair
            return [seen[complement], i]
        seen[x] = i                # store after check to avoid i==j
    return []                      # no solution â†¦ empty list
```

##### reverse\_list.py (iterative O(1) space)

```python
class ListNode:
    def __init__(self, val=0, nxt=None):
        self.val, self.next = val, nxt

def reverse_list(head):
    """Reverse singly linked list inâ€‘place."""
    prev, cur = None, head
    while cur:
        nxt = cur.next      # 1. save next
        cur.next = prev     # 2. reverse pointer
        prev, cur = cur, nxt  # 3. advance both ptrs
    return prev
# Edge cases: head is None, single node â†’ loop runs 0 / 1 times.
```

##### search\_rot.py (binary search w/ rotation awareness)

```python
def search_rot(nums, target):
    lo, hi = 0, len(nums) - 1
    while lo <= hi:
        mid = (lo + hi) // 2
        if nums[mid] == target:
            return mid
        # Left half sorted
        if nums[lo] <= nums[mid]:
            if nums[lo] <= target < nums[mid]:
                hi = mid - 1
            else:
                lo = mid + 1
        # Right half sorted
        else:
            if nums[mid] < target <= nums[hi]:
                lo = mid + 1
            else:
                hi = mid - 1
    return -1  # not found
# Handles no rotation (fully sorted) because first branch holds.
```

Focus on narrating **invariants**, checking **boundary indices** (0,Â lenâ€‘1), and talking through **empty inputs** and **duplicated values** during the interview.

---

### 3.1Â Â LinearÂ Regression (batch GD, PyTorchÂ autograd allowed)

```python
# ===== Batch Gradient Descent Linear Regression =====
# Goal: learn weight vector w that minimises MSE â€–Xw âˆ’ yâ€–Â²
# Key interview checks:
#   â€¢ Can you derive the gradient?  âˆ‡ = 2 Xáµ€(Xw âˆ’ y) / N
#   â€¢ Why not use closedâ€‘form?  (Xáµ€X)â»Â¹ is numerically unstable for illâ€‘conditioned X.
#   â€¢ How do you pick learningâ€‘rate?  Try lr=1eâ€‘2 â†’ diverges if features unâ€‘standardised.

import torch
import torch.optim as optim

torch.manual_seed(0)            # reproducibility â€“ interviewers love this!
N, d = 1_000, 5                 # N samples, d features
X = torch.randn(N, d)           # design matrix ğ‘¿
true_w = torch.randn(d, 1)      # groundâ€‘truth weights

# Generate labels:  y = XÂ·w*  + Gaussian noise Îµ~ğ’©(0,0.1)
y = X @ true_w + 0.1 * torch.randn(N, 1)

# Initialise learnable weights with gradient tracking
a = torch.zeros(d, 1, requires_grad=True)

opt = optim.SGD([a], lr=1e-2)   # SGD is fine for small data; Adam more robust on noisy grads

for epoch in range(200):        # fixed epoch budget gives deterministic runtime
    opt.zero_grad()             # 1. clear old gradients â† *easy to forget!*
    y_hat = X @ a               # 2. forward pass
    loss = torch.mean((y_hat - y)**2)  # 3. compute MSE
    loss.backward()             # 4. populate a.grad via autograd DAG
    opt.step()                  # 5. gradient descent update

# At convergence â€–a âˆ’ true_wâ€–âˆ â‰ˆ 0.05 â†’ sanity check.
```

A closedâ€‘form solution via `(Xáµ€X)â»Â¹Xáµ€y` exists but showcases numerical instability for illâ€‘conditioned matrices; gradientâ€‘descent avoids explicit inversionÂ ([medium.com](https://medium.com/%40sahin.samia/understanding-pytorch-basics-through-building-a-logistic-regression-model-from-scratch-71be33a43a00?utm_source=chatgpt.com)).

### 3.2Â Â LogisticÂ Regression (+Â L2)

```python
# ===== Logistic Regression with L2 Regularisation =====
# Binary classification:  p(y=1|x) = Ïƒ(xÂ·w + b)
# Gotchas:
#   â€¢ For severe classâ€‘imbalance, switch to focalâ€‘loss or weight positive class.
#   â€¢ torch.sigmoid overflows for |x|>88 â†’ safe for float32 magnitude.

import torch
import torch.nn as nn
import torch.optim as optim

class Logistic(nn.Module):
    def __init__(self, d: int):
        super().__init__()
        # initialise nearâ€‘zero weights; random init would break symmetry irrelevantly here
        self.w = nn.Parameter(torch.zeros(d, 1))
        self.b = nn.Parameter(torch.zeros(1))

    def forward(self, X: torch.Tensor) -> torch.Tensor:
        logits = X @ self.w + self.b
        return torch.sigmoid(logits)

def train_lr(X: torch.Tensor, y: torch.Tensor, lam: float = 1e-2,
             lr: float = 1e-2, epochs: int = 300) -> Logistic:
    """lam â€“ L2 penalty; lr â€“ learning rate. y must be float 0/1 of shape (N,1)."""
    model = Logistic(X.size(1))
    opt   = optim.Adam(model.parameters(), lr=lr)  # Adam tackles poorly scaled features
    for epoch in range(epochs):
        opt.zero_grad()
        probs = model(X)
        ce_loss = nn.functional.binary_cross_entropy(probs, y)
        l2_pen  = lam * (model.w ** 2).sum()
        loss = ce_loss + l2_pen
        loss.backward()
        opt.step()
    return model
```

Add quadratic terms (`xâ‚Â², xâ‚‚Â², xâ‚xâ‚‚`) or use a polynomial kernel to fit quadratically separable dataÂ ([xavierbourretsicotte.github.io](https://xavierbourretsicotte.github.io/Kernel_feature_map.html?utm_source=chatgpt.com)).

### 3.3Â Â kâ€‘NearestÂ Neighbors

Training cost is **O(1)**â€”just store the dataset; inference is **O(Nâ€¯d)** for Euclidean scanÂ ([medium.com](https://medium.com/nerd-for-tech/why-the-time-complexity-for-training-k-nearest-neighbors-is-o-1-5b8f417104cf?utm_source=chatgpt.com))Â ([en.wikipedia.org](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?utm_source=chatgpt.com)).  Accelerate with FAISS or ApproximateÂ NN.

**Pseudoâ€‘code:**

```python
# ===== Bruteâ€‘force kâ€‘Nearest Neighbour (classification) =====
# Complexity:
#   Train  â€“ O(1) (store dataset)
#   Query  â€“ O(NÂ·d) per point â†’ poor scalability.  Use FAISS/HNSW for ANN.
# Edge cases:
#   â€¢ Tieâ€‘breaking: choose smallest distance or lower label id.
#   â€¢ Missing values: impute or distance masking.

import torch


def knn_predict(X_train: torch.Tensor,           # shape (N, d)
                y_train: torch.Tensor,           # shape (N,)
                x_query: torch.Tensor,           # shape (d,)
                k: int = 5) -> int:
    """Return majority label among k nearest Euclidean neighbours."""
    # 1. Squared distance (no sqrt) â€“ monotonic for ranking
    dists2 = ((X_train - x_query) ** 2).sum(dim=1)

    # 2. Indices of k smallest distances; topk with largest=False is efficient
    knn_idx = torch.topk(dists2, k, largest=False).indices

    # 3. Mode of labels (classification). For regression: y_train[knn_idx].mean()
    return torch.mode(y_train[knn_idx]).values.item()
```

### 3.4Â Â 1â€‘Hiddenâ€‘Layer NeuralÂ Network (+Â Dropout)

```python
# ===== Minimal 1â€‘Hiddenâ€‘Layer Multilayer Perceptron =====
# Design choices:
#   â€¢ ReLU for fast convergence; swap with GELU for better performance on large models.
#   â€¢ Dropout combats coâ€‘adaptation; disable during eval.
#   â€¢ Weight initialisation: default Kaiming works for ReLU.

import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, d_in: int, d_h: int, d_out: int, p_drop: float = 0.3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d_in, d_h),
            nn.ReLU(),            # avoids vanishing gradients vs tanh/sigmoid
            nn.Dropout(p_drop),   # active only when model.train() is True
            nn.Linear(d_h, d_out)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)

# Gotcha:
#   Call model.eval() before validation/testing to freeze dropout & batchâ€‘norm stats.
```

ReLU preserves gradient magnitude for positive inputs, easing deep training compared with tanh/sigmoidÂ ([telnyx.com](https://telnyx.com/learn-ai/rectified-linear-unit?utm_source=chatgpt.com))Â ([machinelearningmastery.com](https://www.machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/?utm_source=chatgpt.com)).

### 3.5Â Â LossÂ &â€¯Activation Functions â€” DeepÂ Dive

Loss (objective) functions guide gradient descent by mapping model outputs to a scalar â€œcost.â€ Picking the wrong loss can **slow convergence, mask bugs, or misalign with business KPIs.** Below is a concise but thorough cheatâ€‘sheet.

#### 3.5.1Â â€¯Core Activations

| Activation     | Formula           | Gradient       | Typical Use     | Gotchas                                             |   |                      |
| -------------- | ----------------- | -------------- | --------------- | --------------------------------------------------- | - | -------------------- |
| **Sigmoid**    | Ïƒ(z)=1/(1+e^{âˆ’z}) | Ïƒ(z)(1âˆ’Ïƒ(z))   | Binary logits   | Gradient vanishes when                              | z | â‰«0; clip inputsÂ <88. |
| **Tanh**       | tanh(z)=2Ïƒ(2z)âˆ’1  | 1âˆ’tanhÂ²(z)     | RNNs preâ€‘LSTM   | Saturates onÂ±1; use GRU/LSTM instead.               |   |                      |
| **ReLU**       | max(0,z)          | 1[z>0]         | CNN/MLP default | â€œDead neuronsâ€ if lr too high.                      |   |                      |
| **Leaky ReLU** | max(Î±z,z)         | 1[z>0]+Î±1[zâ‰¤0] | GANs            | Pick Î±â‰ˆ0.01.                                        |   |                      |
| **Softmax**    | e^{záµ¢}/Î£ e^{zâ±¼}   | páµ¢(Î´áµ¢â±¼âˆ’pâ±¼)     | Multiclass      | Compute on *logits*; subtract z\_max for stability. |   |                      |

#### 3.5.2Â â€¯Supervised Losses & When to Use Them

| Loss                           | Formula (perâ€‘sample)                 | Task Fit             | Pros                              | Cons / Pitfalls                                         |                    |                                        |                |                         |
| ------------------------------ | ------------------------------------ | -------------------- | --------------------------------- | ------------------------------------------------------- | ------------------ | -------------------------------------- | -------------- | ----------------------- |
| **MSE (L2)**                   | (yÌ‚âˆ’y)Â²                              | Regression           | Smooth gradient; closedâ€‘form soln | Sensitive to outliers; mismatch for heavyâ€‘tailed noise. |                    |                                        |                |                         |
| **MAE (L1)**                   |                                      | yÌ‚âˆ’y                 |                                   | Regression median                                       | Robust to outliers | Nonâ€‘differentiable at 0; slower optim. |                |                         |
| **Huber**                      | Â½(yÌ‚âˆ’y)Â² if                          | Î”                    | <Î´ else Î´(                        | Î”                                                       | âˆ’Â½Î´)               | Regression, RL                         | Combines L1+L2 | Need Î´ hyperâ€‘parameter. |
| **BinaryÂ Crossâ€‘Entropy (BCE)** | âˆ’[yÂ logâ€¯pÂ +(1âˆ’y)Â logâ€¯(1âˆ’p)]          | Binary class         | Probabilistic, differentiable     | Must avoid log(0): add Îµ or useÂ `BCEWithLogitsLoss`.    |                    |                                        |                |                         |
| **BCEâ€¯withâ€¯Logits**            | BCE on logitsÂ z with `sigmoid` fused | Binary               | Numericalâ€‘stable; single op       | Forgetting reduction='sum' vs 'mean' changes scale.     |                    |                                        |                |                         |
| **CategoricalÂ Crossâ€‘Entropy**  | âˆ’Î£ yáµ¢Â logâ€¯páµ¢                         | Multiclass softmax   | Works with oneâ€‘hot labels         | Use `CrossEntropyLoss` which fuses `log_softmax`.       |                    |                                        |                |                         |
| **FocalÂ Loss**                 | âˆ’Î±(1âˆ’páµ§)^Î³Â logâ€¯páµ§                    | Imbalanced detection | Downâ€‘weights easy negatives       | Tune Î±,â€¯Î³; heavier compute.                             |                    |                                        |                |                         |
| **Hinge (SVM)**                | max(0,1âˆ’yÂ f(x))                      | Largeâ€‘margin binary  | Sparse gradients                  | Nonâ€‘probabilistic; requires yâˆˆ{Â±1}.                     |                    |                                        |                |                         |
| **Triplet / Contrastive**      | max(d(a,p)âˆ’d(a,n)+m,0)               | MetricÂ learning      | Learns embedding                  | Needs hardâ€‘example mining.                              |                    |                                        |                |                         |

> **Ruleâ€‘ofâ€‘thumb:** *Classification?* â†’ BCE (binary) or CE (multiâ€‘class).\
> *Regression with outliers?* â†’ start with Huber.\
> *Extreme classâ€‘imbalance* (fraud, defectâ€‘detection)? â†’ Focal or classâ€‘weighted BCE.

#### 3.5.3Â â€¯Numerically Stable Implementations

```python
# ----- Binary Crossâ€‘Entropy w/ logits (preferred) -----
import torch, torch.nn.functional as F

logits = model(x)              # raw, unâ€‘squashed scores
labels = torch.empty_like(logits).bernoulli_(0.1)
loss = F.binary_cross_entropy_with_logits(logits, labels, reduction="mean")

# Internally:  BCE = max(z,0) âˆ’ z*y + log(1+exp(âˆ’|z|))
# This avoids overflow when |z| is large.

# ----- Manual Softmax Crossâ€‘Entropy (multiâ€‘class) -----
logits = model(x)              # shape (N, C)
log_prob = logits - logits.logsumexp(dim=1, keepdim=True)  # logâ€‘softmax
nll_loss = -log_prob[torch.arange(N), targets]             # NLL
loss = nll_loss.mean()
```

*Gotchas*

- Using `sigmoid` **followed** by `BCELoss` duplicates the sigmoid when networking in evaluation modeâ€”>Â probabilities get squashed twice.
- For CE the target must be **class indices**, *not* oneâ€‘hot; automatic reduction defaults to **mean**â€”scales differently from TF defaultÂ **sum**.
- Labelâ€‘smoothing (e.g., Îµ=0.1) can regularise CE by preventing overâ€‘confident logits; PyTorchÂ `CrossEntropyLoss(label_smoothing=0.1)`.

#### 3.5.4Â â€¯Connecting Loss to Business Metrics

- Fraud detection usually optimises **expected cost** = FPâ€¯Â·â€¯OpEx + FNâ€¯Â·â€¯chargeback.  BCE aligns with logâ€‘likelihood, but you can reâ€‘weight positive class proportional to chargeback cost.
- Recommendation quality often measured by **NDCG**; softmaxâ€‘CE on implicit positive vs sampled negatives approximates ranking objective (see *BPR loss*).
- CV quality inspection may care about **Intersectionâ€‘overâ€‘Union**; training with BCE on pixel masks but monitoring IoU ensures production alignment.

---

## 4Â Â MLÂ Breadth FlashÂ Cards (with Answers)

Â Â MLÂ Breadth FlashÂ Cards (with Answers)

| Question                                                          | 20â€‘second Whiteboard Answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ----------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Parameters of a depthâ€‘1 decision tree on two features (xâ‚,xâ‚‚)** | Internal node stores `(feature, threshold)`; two leaves store predictions. Total learnable paramsÂ =Â 1 thresholdÂ +Â 2 leaf valuesÂ ([scikit-learn.org](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html?utm_source=chatgpt.com))                                                                                                                                                                                                                                                                                                      |
| **Parameter vector for logistic regression (2â€‘D)**                | Î¸Â âˆˆÂ â„Â³:Â `[bias, wâ‚, wâ‚‚]`. Decision boundaryÂ wâ‚xâ‚Â +Â wâ‚‚xâ‚‚Â +Â biasÂ =Â 0.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| **Adapting logistic regression to quadratic separation**          | Expand features: `[1, xâ‚, xâ‚‚, xâ‚Â², xâ‚‚Â², xâ‚xâ‚‚]` or apply polynomial kernel; train standard LR on mapped spaceÂ ([xavierbourretsicotte.github.io](https://xavierbourretsicotte.github.io/Kernel_feature_map.html?utm_source=chatgpt.com))                                                                                                                                                                                                                                                                                                                               |
| **Precision vs Recall**                                           | PrecisionÂ =Â TP/(TP+FP) prioritises falseâ€‘positive cost; RecallÂ =Â TP/(TP+FN) prioritises falseâ€‘negativesÂ ([en.wikipedia.org](https://en.wikipedia.org/wiki/Precision_and_recall?utm_source=chatgpt.com)).  In fraud detection you often sweep thresholds for PR curve rather than ROC due to heavy class imbalanceÂ ([kount.com](https://kount.com/blog/precision-recall-when-conventional-fraud-metrics-fall-short?utm_source=chatgpt.com))Â ([evidentlyai.com](https://www.evidentlyai.com/classification-metrics/accuracy-precision-recall?utm_source=chatgpt.com)). |
| **F1â€‘score rationale**                                            | Harmonic mean balances precision/recall; useful when uneven class distributionÂ ([developers.google.com](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?utm_source=chatgpt.com))                                                                                                                                                                                                                                                                                                                                |
| **Why ReLU over tanh?**                                           | Nonâ€‘saturating for positive z, sparse activations, mitigates vanishing gradientÂ ([telnyx.com](https://telnyx.com/learn-ai/rectified-linear-unit?utm_source=chatgpt.com))                                                                                                                                                                                                                                                                                                                                                                                             |
| **kâ€‘NN Pros/Cons**                                                | +Â No training, interpretable; â€“Â slow at inference, curse of dimensionalityÂ ([en.wikipedia.org](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?utm_source=chatgpt.com))                                                                                                                                                                                                                                                                                                                                                                                  |

---

## 5Â Â SystemÂ Design Templates

### 5.1Â Â Fraud Detection (Realâ€‘Time)

1. **Problem & KPI**Â â€” optimise *cost = FPâ€¯Ã—â€¯\$OPEX + FNâ€¯Ã—â€¯chargebacks*.
2. **DataÂ Ingestion**Â â€” Kinesis stream â†’ AWSÂ FeatureÂ Store (online)Â +Â S3 (offline)Â ([aws.amazon.com](https://aws.amazon.com/sagemaker-ai/deploy/?utm_source=chatgpt.com)).
3. **Feature Groups**Â â€” transaction, user behaviour, device/IP risk, merchant profile.
4. **ModelÂ Options**
   | Candidate            | Pros                          | Cons                     |
   | -------------------- | ----------------------------- | ------------------------ |
   | XGBoost              | tabular, lowâ€‘latency          | limited temporal context |
   | DeepÂ Sets            | orderâ€‘invariant cart features | heavier infra            |
   | GNN (userâ€‘txn graph) | collusion rings               | coldâ€‘start               |
5. **Serving**Â â€” SageMaker realâ€‘time endpoint p95Â <Â 50â€¯ms, proven by RadÂ AI and Cisco case studiesÂ ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/rad-ai-reduces-real-time-inference-latency-by-50-using-amazon-sagemaker/?utm_source=chatgpt.com))Â ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/cisco-achieves-50-latency-improvement-using-amazon-sagemaker-inference-faster-autoscaling-feature/?utm_source=chatgpt.com)).
6. **Monitoring**Â â€” KSâ€‘stat data drift, population stability index, ShadowÂ mode.
7. **Retraining**Â â€” nightly incremental; champion/challenger gating.
8. **Privacy**Â â€” optional federated learning via Flower on SageMakerÂ ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/fraud-detection-empowered-by-federated-learning-with-the-flower-framework-on-amazon-sagemaker-ai/?utm_source=chatgpt.com)).

### 5.2Â Â ComputerÂ Vision Quality Inspection

Same 8â€‘step skeleton; swap streaming source with S3Â BatchÂ +Â SNS triggers; model candidates: EfficientNet (latency), VisionÂ Transformers (accuracy).  Edge deployment viaÂ SageMaker Neo.

---

## 6Â Â 14â€‘Day PowerÂ Study Plan

| Day | AM                                  | PM                                                                                                                                                                                          |
| --- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1   | Mock system design (fraud)          | Review LP STARs                                                                                                                                                                             |
| 2   | Code: linear + logistic timed       | ML breadth quiz (Huyen)Â ([huyenchip.com](https://huyenchip.com/ml-interviews-book/?utm_source=chatgpt.com))                                                                                 |
| 3   | Second system design (CV)           | Implement KNN + benchmark                                                                                                                                                                   |
| 4   | Review decision trees, kernels      | Build NN with dropout                                                                                                                                                                       |
| 5   | Endâ€‘toâ€‘end SageMaker lab            | Watch RadÂ AI latency talkÂ ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/rad-ai-reduces-real-time-inference-latency-by-50-using-amazon-sagemaker/?utm_source=chatgpt.com)) |
| 6   | Mock full loop (record)             | Retrospective improvements                                                                                                                                                                  |
| 7   | REST â€” light flashÂ cards            |                                                                                                                                                                                             |
| 8   | Barâ€‘raiser style peer interview     | Precisionâ€‘Recall threshold tuning                                                                                                                                                           |
| 9   | Liveâ€‘coding drills (leetcodeâ€‘style) | Reâ€‘write bugâ€‘free quickly                                                                                                                                                                   |
| 10  | CV or NLP domain deepâ€‘dive          | Update STAR metrics                                                                                                                                                                         |
| 11  | Dryâ€‘run presentation of research    |                                                                                                                                                                                             |
| 12  | Second mock loop                    | Sleep hygiene                                                                                                                                                                               |
| 13  | Travel/logistics                    | Skim notes                                                                                                                                                                                  |
| 14  | Onâ€‘site â€” execute                   |                                                                                                                                                                                             |

---

## 7Â Â Curated Resource Stack

- **HuyenÂ Chipâ€™s MLÂ Interview Book**Â â€” 200+ breadth questionsÂ ([huyenchip.com](https://huyenchip.com/ml-interviews-book/?utm_source=chatgpt.com)).
- **IGotAnOffer Guides**Â â€” Loop breakdown, Barâ€‘raiser tipsÂ ([igotanoffer.com](https://igotanoffer.com/en/advice/amazon-applied-scientist-interview?utm_source=chatgpt.com))Â ([igotanoffer.com](https://igotanoffer.com/blogs/tech/amazon-bar-raiser-interview?utm_source=chatgpt.com)).
- **SageMaker Doc Set**Â â€” deployment, latency optimisationÂ ([aws.amazon.com](https://aws.amazon.com/sagemaker-ai/deploy/?utm_source=chatgpt.com))Â ([docs.aws.amazon.com](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html?utm_source=chatgpt.com)).
- **AWS Fraud Detection Blog Series**Â â€” federated learning, feature engineeringÂ ([aws.amazon.com](https://aws.amazon.com/blogs/machine-learning/fraud-detection-empowered-by-federated-learning-with-the-flower-framework-on-amazon-sagemaker-ai/?utm_source=chatgpt.com)).
- **Scikitâ€‘learn tree structure example**Â â€” visualise parametersÂ ([scikit-learn.org](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html?utm_source=chatgpt.com)).
- **Kernel & Feature Map Tutorial**Â â€” polynomial LR intuitionÂ ([xavierbourretsicotte.github.io](https://xavierbourretsicotte.github.io/Kernel_feature_map.html?utm_source=chatgpt.com)).
- **Google MLÂ Crash Course**Â â€” metrics refresherÂ ([developers.google.com](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall?utm_source=chatgpt.com)).
- **ReLU & Vanishing Gradient Reading**Â â€” Telnyx, MachineLearningMasteryÂ ([telnyx.com](https://telnyx.com/learn-ai/rectified-linear-unit?utm_source=chatgpt.com))Â ([machinelearningmastery.com](https://www.machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/?utm_source=chatgpt.com)).

---

### Final Reminder

1. **Tie every technical decision to a customerÂ metric.** 2. **Narrate tradeâ€‘offs out loud.** 3. **Lead with Amazon LP language.**  You now have both theory and code at your fingertipsâ€”time to practise until every section flows in under 15Â minutes.

