{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362732a3",
   "metadata": {},
   "source": [
    "# Transformer Models Interview Preparation\n",
    "\n",
    "## Complete Guide for Applied Scientist Roles\n",
    "\n",
    "This notebook covers comprehensive transformer knowledge for **Amazon Applied Scientist** and similar roles, including:\n",
    "\n",
    "### üèóÔ∏è **Core Architecture & Implementation:**\n",
    "- **Attention Mechanism**: Self-attention, multi-head attention, scaled dot-product\n",
    "- **Positional Encoding**: Sinusoidal, learned, relative positioning\n",
    "- **Layer Normalization**: Pre-norm vs post-norm, RMSNorm alternatives\n",
    "- **Feed-Forward Networks**: Position-wise, activation functions (GELU, SwiGLU)\n",
    "\n",
    "### üîß **Fine-tuning & Optimization:**\n",
    "- **Parameter-Efficient Fine-tuning**: LoRA, AdaLoRA, QLoRA, Prefix Tuning\n",
    "- **Full Fine-tuning**: Task-specific adaptation, catastrophic forgetting\n",
    "- **Instruction Tuning**: RLHF, constitutional AI, alignment techniques\n",
    "- **Domain Adaptation**: Medical, legal, financial domain fine-tuning\n",
    "\n",
    "### üß† **RAG (Retrieval-Augmented Generation):**\n",
    "- **Vector Databases**: Embedding storage, similarity search\n",
    "- **Retrieval Strategies**: Dense, sparse, hybrid retrieval\n",
    "- **Chunk Optimization**: Size, overlap, hierarchical chunking\n",
    "- **Re-ranking**: Cross-encoder re-ranking, relevance scoring\n",
    "\n",
    "### üìù **Prompt Engineering & Templates:**\n",
    "- **Prompt Design**: Zero-shot, few-shot, chain-of-thought\n",
    "- **Template Systems**: Dynamic templates, role-based prompting\n",
    "- **Context Management**: Token optimization, context window usage\n",
    "- **Evaluation**: Prompt effectiveness metrics, A/B testing\n",
    "\n",
    "### üöÄ **Production Deployment:**\n",
    "- **Model Serving**: Azure OpenAI, Hugging Face Inference\n",
    "- **Optimization**: Quantization, pruning, knowledge distillation\n",
    "- **Monitoring**: Performance metrics, drift detection\n",
    "- **Scaling**: Load balancing, auto-scaling strategies\n",
    "\n",
    "### ‚ö° **Advanced Topics:**\n",
    "- **Mixture of Experts (MoE)**: Sparse activation, routing\n",
    "- **Multi-modal Transformers**: Vision-language models\n",
    "- **Long Context**: Efficient attention, memory mechanisms\n",
    "- **Safety & Alignment**: Bias detection, content filtering\n",
    "\n",
    "### üéØ **Interview Focus Areas:**\n",
    "1. ‚úÖ **Mathematical foundations** - Attention computation, complexity analysis\n",
    "2. ‚úÖ **Implementation details** - Custom layers, training loops\n",
    "3. ‚úÖ **Production challenges** - Latency, throughput, cost optimization\n",
    "4. ‚úÖ **Evaluation metrics** - BLEU, ROUGE, human evaluation\n",
    "5. ‚úÖ **Failure modes** - Hallucination, catastrophic forgetting\n",
    "6. ‚úÖ **Recent advances** - Latest papers, architectural innovations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d65385d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Essential libraries for transformer implementation and production deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c54861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformer-specific libraries\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModel, AutoModelForCausalLM,\n",
    "        TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "        pipeline, BitsAndBytesConfig\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "    print(\"‚úÖ Transformers libraries imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Transformers libraries not available - will show implementation concepts\")\n",
    "\n",
    "# Vector search and RAG\n",
    "try:\n",
    "    import faiss\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Vector search libraries imported\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Vector search libraries not available - will show concepts\")\n",
    "\n",
    "# Azure libraries for production\n",
    "try:\n",
    "    from azure.ai.ml import MLClient\n",
    "    from azure.identity import DefaultAzureCredential\n",
    "    print(\"‚úÖ Azure ML libraries imported\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Azure libraries not available - will show deployment concepts\")\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"\\nüöÄ Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nüéØ Ready for transformer deep dive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6848fe4",
   "metadata": {},
   "source": [
    "## 2. Core Transformer Architecture: Mathematical Foundation\n",
    "\n",
    "### üßÆ **Attention Mechanism - The Heart of Transformers**\n",
    "\n",
    "**Scaled Dot-Product Attention:**\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **Q (Query)**: What we're looking for\n",
    "- **K (Key)**: What we're looking in  \n",
    "- **V (Value)**: What we retrieve\n",
    "- **d_k**: Dimension of key vectors (for scaling)\n",
    "\n",
    "**Multi-Head Attention:**\n",
    "```\n",
    "MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O\n",
    "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "```\n",
    "\n",
    "### üìä **Complexity Analysis:**\n",
    "- **Self-Attention**: O(n¬≤d) time, O(n¬≤) space\n",
    "- **Feed-Forward**: O(nd¬≤) time, O(d¬≤) space  \n",
    "- **Total per layer**: O(n¬≤d + nd¬≤)\n",
    "\n",
    "### üîß **Key Design Decisions:**\n",
    "1. **Why scaling by ‚àöd_k?** Prevents softmax saturation\n",
    "2. **Why multiple heads?** Different representation subspaces\n",
    "3. **Why residual connections?** Gradient flow, easier training\n",
    "4. **Why layer normalization?** Training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc6282",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention implementation with all the production considerations.\n",
    "    \n",
    "    Key Interview Points:\n",
    "    1. Scaled dot-product attention formula\n",
    "    2. Multiple heads for different representation subspaces\n",
    "    3. Attention weights interpretation\n",
    "    4. Computational complexity: O(n¬≤d)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # Dimension per head\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)  # Output projection\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)  # Scaling factor\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None, return_attention=False):\n",
    "        batch_size, seq_len = query.size(0), query.size(1)\n",
    "        \n",
    "        # 1. Linear projections and reshape for multi-head\n",
    "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        # Shape: (batch_size, n_heads, seq_len, d_k)\n",
    "        \n",
    "        # 2. Scaled dot-product attention\n",
    "        attention, attention_weights = self.scaled_dot_product_attention(\n",
    "            Q, K, V, mask, self.scale, self.dropout\n",
    "        )\n",
    "        \n",
    "        # 3. Concatenate heads and project\n",
    "        attention = attention.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        output = self.w_o(attention)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(Q, K, V, mask, scale, dropout):\n",
    "        \"\"\"\n",
    "        Core attention computation: Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n",
    "        \n",
    "        INTERVIEW CRITICAL: Explain each step and why scaling is needed\n",
    "        \"\"\"\n",
    "        # Compute attention scores: QK^T\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / scale\n",
    "        \n",
    "        # Apply mask (for padding, future positions)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attention = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return attention, attention_weights\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding for transformers.\n",
    "    \n",
    "    Formula: PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "             PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \n",
    "    Interview Questions:\n",
    "    - Why not learnable positions?\n",
    "    - How does this generalize to unseen lengths?\n",
    "    - Alternative: Relative position encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Even positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd positions\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete transformer block with attention + feed-forward.\n",
    "    \n",
    "    Architecture: LayerNorm -> MultiHeadAttention -> Residual -> LayerNorm -> FFN -> Residual\n",
    "    \n",
    "    Key Design Choices:\n",
    "    1. Pre-norm vs Post-norm (Pre-norm is more stable)\n",
    "    2. GELU activation (smoother than ReLU)\n",
    "    3. Residual connections (gradient flow)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),  # GELU is smoother than ReLU\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm architecture (more stable training)\n",
    "        # Self-attention block\n",
    "        norm_x = self.norm1(x)\n",
    "        attn_output = self.attention(norm_x, norm_x, norm_x, mask)\n",
    "        x = x + self.dropout(attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed-forward block\n",
    "        norm_x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        x = x + ff_output  # Residual connection\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "print(\"üèóÔ∏è TRANSFORMER ARCHITECTURE COMPONENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test parameters\n",
    "batch_size, seq_len, d_model = 2, 10, 512\n",
    "n_heads, d_ff = 8, 2048\n",
    "\n",
    "# Create test input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Test each component\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Positional encoding\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "x_with_pos = pos_enc(x)\n",
    "print(f\"After positional encoding: {x_with_pos.shape}\")\n",
    "\n",
    "# Multi-head attention\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "attn_output, attn_weights = mha(x, x, x, return_attention=True)\n",
    "print(f\"Attention output: {attn_output.shape}\")\n",
    "print(f\"Attention weights: {attn_weights.shape}\")\n",
    "\n",
    "# Full transformer block\n",
    "transformer_block = TransformerBlock(d_model, n_heads, d_ff)\n",
    "block_output = transformer_block(x)\n",
    "print(f\"Transformer block output: {block_output.shape}\")\n",
    "\n",
    "# Calculate model parameters\n",
    "total_params = sum(p.numel() for p in transformer_block.parameters())\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Memory per sample: ~{(seq_len * d_model * 4) / 1024:.1f} KB\")\n",
    "print(f\"Attention complexity: O(n¬≤) where n = {seq_len}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Core transformer components implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a791bf3",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning Strategies: From Full to Parameter-Efficient\n",
    "\n",
    "### üéØ **Fine-tuning Approaches Comparison:**\n",
    "\n",
    "| Method | Parameters Updated | Memory | Training Speed | Performance |\n",
    "|--------|-------------------|---------|----------------|-------------|\n",
    "| **Full Fine-tuning** | 100% | High | Slow | Best |\n",
    "| **LoRA** | 0.1-1% | Low | Fast | 95-99% of full |\n",
    "| **AdaLoRA** | 0.1-1% | Low | Fast | Better than LoRA |\n",
    "| **QLoRA** | 0.1-1% | Very Low | Fast | Good (4-bit) |\n",
    "| **Prefix Tuning** | 0.1% | Very Low | Very Fast | 90-95% of full |\n",
    "\n",
    "### üîß **LoRA (Low-Rank Adaptation):**\n",
    "- **Key Idea**: Decompose weight updates into low-rank matrices\n",
    "- **Formula**: W = W‚ÇÄ + ŒîW = W‚ÇÄ + BA (where B ‚àà ‚Ñù·µàÀ£ ≥, A ‚àà ‚Ñù ≥À£·µè)\n",
    "- **Rank r**: Typically 4-64 (much smaller than original dimensions)\n",
    "- **Advantages**: Memory efficient, fast training, preserves original model\n",
    "\n",
    "### ‚ö° **Production Considerations:**\n",
    "1. **Memory Requirements**: LoRA reduces GPU memory by 3-4x\n",
    "2. **Training Speed**: 2-3x faster than full fine-tuning\n",
    "3. **Inference**: Can merge adapters for zero overhead\n",
    "4. **Multi-task**: Multiple adapters for different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab4eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA (Low-Rank Adaptation) implementation for efficient fine-tuning.\n",
    "    \n",
    "    Key Innovation: Instead of updating full weight matrix W,\n",
    "    we learn low-rank decomposition: ŒîW = B @ A\n",
    "    where B ‚àà R^(d√ór), A ‚àà R^(r√ók), and r << min(d,k)\n",
    "    \n",
    "    Interview Points:\n",
    "    1. Why low-rank? Most updates lie in low-dimensional subspace\n",
    "    2. Parameter reduction: d√ók ‚Üí (d+k)√ór\n",
    "    3. Initialization: A ~ N(0, œÉ), B = 0 (preserves original model)\n",
    "    4. Scaling factor Œ± controls adaptation strength\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 4, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.original_layer = original_layer\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Get dimensions\n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # LoRA matrices: W + Œ±(B @ A)\n",
    "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.02)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        \n",
    "        # Freeze original weights\n",
    "        for param in self.original_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Calculate parameter reduction\n",
    "        original_params = in_features * out_features\n",
    "        lora_params = rank * (in_features + out_features)\n",
    "        self.param_reduction = original_params / lora_params\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Original computation\n",
    "        original_output = self.original_layer(x)\n",
    "        \n",
    "        # LoRA adaptation: x @ A^T @ B^T\n",
    "        lora_output = (x @ self.lora_A.T) @ self.lora_B.T\n",
    "        \n",
    "        # Combine with scaling\n",
    "        return original_output + (self.alpha / self.rank) * lora_output\n",
    "    \n",
    "    def merge_weights(self):\n",
    "        \"\"\"Merge LoRA weights into original layer for inference efficiency.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Compute ŒîW = Œ±(B @ A) / rank\n",
    "            delta_w = (self.alpha / self.rank) * (self.lora_B @ self.lora_A)\n",
    "            # Update original weights\n",
    "            self.original_layer.weight.data += delta_w\n",
    "            \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get LoRA statistics for analysis.\"\"\"\n",
    "        return {\n",
    "            'rank': self.rank,\n",
    "            'alpha': self.alpha,\n",
    "            'param_reduction': f\"{self.param_reduction:.1f}x\",\n",
    "            'trainable_params': self.lora_A.numel() + self.lora_B.numel(),\n",
    "            'original_params': self.original_layer.weight.numel()\n",
    "        }\n",
    "\n",
    "\n",
    "class FineTuningStrategies:\n",
    "    \"\"\"\n",
    "    Collection of fine-tuning strategies for different scenarios.\n",
    "    \n",
    "    Interview Topics:\n",
    "    1. When to use each strategy?\n",
    "    2. Trade-offs: Performance vs Efficiency\n",
    "    3. Catastrophic forgetting prevention\n",
    "    4. Domain adaptation techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_lora_to_model(model, rank=4, alpha=1.0, target_modules=None):\n",
    "        \"\"\"\n",
    "        Apply LoRA to specific modules in a model.\n",
    "        \n",
    "        Common target modules:\n",
    "        - Attention: q_proj, k_proj, v_proj, o_proj\n",
    "        - Feed-forward: gate_proj, up_proj, down_proj\n",
    "        \"\"\"\n",
    "        if target_modules is None:\n",
    "            target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "        \n",
    "        lora_modules = {}\n",
    "        trainable_params = 0\n",
    "        total_params = 0\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear) and any(target in name for target in target_modules):\n",
    "                lora_module = LoRALinear(module, rank, alpha)\n",
    "                lora_modules[name] = lora_module\n",
    "                \n",
    "                # Replace in model\n",
    "                parent = model\n",
    "                components = name.split('.')\n",
    "                for component in components[:-1]:\n",
    "                    parent = getattr(parent, component)\n",
    "                setattr(parent, components[-1], lora_module)\n",
    "                \n",
    "                trainable_params += lora_module.lora_A.numel() + lora_module.lora_B.numel()\n",
    "            \n",
    "            if isinstance(module, nn.Linear):\n",
    "                total_params += module.weight.numel()\n",
    "        \n",
    "        efficiency = (trainable_params / total_params) * 100\n",
    "        \n",
    "        return {\n",
    "            'lora_modules': lora_modules,\n",
    "            'trainable_params': trainable_params,\n",
    "            'total_params': total_params,\n",
    "            'efficiency': f\"{efficiency:.2f}%\"\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_training_config(strategy='lora', task_type='causal_lm'):\n",
    "        \"\"\"\n",
    "        Get recommended training configurations for different strategies.\n",
    "        \n",
    "        Production considerations:\n",
    "        1. Learning rates: LoRA needs higher LR than full fine-tuning\n",
    "        2. Batch sizes: Can be larger due to memory efficiency\n",
    "        3. Warmup: Important for stability\n",
    "        4. Weight decay: Lower for parameter-efficient methods\n",
    "        \"\"\"\n",
    "        configs = {\n",
    "            'full_finetuning': {\n",
    "                'learning_rate': 5e-5,\n",
    "                'batch_size': 8,\n",
    "                'warmup_steps': 100,\n",
    "                'weight_decay': 0.01,\n",
    "                'gradient_accumulation': 4,\n",
    "                'memory_usage': 'High'\n",
    "            },\n",
    "            'lora': {\n",
    "                'learning_rate': 1e-4,\n",
    "                'batch_size': 16,\n",
    "                'warmup_steps': 50,\n",
    "                'weight_decay': 0.001,\n",
    "                'gradient_accumulation': 2,\n",
    "                'memory_usage': 'Low',\n",
    "                'rank': 4,\n",
    "                'alpha': 8\n",
    "            },\n",
    "            'qlora': {\n",
    "                'learning_rate': 2e-4,\n",
    "                'batch_size': 32,\n",
    "                'warmup_steps': 50,\n",
    "                'weight_decay': 0.001,\n",
    "                'gradient_accumulation': 1,\n",
    "                'memory_usage': 'Very Low',\n",
    "                'quantization': '4-bit',\n",
    "                'rank': 8,\n",
    "                'alpha': 16\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return configs.get(strategy, configs['lora'])\n",
    "\n",
    "\n",
    "# Demonstrate LoRA implementation\n",
    "print(\"üîß FINE-TUNING STRATEGIES DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a sample transformer layer\n",
    "sample_transformer = TransformerBlock(d_model=512, n_heads=8, d_ff=2048)\n",
    "\n",
    "# Get original parameter count\n",
    "original_params = sum(p.numel() for p in sample_transformer.parameters() if p.requires_grad)\n",
    "print(f\"Original model parameters: {original_params:,}\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_stats = FineTuningStrategies.apply_lora_to_model(\n",
    "    sample_transformer, rank=4, alpha=8\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä LoRA Statistics:\")\n",
    "print(f\"Trainable parameters: {lora_stats['trainable_params']:,}\")\n",
    "print(f\"Parameter efficiency: {lora_stats['efficiency']}\")\n",
    "print(f\"Memory reduction: ~{original_params // lora_stats['trainable_params']}x\")\n",
    "\n",
    "# Test different configurations\n",
    "print(f\"\\n‚öôÔ∏è Training Configurations:\")\n",
    "for strategy in ['full_finetuning', 'lora', 'qlora']:\n",
    "    config = FineTuningStrategies.get_training_config(strategy)\n",
    "    print(f\"\\n{strategy.upper()}:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test LoRA module\n",
    "print(f\"\\nüß™ LoRA Module Test:\")\n",
    "original_linear = nn.Linear(512, 512)\n",
    "lora_linear = LoRALinear(original_linear, rank=4, alpha=8)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 10, 512)\n",
    "output = lora_linear(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = lora_linear.get_stats()\n",
    "print(f\"LoRA stats: {stats}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Fine-tuning strategies implemented successfully!\")\n",
    "print(f\"\\nüí° Key Interview Points:\")\n",
    "print(f\"‚Ä¢ LoRA reduces parameters by {stats['param_reduction']} while maintaining ~95% performance\")\n",
    "print(f\"‚Ä¢ QLoRA combines 4-bit quantization with LoRA for extreme efficiency\")\n",
    "print(f\"‚Ä¢ Rank selection: Higher rank = more capacity but more parameters\")\n",
    "print(f\"‚Ä¢ Alpha controls adaptation strength (usually 2x rank)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4509e53",
   "metadata": {},
   "source": [
    "## 4. RAG (Retrieval-Augmented Generation) Implementation\n",
    "\n",
    "### üîç **RAG Architecture Overview:**\n",
    "\n",
    "```\n",
    "Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Retrieve Docs ‚Üí Re-rank ‚Üí Context + Query ‚Üí LLM ‚Üí Response\n",
    "```\n",
    "\n",
    "### üìä **Key Components:**\n",
    "\n",
    "1. **Vector Database**: FAISS, Pinecone, Weaviate, Azure Cognitive Search\n",
    "2. **Embedding Models**: sentence-transformers, OpenAI embeddings, E5\n",
    "3. **Chunking Strategies**: Fixed size, semantic, recursive\n",
    "4. **Retrieval Methods**: Dense, sparse (BM25), hybrid\n",
    "5. **Re-ranking**: Cross-encoder models, relevance scoring\n",
    "\n",
    "### üéØ **Production Challenges:**\n",
    "- **Latency**: Sub-100ms retrieval for real-time applications\n",
    "- **Relevance**: Balancing recall vs precision\n",
    "- **Context Length**: Fitting retrieved docs within token limits\n",
    "- **Cost**: Embedding computation and storage costs\n",
    "- **Freshness**: Updating vector indices with new documents\n",
    "\n",
    "### üí° **Interview Focus:**\n",
    "1. **Chunking strategies** - How to split documents optimally?\n",
    "2. **Embedding quality** - Which models for which domains?\n",
    "3. **Retrieval evaluation** - How to measure retrieval quality?\n",
    "4. **Context optimization** - How to maximize relevant context?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d1134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    \"\"\"Document representation for RAG systems.\"\"\"\n",
    "    content: str\n",
    "    metadata: Dict[str, any] = None\n",
    "    doc_id: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.doc_id is None:\n",
    "            self.doc_id = hashlib.md5(self.content.encode()).hexdigest()[:8]\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    \"\"\"Result from retrieval system.\"\"\"\n",
    "    document: Document\n",
    "    score: float\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    \"\"\"\n",
    "    Advanced document chunking strategies for optimal RAG performance.\n",
    "    \n",
    "    Interview Points:\n",
    "    1. Fixed vs semantic chunking trade-offs\n",
    "    2. Overlap importance for context preservation\n",
    "    3. Chunk size optimization (token limits vs coherence)\n",
    "    4. Domain-specific chunking (code, legal, medical)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fixed_size_chunking(text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Fixed-size chunking with overlap.\n",
    "        \n",
    "        Pros: Predictable, simple, fast\n",
    "        Cons: May split sentences/paragraphs awkwardly\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk.strip():\n",
    "                chunks.append(chunk)\n",
    "                \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_chunking(text: str, max_chunk_size: int = 512) -> List[str]:\n",
    "        \"\"\"\n",
    "        Semantic chunking based on paragraphs and sentences.\n",
    "        \n",
    "        Pros: Preserves semantic coherence\n",
    "        Cons: Variable chunk sizes, more complex\n",
    "        \"\"\"\n",
    "        # Split by paragraphs first\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # Split long paragraphs by sentences\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', paragraph)\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if len((current_chunk + \" \" + sentence).split()) <= max_chunk_size:\n",
    "                    current_chunk = (current_chunk + \" \" + sentence).strip()\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    current_chunk = sentence\n",
    "            \n",
    "            # Add paragraph break\n",
    "            if current_chunk and len(current_chunk.split()) < max_chunk_size * 0.8:\n",
    "                continue\n",
    "            else:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk)\n",
    "                current_chunk = \"\"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "        return [chunk for chunk in chunks if chunk.strip()]\n",
    "    \n",
    "    @staticmethod\n",
    "    def recursive_chunking(text: str, chunk_size: int = 512, separators: List[str] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recursive chunking with hierarchical separators.\n",
    "        \n",
    "        Used in LangChain - tries larger separators first, then smaller ones.\n",
    "        \"\"\"\n",
    "        if separators is None:\n",
    "            separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        \n",
    "        def _recursive_split(text: str, separators: List[str]) -> List[str]:\n",
    "            if not separators or len(text.split()) <= chunk_size:\n",
    "                return [text] if text.strip() else []\n",
    "            \n",
    "            separator = separators[0]\n",
    "            splits = text.split(separator)\n",
    "            \n",
    "            if len(splits) == 1:\n",
    "                # Separator not found, try next one\n",
    "                return _recursive_split(text, separators[1:])\n",
    "            \n",
    "            chunks = []\n",
    "            current_chunk = \"\"\n",
    "            \n",
    "            for split in splits:\n",
    "                if len((current_chunk + separator + split).split()) <= chunk_size:\n",
    "                    current_chunk = (current_chunk + separator + split) if current_chunk else split\n",
    "                else:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    \n",
    "                    # If single split is too large, recursively split it\n",
    "                    if len(split.split()) > chunk_size:\n",
    "                        chunks.extend(_recursive_split(split, separators[1:]))\n",
    "                        current_chunk = \"\"\n",
    "                    else:\n",
    "                        current_chunk = split\n",
    "            \n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "                \n",
    "            return chunks\n",
    "        \n",
    "        return _recursive_split(text, separators)\n",
    "\n",
    "\n",
    "class VectorSearchEngine:\n",
    "    \"\"\"\n",
    "    Vector search engine using FAISS for similarity search.\n",
    "    \n",
    "    Production Features:\n",
    "    1. Efficient similarity search with FAISS\n",
    "    2. Multiple index types (Flat, IVF, HNSW)\n",
    "    3. Batch processing for large document sets\n",
    "    4. Persistent storage and loading\n",
    "    \n",
    "    Interview Topics:\n",
    "    1. Vector similarity metrics (cosine, dot product, L2)\n",
    "    2. Index types and trade-offs\n",
    "    3. Approximate vs exact search\n",
    "    4. Scaling to millions of documents\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 384, index_type: str = \"flat\"):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.index_type = index_type\n",
    "        self.index = None\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        \n",
    "        # Initialize embedding model (mock for now)\n",
    "        self.embedding_model = self._create_mock_embedding_model()\n",
    "        \n",
    "    def _create_mock_embedding_model(self):\n",
    "        \"\"\"Create mock embedding model for demonstration.\"\"\"\n",
    "        class MockEmbedder:\n",
    "            def encode(self, texts, batch_size=32):\n",
    "                # Simple mock: use text length and hash for \"embedding\"\n",
    "                embeddings = []\n",
    "                for text in texts:\n",
    "                    # Create pseudo-embedding based on text features\n",
    "                    words = text.lower().split()\n",
    "                    embedding = np.random.randn(384)  # Mock 384-dim embedding\n",
    "                    # Add some text-specific features\n",
    "                    embedding[0] = len(words) / 100  # Length feature\n",
    "                    embedding[1] = len(set(words)) / len(words) if words else 0  # Diversity\n",
    "                    embeddings.append(embedding)\n",
    "                return np.array(embeddings)\n",
    "        \n",
    "        return MockEmbedder()\n",
    "    \n",
    "    def _create_index(self, embeddings: np.ndarray):\n",
    "        \"\"\"Create FAISS index based on type.\"\"\"\n",
    "        try:\n",
    "            import faiss\n",
    "            \n",
    "            if self.index_type == \"flat\":\n",
    "                # Exact search, good for small datasets\n",
    "                index = faiss.IndexFlatIP(self.embedding_dim)  # Inner product\n",
    "            elif self.index_type == \"ivf\":\n",
    "                # Approximate search, good for large datasets\n",
    "                quantizer = faiss.IndexFlatIP(self.embedding_dim)\n",
    "                index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, 100)\n",
    "                index.train(embeddings)\n",
    "            else:\n",
    "                # Default to flat\n",
    "                index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "                \n",
    "            return index\n",
    "        except ImportError:\n",
    "            print(\"FAISS not available, using numpy-based search\")\n",
    "            return None\n",
    "    \n",
    "    def add_documents(self, documents: List[Document], batch_size: int = 32):\n",
    "        \"\"\"Add documents to the search index.\"\"\"\n",
    "        print(f\"üîÑ Adding {len(documents)} documents to vector index...\")\n",
    "        \n",
    "        # Extract text content\n",
    "        texts = [doc.content for doc in documents]\n",
    "        \n",
    "        # Generate embeddings in batches\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            batch_embeddings = self.embedding_model.encode(batch_texts)\n",
    "            all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        embeddings = np.vstack(all_embeddings)\n",
    "        \n",
    "        # Create or update index\n",
    "        if self.index is None:\n",
    "            self.index = self._create_index(embeddings)\n",
    "        \n",
    "        # Add to index\n",
    "        if self.index is not None:\n",
    "            self.index.add(embeddings)\n",
    "        \n",
    "        # Store documents and embeddings\n",
    "        self.documents.extend(documents)\n",
    "        self.embeddings.append(embeddings)\n",
    "        \n",
    "        print(f\"‚úÖ Added {len(documents)} documents. Total: {len(self.documents)}\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[RetrievalResult]:\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        if not self.documents:\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        if self.index is not None:\n",
    "            # Use FAISS search\n",
    "            scores, indices = self.index.search(\n",
    "                query_embedding.reshape(1, -1), top_k\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for score, idx in zip(scores[0], indices[0]):\n",
    "                if idx < len(self.documents):\n",
    "                    results.append(RetrievalResult(\n",
    "                        document=self.documents[idx],\n",
    "                        score=float(score)\n",
    "                    ))\n",
    "        else:\n",
    "            # Fallback to numpy search\n",
    "            all_embeddings = np.vstack(self.embeddings)\n",
    "            similarities = np.dot(all_embeddings, query_embedding)\n",
    "            top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                results.append(RetrievalResult(\n",
    "                    document=self.documents[idx],\n",
    "                    score=float(similarities[idx])\n",
    "                ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_index(self, filepath: str):\n",
    "        \"\"\"Save index and documents to disk.\"\"\"\n",
    "        if self.index is not None:\n",
    "            try:\n",
    "                import faiss\n",
    "                faiss.write_index(self.index, f\"{filepath}.index\")\n",
    "            except ImportError:\n",
    "                pass\n",
    "        \n",
    "        # Save documents and metadata\n",
    "        with open(f\"{filepath}.pkl\", 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'documents': self.documents,\n",
    "                'embeddings': self.embeddings,\n",
    "                'embedding_dim': self.embedding_dim,\n",
    "                'index_type': self.index_type\n",
    "            }, f)\n",
    "    \n",
    "    def load_index(self, filepath: str):\n",
    "        \"\"\"Load index and documents from disk.\"\"\"\n",
    "        try:\n",
    "            import faiss\n",
    "            self.index = faiss.read_index(f\"{filepath}.index\")\n",
    "        except (ImportError, FileNotFoundError):\n",
    "            pass\n",
    "        \n",
    "        # Load documents and metadata\n",
    "        with open(f\"{filepath}.pkl\", 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.documents = data['documents']\n",
    "            self.embeddings = data['embeddings']\n",
    "            self.embedding_dim = data['embedding_dim']\n",
    "            self.index_type = data['index_type']\n",
    "\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"\n",
    "    Complete RAG (Retrieval-Augmented Generation) system.\n",
    "    \n",
    "    Combines retrieval, re-ranking, and generation for enhanced LLM responses.\n",
    "    \n",
    "    Production Considerations:\n",
    "    1. Latency optimization (sub-100ms retrieval)\n",
    "    2. Context window management\n",
    "    3. Relevance scoring and filtering\n",
    "    4. Cost optimization (embedding + generation costs)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_engine: VectorSearchEngine, context_limit: int = 4000):\n",
    "        self.vector_engine = vector_engine\n",
    "        self.context_limit = context_limit\n",
    "        \n",
    "    def retrieve_and_rerank(self, query: str, top_k: int = 10, final_k: int = 3) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Two-stage retrieval: fast retrieval + careful re-ranking.\n",
    "        \n",
    "        Stage 1: Fast vector search for candidates\n",
    "        Stage 2: Cross-encoder re-ranking for relevance\n",
    "        \"\"\"\n",
    "        # Stage 1: Vector retrieval\n",
    "        candidates = self.vector_engine.search(query, top_k)\n",
    "        \n",
    "        # Stage 2: Re-ranking (simplified scoring)\n",
    "        # In production: use cross-encoder models like ms-marco-MiniLM\n",
    "        for result in candidates:\n",
    "            # Simple relevance scoring based on query overlap\n",
    "            query_words = set(query.lower().split())\n",
    "            doc_words = set(result.document.content.lower().split())\n",
    "            overlap = len(query_words.intersection(doc_words))\n",
    "            result.score = overlap / len(query_words) if query_words else 0\n",
    "        \n",
    "        # Sort by re-ranking score and return top results\n",
    "        candidates.sort(key=lambda x: x.score, reverse=True)\n",
    "        return candidates[:final_k]\n",
    "    \n",
    "    def build_context(self, query: str, retrieved_docs: List[RetrievalResult]) -> str:\n",
    "        \"\"\"\n",
    "        Build context from retrieved documents with token management.\n",
    "        \n",
    "        Strategies:\n",
    "        1. Prioritize by relevance score\n",
    "        2. Truncate documents if needed\n",
    "        3. Add source citations\n",
    "        \"\"\"\n",
    "        context_parts = [f\"Query: {query}\\\\n\\\\nRelevant Information:\\\\n\"]\n",
    "        current_length = len(context_parts[0])\n",
    "        \n",
    "        for i, result in enumerate(retrieved_docs):\n",
    "            doc_text = result.document.content\n",
    "            source_info = f\"\\\\n[Source {i+1}]: {doc_text}\\\\n\"\n",
    "            \n",
    "            if current_length + len(source_info) > self.context_limit:\n",
    "                # Truncate document to fit\n",
    "                remaining_space = self.context_limit - current_length - 20\n",
    "                if remaining_space > 100:  # Only add if meaningful space left\n",
    "                    truncated_text = doc_text[:remaining_space] + \"...\"\n",
    "                    source_info = f\"\\\\n[Source {i+1}]: {truncated_text}\\\\n\"\n",
    "                    context_parts.append(source_info)\n",
    "                break\n",
    "            \n",
    "            context_parts.append(source_info)\n",
    "            current_length += len(source_info)\n",
    "        \n",
    "        return \"\".join(context_parts)\n",
    "    \n",
    "    def generate_response(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context.\n",
    "        \n",
    "        In production: Use OpenAI API, Azure OpenAI, or local LLM\n",
    "        \"\"\"\n",
    "        # Mock response generation\n",
    "        response = f\"Based on the retrieved information:\\\\n\\\\n\"\n",
    "        response += f\"Answer: This is a mock response to '{query}' using the provided context. \"\n",
    "        response += f\"In production, this would be generated by an LLM like GPT-4 or Claude.\\\\n\\\\n\"\n",
    "        response += f\"Context used: {len(context)} characters from retrieved documents.\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Complete RAG pipeline: retrieve, rerank, generate.\n",
    "        \"\"\"\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        retrieved_docs = self.retrieve_and_rerank(question, top_k)\n",
    "        \n",
    "        # Step 2: Build context\n",
    "        context = self.build_context(question, retrieved_docs)\n",
    "        \n",
    "        # Step 3: Generate response\n",
    "        response = self.generate_response(question, context)\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'response': response,\n",
    "            'retrieved_documents': retrieved_docs,\n",
    "            'context': context,\n",
    "            'metadata': {\n",
    "                'num_retrieved': len(retrieved_docs),\n",
    "                'context_length': len(context),\n",
    "                'avg_relevance_score': np.mean([r.score for r in retrieved_docs]) if retrieved_docs else 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Demonstrate RAG system\n",
    "print(\"üîç RAG SYSTEM DEMONSTRATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample documents\n",
    "sample_documents = [\n",
    "    Document(\"Transformers use self-attention mechanisms to process sequences in parallel, making them much faster than RNNs for training.\"),\n",
    "    Document(\"LoRA (Low-Rank Adaptation) allows efficient fine-tuning by learning low-rank matrices that approximate weight updates.\"),\n",
    "    Document(\"BERT uses bidirectional attention while GPT uses causal (unidirectional) attention for different tasks.\"),\n",
    "    Document(\"Vector databases store high-dimensional embeddings and enable fast similarity search for RAG applications.\"),\n",
    "    Document(\"Fine-tuning strategies include full fine-tuning, LoRA, QLoRA, and prefix tuning, each with different trade-offs.\"),\n",
    "    Document(\"Attention complexity is O(n¬≤) where n is sequence length, which becomes problematic for very long sequences.\"),\n",
    "    Document(\"RAG combines retrieval and generation to provide factual, up-to-date responses by grounding LLMs in external knowledge.\"),\n",
    "]\n",
    "\n",
    "# Test chunking strategies\n",
    "print(\"\\\\nüìã CHUNKING STRATEGIES COMPARISON:\")\n",
    "sample_text = \" \".join([doc.content for doc in sample_documents])\n",
    "\n",
    "chunker = DocumentChunker()\n",
    "fixed_chunks = chunker.fixed_size_chunking(sample_text, chunk_size=50, overlap=10)\n",
    "semantic_chunks = chunker.semantic_chunking(sample_text, max_chunk_size=50)\n",
    "recursive_chunks = chunker.recursive_chunking(sample_text, chunk_size=50)\n",
    "\n",
    "print(f\"Original text length: {len(sample_text.split())} words\")\n",
    "print(f\"Fixed chunking: {len(fixed_chunks)} chunks\")\n",
    "print(f\"Semantic chunking: {len(semantic_chunks)} chunks\")\n",
    "print(f\"Recursive chunking: {len(recursive_chunks)} chunks\")\n",
    "\n",
    "# Create and populate vector search engine\n",
    "print(\"\\\\nüîß VECTOR SEARCH ENGINE:\")\n",
    "vector_engine = VectorSearchEngine(embedding_dim=384)\n",
    "vector_engine.add_documents(sample_documents)\n",
    "\n",
    "# Test search\n",
    "test_query = \"What is LoRA and how does it work?\"\n",
    "search_results = vector_engine.search(test_query, top_k=3)\n",
    "\n",
    "print(f\"\\\\nQuery: '{test_query}'\")\n",
    "print(\"Search Results:\")\n",
    "for i, result in enumerate(search_results):\n",
    "    print(f\"{i+1}. Score: {result.score:.3f}\")\n",
    "    print(f\"   Content: {result.document.content[:100]}...\")\n",
    "\n",
    "# Create and test RAG system\n",
    "print(\"\\\\nüß† COMPLETE RAG SYSTEM:\")\n",
    "rag_system = RAGSystem(vector_engine)\n",
    "\n",
    "rag_result = rag_system.query(\"Explain attention mechanisms in transformers\")\n",
    "print(f\"\\\\nRAG Query: {rag_result['question']}\")\n",
    "print(f\"Response: {rag_result['response']}\")\n",
    "print(f\"\\\\nMetadata:\")\n",
    "for key, value in rag_result['metadata'].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ RAG system implemented successfully!\")\n",
    "print(f\"\\\\nüí° Production Considerations:\")\n",
    "print(f\"‚Ä¢ Embedding model choice: sentence-transformers vs OpenAI vs domain-specific\")\n",
    "print(f\"‚Ä¢ Vector database scaling: FAISS vs Pinecone vs Weaviate\")\n",
    "print(f\"‚Ä¢ Chunking optimization: Balance coherence vs retrieval accuracy\")\n",
    "print(f\"‚Ä¢ Re-ranking models: Cross-encoders for better relevance\")\n",
    "print(f\"‚Ä¢ Context management: Token limits vs information completeness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bd24c",
   "metadata": {},
   "source": [
    "## 5. Prompt Engineering & Template Systems\n",
    "\n",
    "### üìù **Prompt Engineering Fundamentals:**\n",
    "\n",
    "**Prompt Types:**\n",
    "1. **Zero-shot**: No examples, just instruction\n",
    "2. **Few-shot**: Include examples in the prompt\n",
    "3. **Chain-of-Thought (CoT)**: Step-by-step reasoning\n",
    "4. **Self-Consistency**: Multiple reasoning paths\n",
    "5. **Tree of Thoughts**: Explore multiple solution branches\n",
    "\n",
    "### üéØ **Prompt Design Principles:**\n",
    "\n",
    "| Principle | Description | Example |\n",
    "|-----------|-------------|---------|\n",
    "| **Clarity** | Clear, specific instructions | \"Summarize in 3 bullet points\" vs \"Summarize\" |\n",
    "| **Context** | Provide relevant background | Include domain knowledge, constraints |\n",
    "| **Examples** | Show desired output format | Few-shot demonstrations |\n",
    "| **Structure** | Organize with headers, sections | Use markdown, numbered steps |\n",
    "| **Constraints** | Specify limits, requirements | Word count, format, style |\n",
    "\n",
    "### üîß **Advanced Techniques:**\n",
    "- **Role Playing**: \"You are an expert data scientist...\"\n",
    "- **Metacognitive Prompting**: \"Think step by step\", \"Explain your reasoning\"\n",
    "- **Self-Correction**: \"Review your answer and fix any errors\"\n",
    "- **Multi-Turn Conversations**: Context-aware dialogue systems\n",
    "\n",
    "### üöÄ **Production Considerations:**\n",
    "- **Token Efficiency**: Minimize prompt length while maintaining quality\n",
    "- **Template Versioning**: A/B testing different prompt versions\n",
    "- **Dynamic Context**: Adaptive prompts based on user/task\n",
    "- **Safety Filtering**: Prompt injection prevention, content moderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6931863b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     TREE_OF_THOUGHTS = \u001b[33m\"\u001b[39m\u001b[33mtree_of_thoughts\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     ROLE_PLAYING = \u001b[33m\"\u001b[39m\u001b[33mrole_playing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPromptExample\u001b[39;00m:\n\u001b[32m     19\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Example for few-shot prompting.\"\"\"\u001b[39;00m\n\u001b[32m     20\u001b[39m     input_text: \u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from jinja2 import Template\n",
    "import re\n",
    "from typing import List, Dict, Optional, Union, Callable\n",
    "import time\n",
    "import json\n",
    "\n",
    "class PromptType(Enum):\n",
    "    \"\"\"Different types of prompting strategies.\"\"\"\n",
    "    ZERO_SHOT = \"zero_shot\"\n",
    "    FEW_SHOT = \"few_shot\"\n",
    "    CHAIN_OF_THOUGHT = \"chain_of_thought\"\n",
    "    SELF_CONSISTENCY = \"self_consistency\"\n",
    "    TREE_OF_THOUGHTS = \"tree_of_thoughts\"\n",
    "    ROLE_PLAYING = \"role_playing\"\n",
    "\n",
    "@dataclass\n",
    "class PromptExample:\n",
    "    \"\"\"Example for few-shot prompting.\"\"\"\n",
    "    input_text: str\n",
    "    output_text: str\n",
    "    reasoning: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    \"\"\"Template for consistent prompt generation.\"\"\"\n",
    "    name: str\n",
    "    template: str\n",
    "    prompt_type: PromptType\n",
    "    examples: List[PromptExample] = None\n",
    "    metadata: Dict[str, any] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.examples is None:\n",
    "            self.examples = []\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "\n",
    "class PromptTemplateEngine:\n",
    "    \"\"\"\n",
    "    Advanced prompt template system with dynamic content generation.\n",
    "    \n",
    "    Features:\n",
    "    1. Template inheritance and composition\n",
    "    2. Dynamic variable injection\n",
    "    3. Conditional logic\n",
    "    4. Multi-language support\n",
    "    5. A/B testing support\n",
    "    \n",
    "    Interview Points:\n",
    "    1. Template design patterns\n",
    "    2. Variable sanitization and injection\n",
    "    3. Performance optimization\n",
    "    4. Version control for prompts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {}\n",
    "        self.base_templates = {}\n",
    "        self.evaluation_metrics = {}\n",
    "        \n",
    "    def register_template(self, template: PromptTemplate):\n",
    "        \"\"\"Register a new prompt template.\"\"\"\n",
    "        self.templates[template.name] = template\n",
    "        \n",
    "    def get_zero_shot_template(self, task_description: str, constraints: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate zero-shot prompt template.\n",
    "        \n",
    "        Structure:\n",
    "        1. Task description\n",
    "        2. Constraints and requirements\n",
    "        3. Output format specification\n",
    "        \"\"\"\n",
    "        template_parts = [\n",
    "            f\"Task: {task_description}\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        if constraints:\n",
    "            template_parts.extend([\n",
    "                \"Requirements:\",\n",
    "                *[f\"- {constraint}\" for constraint in constraints],\n",
    "                \"\"\n",
    "            ])\n",
    "        \n",
    "        template_parts.extend([\n",
    "            \"Input: {{input}}\",\n",
    "            \"\",\n",
    "            \"Output:\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\\\n\".join(template_parts)\n",
    "    \n",
    "    def get_few_shot_template(self, task_description: str, examples: List[PromptExample], \n",
    "                            constraints: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate few-shot prompt with examples.\n",
    "        \n",
    "        Best Practices:\n",
    "        1. 2-5 diverse examples\n",
    "        2. Consistent format across examples\n",
    "        3. Include edge cases\n",
    "        4. Show reasoning if applicable\n",
    "        \"\"\"\n",
    "        template_parts = [\n",
    "            f\"Task: {task_description}\",\n",
    "            \"\"\n",
    "        ]\n",
    "        \n",
    "        if constraints:\n",
    "            template_parts.extend([\n",
    "                \"Requirements:\",\n",
    "                *[f\"- {constraint}\" for constraint in constraints],\n",
    "                \"\"\n",
    "            ])\n",
    "        \n",
    "        # Add examples\n",
    "        template_parts.append(\"Examples:\")\n",
    "        for i, example in enumerate(examples, 1):\n",
    "            template_parts.extend([\n",
    "                f\"Example {i}:\",\n",
    "                f\"Input: {example.input_text}\",\n",
    "                f\"Output: {example.output_text}\"\n",
    "            ])\n",
    "            if example.reasoning:\n",
    "                template_parts.append(f\"Reasoning: {example.reasoning}\")\n",
    "            template_parts.append(\"\")\n",
    "        \n",
    "        # Add current input\n",
    "        template_parts.extend([\n",
    "            \"Now, please complete this task:\",\n",
    "            \"Input: {{input}}\",\n",
    "            \"Output:\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\\\n\".join(template_parts)\n",
    "    \n",
    "    def get_chain_of_thought_template(self, task_description: str, \n",
    "                                    reasoning_structure: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate Chain-of-Thought prompting template.\n",
    "        \n",
    "        Encourages step-by-step reasoning for complex problems.\n",
    "        \"\"\"\n",
    "        if reasoning_structure is None:\n",
    "            reasoning_structure = [\n",
    "                \"1. Understand the problem\",\n",
    "                \"2. Identify key information\", \n",
    "                \"3. Apply relevant knowledge\",\n",
    "                \"4. Reason through step-by-step\",\n",
    "                \"5. Provide final answer\"\n",
    "            ]\n",
    "        \n",
    "        template_parts = [\n",
    "            f\"Task: {task_description}\",\n",
    "            \"\",\n",
    "            \"Please solve this step-by-step by following this reasoning structure:\",\n",
    "            *reasoning_structure,\n",
    "            \"\",\n",
    "            \"Input: {{input}}\",\n",
    "            \"\",\n",
    "            \"Step-by-step solution:\"\n",
    "        ]\n",
    "        \n",
    "        return \"\\\\n\".join(template_parts)\n",
    "    \n",
    "    def get_role_playing_template(self, role: str, expertise: str, \n",
    "                                task_description: str, context: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate role-playing prompt template.\n",
    "        \n",
    "        Helps the model adopt specific perspectives and expertise.\n",
    "        \"\"\"\n",
    "        template_parts = [\n",
    "            f\"You are {role} with expertise in {expertise}.\",\n",
    "        ]\n",
    "        \n",
    "        if context:\n",
    "            template_parts.append(f\"Context: {context}\")\n",
    "        \n",
    "        template_parts.extend([\n",
    "            \"\",\n",
    "            f\"Task: {task_description}\",\n",
    "            \"\",\n",
    "            \"Please respond from your professional perspective, drawing on your expertise.\",\n",
    "            \"\",\n",
    "            \"Input: {{input}}\",\n",
    "            \"\",\n",
    "            \"Response:\"\n",
    "        ])\n",
    "        \n",
    "        return \"\\\\n\".join(template_parts)\n",
    "    \n",
    "    def render_template(self, template_name: str, variables: Dict[str, any]) -> str:\n",
    "        \"\"\"\n",
    "        Render template with variables using Jinja2.\n",
    "        \n",
    "        Supports:\n",
    "        1. Variable substitution\n",
    "        2. Conditional logic\n",
    "        3. Loops and iterations\n",
    "        4. Filters and functions\n",
    "        \"\"\"\n",
    "        if template_name not in self.templates:\n",
    "            raise ValueError(f\"Template '{template_name}' not found\")\n",
    "        \n",
    "        template_obj = self.templates[template_name]\n",
    "        jinja_template = Template(template_obj.template)\n",
    "        \n",
    "        # Add safety checks for variables\n",
    "        safe_variables = self._sanitize_variables(variables)\n",
    "        \n",
    "        return jinja_template.render(**safe_variables)\n",
    "    \n",
    "    def _sanitize_variables(self, variables: Dict[str, any]) -> Dict[str, any]:\n",
    "        \"\"\"Sanitize template variables to prevent injection attacks.\"\"\"\n",
    "        safe_vars = {}\n",
    "        \n",
    "        for key, value in variables.items():\n",
    "            if isinstance(value, str):\n",
    "                # Basic sanitization - remove potentially dangerous patterns\n",
    "                safe_value = re.sub(r'[{}\\\\\\\\]', '', value)  # Remove template syntax\n",
    "                safe_value = safe_value.strip()\n",
    "                safe_vars[key] = safe_value\n",
    "            else:\n",
    "                safe_vars[key] = value\n",
    "                \n",
    "        return safe_vars\n",
    "\n",
    "\n",
    "class PromptOptimizer:\n",
    "    \"\"\"\n",
    "    Advanced prompt optimization and evaluation system.\n",
    "    \n",
    "    Features:\n",
    "    1. A/B testing for prompt variants\n",
    "    2. Performance metrics tracking\n",
    "    3. Automatic prompt refinement\n",
    "    4. Cost optimization\n",
    "    \n",
    "    Interview Topics:\n",
    "    1. How to measure prompt quality?\n",
    "    2. Automated prompt engineering\n",
    "    3. Cost vs performance trade-offs\n",
    "    4. Multi-objective optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.experiments = {}\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def create_experiment(self, name: str, prompt_variants: List[str], \n",
    "                        evaluation_function: Callable) -> str:\n",
    "        \"\"\"\n",
    "        Create A/B testing experiment for prompt optimization.\n",
    "        \n",
    "        Args:\n",
    "            name: Experiment identifier\n",
    "            prompt_variants: List of prompt templates to test\n",
    "            evaluation_function: Function to evaluate prompt performance\n",
    "        \"\"\"\n",
    "        experiment_id = f\"{name}_{int(time.time())}\"\n",
    "        \n",
    "        self.experiments[experiment_id] = {\n",
    "            'name': name,\n",
    "            'variants': prompt_variants,\n",
    "            'evaluator': evaluation_function,\n",
    "            'results': [],\n",
    "            'best_variant': None,\n",
    "            'created_at': time.time()\n",
    "        }\n",
    "        \n",
    "        return experiment_id\n",
    "    \n",
    "    def evaluate_prompt_variant(self, experiment_id: str, variant_idx: int, \n",
    "                              test_inputs: List[str], expected_outputs: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a specific prompt variant.\n",
    "        \n",
    "        Metrics:\n",
    "        1. Task completion rate\n",
    "        2. Output quality score\n",
    "        3. Latency\n",
    "        4. Token usage (cost)\n",
    "        \"\"\"\n",
    "        if experiment_id not in self.experiments:\n",
    "            raise ValueError(f\"Experiment {experiment_id} not found\")\n",
    "        \n",
    "        experiment = self.experiments[experiment_id]\n",
    "        prompt_template = experiment['variants'][variant_idx]\n",
    "        \n",
    "        results = {\n",
    "            'variant_idx': variant_idx,\n",
    "            'prompt_template': prompt_template,\n",
    "            'test_results': [],\n",
    "            'avg_score': 0,\n",
    "            'avg_latency': 0,\n",
    "            'total_tokens': 0\n",
    "        }\n",
    "        \n",
    "        # Test each input\n",
    "        for i, test_input in enumerate(test_inputs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Mock LLM response (in production: call actual LLM)\n",
    "            mock_output = self._mock_llm_response(prompt_template, test_input)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "            \n",
    "            # Evaluate response quality\n",
    "            if expected_outputs and i < len(expected_outputs):\n",
    "                quality_score = self._calculate_quality_score(mock_output, expected_outputs[i])\n",
    "            else:\n",
    "                quality_score = self._evaluate_response_quality(mock_output)\n",
    "            \n",
    "            # Count tokens (mock)\n",
    "            token_count = len(prompt_template.split()) + len(mock_output.split())\n",
    "            \n",
    "            test_result = {\n",
    "                'input': test_input,\n",
    "                'output': mock_output,\n",
    "                'quality_score': quality_score,\n",
    "                'latency': latency,\n",
    "                'tokens': token_count\n",
    "            }\n",
    "            \n",
    "            results['test_results'].append(test_result)\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        results['avg_score'] = np.mean([r['quality_score'] for r in results['test_results']])\n",
    "        results['avg_latency'] = np.mean([r['latency'] for r in results['test_results']])\n",
    "        results['total_tokens'] = sum([r['tokens'] for r in results['test_results']])\n",
    "        \n",
    "        # Store results\n",
    "        experiment['results'].append(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _mock_llm_response(self, prompt_template: str, user_input: str) -> str:\n",
    "        \"\"\"Mock LLM response for demonstration.\"\"\"\n",
    "        # Simple mock based on prompt complexity\n",
    "        template_length = len(prompt_template.split())\n",
    "        input_length = len(user_input.split())\n",
    "        \n",
    "        response_parts = []\n",
    "        \n",
    "        if \"step-by-step\" in prompt_template.lower():\n",
    "            response_parts.extend([\n",
    "                \"Let me think through this step by step:\",\n",
    "                f\"1. First, I'll analyze: {user_input[:50]}...\",\n",
    "                \"2. Then I'll consider the requirements...\",\n",
    "                \"3. Finally, I'll provide the solution:\",\n",
    "                f\"Based on the analysis, here's my response to '{user_input[:30]}...'\"\n",
    "            ])\n",
    "        elif \"examples\" in prompt_template.lower():\n",
    "            response_parts.append(f\"Following the examples provided, my response to '{user_input[:30]}...' is:\")\n",
    "            response_parts.append(\"This follows the same pattern as the examples.\")\n",
    "        else:\n",
    "            response_parts.append(f\"Response to '{user_input[:50]}...'\")\n",
    "        \n",
    "        return \" \".join(response_parts)\n",
    "    \n",
    "    def _calculate_quality_score(self, output: str, expected: str) -> float:\n",
    "        \"\"\"Calculate quality score by comparing output to expected result.\"\"\"\n",
    "        # Simple similarity score (in production: use BLEU, ROUGE, etc.)\n",
    "        output_words = set(output.lower().split())\n",
    "        expected_words = set(expected.lower().split())\n",
    "        \n",
    "        if not expected_words:\n",
    "            return 0.0\n",
    "        \n",
    "        overlap = len(output_words.intersection(expected_words))\n",
    "        return overlap / len(expected_words)\n",
    "    \n",
    "    def _evaluate_response_quality(self, output: str) -> float:\n",
    "        \"\"\"Evaluate response quality without ground truth.\"\"\"\n",
    "        # Simple heuristics (in production: use trained evaluator models)\n",
    "        score = 0.5  # Base score\n",
    "        \n",
    "        # Check for structure\n",
    "        if any(marker in output.lower() for marker in ['step', 'first', 'second', 'finally']):\n",
    "            score += 0.2\n",
    "        \n",
    "        # Check for length appropriateness\n",
    "        if 20 <= len(output.split()) <= 200:\n",
    "            score += 0.2\n",
    "        \n",
    "        # Check for coherence (simple)\n",
    "        if len(set(output.lower().split())) / len(output.split()) > 0.5:  # Diversity\n",
    "            score += 0.1\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def get_best_variant(self, experiment_id: str) -> Dict:\n",
    "        \"\"\"Get the best performing prompt variant from experiment.\"\"\"\n",
    "        if experiment_id not in self.experiments:\n",
    "            raise ValueError(f\"Experiment {experiment_id} not found\")\n",
    "        \n",
    "        experiment = self.experiments[experiment_id]\n",
    "        \n",
    "        if not experiment['results']:\n",
    "            return None\n",
    "        \n",
    "        # Find best variant based on composite score\n",
    "        best_variant = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for result in experiment['results']:\n",
    "            # Composite score: quality weighted by efficiency\n",
    "            composite_score = (\n",
    "                result['avg_score'] * 0.6 +  # Quality weight\n",
    "                (1 / (result['avg_latency'] + 0.001)) * 0.2 +  # Speed weight\n",
    "                (1 / (result['total_tokens'] / len(result['test_results']) + 1)) * 0.2  # Efficiency weight\n",
    "            )\n",
    "            \n",
    "            if composite_score > best_score:\n",
    "                best_score = composite_score\n",
    "                best_variant = result\n",
    "        \n",
    "        experiment['best_variant'] = best_variant\n",
    "        return best_variant\n",
    "\n",
    "\n",
    "# Comprehensive demonstration\n",
    "print(\"üìù PROMPT ENGINEERING & TEMPLATE SYSTEMS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize template engine\n",
    "template_engine = PromptTemplateEngine()\n",
    "\n",
    "# Create different types of templates\n",
    "print(\"\\\\nüèóÔ∏è CREATING PROMPT TEMPLATES:\")\n",
    "\n",
    "# 1. Zero-shot template\n",
    "zero_shot_prompt = template_engine.get_zero_shot_template(\n",
    "    task_description=\"Classify the sentiment of the given text\",\n",
    "    constraints=[\"Output only: Positive, Negative, or Neutral\", \"Provide confidence score\"]\n",
    ")\n",
    "\n",
    "# 2. Few-shot template  \n",
    "examples = [\n",
    "    PromptExample(\"I love this product!\", \"Positive (0.95)\", \"Strong positive emotion expressed\"),\n",
    "    PromptExample(\"It's okay, nothing special\", \"Neutral (0.70)\", \"Mixed sentiment, neither strongly positive nor negative\"),\n",
    "    PromptExample(\"Terrible experience, would not recommend\", \"Negative (0.90)\", \"Clear negative sentiment with recommendation against\")\n",
    "]\n",
    "\n",
    "few_shot_prompt = template_engine.get_few_shot_template(\n",
    "    task_description=\"Classify the sentiment of the given text with confidence score\",\n",
    "    examples=examples,\n",
    "    constraints=[\"Output format: Sentiment (confidence)\", \"Provide brief reasoning\"]\n",
    ")\n",
    "\n",
    "# 3. Chain-of-thought template\n",
    "cot_prompt = template_engine.get_chain_of_thought_template(\n",
    "    task_description=\"Analyze the sentiment and emotional tone of text\",\n",
    "    reasoning_structure=[\n",
    "        \"1. Identify key emotional words and phrases\",\n",
    "        \"2. Consider context and implicit meanings\", \n",
    "        \"3. Evaluate overall sentiment polarity\",\n",
    "        \"4. Assess confidence based on clarity of sentiment\",\n",
    "        \"5. Provide final classification with reasoning\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Role-playing template\n",
    "role_prompt = template_engine.get_role_playing_template(\n",
    "    role=\"an expert sentiment analysis researcher\",\n",
    "    expertise=\"natural language processing and computational linguistics\",\n",
    "    task_description=\"Analyze the sentiment with academic rigor\",\n",
    "    context=\"You have published papers on sentiment analysis and emotion detection\"\n",
    ")\n",
    "\n",
    "# Register templates\n",
    "templates = [\n",
    "    PromptTemplate(\"sentiment_zero_shot\", zero_shot_prompt, PromptType.ZERO_SHOT),\n",
    "    PromptTemplate(\"sentiment_few_shot\", few_shot_prompt, PromptType.FEW_SHOT, examples),\n",
    "    PromptTemplate(\"sentiment_cot\", cot_prompt, PromptType.CHAIN_OF_THOUGHT),\n",
    "    PromptTemplate(\"sentiment_role\", role_prompt, PromptType.ROLE_PLAYING)\n",
    "]\n",
    "\n",
    "for template in templates:\n",
    "    template_engine.register_template(template)\n",
    "\n",
    "print(f\"‚úÖ Created {len(templates)} template variants\")\n",
    "\n",
    "# Test template rendering\n",
    "test_input = \"The movie was absolutely fantastic! Great acting and storyline.\"\n",
    "\n",
    "print(\"\\\\nüß™ TEMPLATE RENDERING EXAMPLES:\")\n",
    "for template in templates:\n",
    "    print(f\"\\\\n--- {template.name.upper()} ---\")\n",
    "    try:\n",
    "        rendered = template_engine.render_template(template.name, {\"input\": test_input})\n",
    "        print(rendered[:200] + \"...\" if len(rendered) > 200 else rendered)\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering template: {e}\")\n",
    "\n",
    "# Initialize prompt optimizer and run experiments\n",
    "print(\"\\\\n‚ö° PROMPT OPTIMIZATION EXPERIMENT:\")\n",
    "optimizer = PromptOptimizer()\n",
    "\n",
    "# Create experiment with template variants\n",
    "prompt_variants = [\n",
    "    zero_shot_prompt,\n",
    "    few_shot_prompt, \n",
    "    cot_prompt,\n",
    "    role_prompt\n",
    "]\n",
    "\n",
    "def mock_evaluator(output: str, expected: str = None) -> float:\n",
    "    \"\"\"Mock evaluation function for demonstration.\"\"\"\n",
    "    return np.random.uniform(0.6, 0.95)  # Mock quality scores\n",
    "\n",
    "experiment_id = optimizer.create_experiment(\n",
    "    name=\"sentiment_analysis_optimization\",\n",
    "    prompt_variants=prompt_variants,\n",
    "    evaluation_function=mock_evaluator\n",
    ")\n",
    "\n",
    "# Test inputs for evaluation\n",
    "test_inputs = [\n",
    "    \"I absolutely love this new smartphone!\",\n",
    "    \"The service was disappointing and slow.\",\n",
    "    \"It's an average product, nothing extraordinary.\",\n",
    "    \"Best purchase I've made this year!\",\n",
    "    \"Could be better, but it's acceptable.\"\n",
    "]\n",
    "\n",
    "# Evaluate each variant\n",
    "print(f\"\\\\nEvaluating {len(prompt_variants)} prompt variants...\")\n",
    "for i in range(len(prompt_variants)):\n",
    "    result = optimizer.evaluate_prompt_variant(experiment_id, i, test_inputs)\n",
    "    print(f\"\\\\nVariant {i+1} Results:\")\n",
    "    print(f\"  Average Quality Score: {result['avg_score']:.3f}\")\n",
    "    print(f\"  Average Latency: {result['avg_latency']:.3f}s\")\n",
    "    print(f\"  Total Tokens: {result['total_tokens']}\")\n",
    "\n",
    "# Get best variant\n",
    "best_variant = optimizer.get_best_variant(experiment_id)\n",
    "if best_variant:\n",
    "    print(f\"\\\\nüèÜ BEST PERFORMING VARIANT:\")\n",
    "    print(f\"Variant Index: {best_variant['variant_idx']}\")\n",
    "    print(f\"Quality Score: {best_variant['avg_score']:.3f}\")\n",
    "    print(f\"Efficiency: {best_variant['total_tokens'] / len(test_inputs):.1f} tokens/query\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Prompt engineering system implemented successfully!\")\n",
    "\n",
    "print(f\"\\\\nüí° Key Interview Points:\")\n",
    "print(f\"‚Ä¢ Zero-shot vs Few-shot: Trade-off between simplicity and performance\")\n",
    "print(f\"‚Ä¢ Chain-of-Thought: Improves reasoning for complex tasks by 10-50%\")\n",
    "print(f\"‚Ä¢ Role-playing: Leverages model's training on diverse perspectives\")\n",
    "print(f\"‚Ä¢ A/B testing: Essential for production prompt optimization\")\n",
    "print(f\"‚Ä¢ Token efficiency: Cost optimization through smart prompt design\")\n",
    "print(f\"‚Ä¢ Safety considerations: Prompt injection prevention, content filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf518440",
   "metadata": {},
   "source": [
    "## 6. Production Deployment & Azure Integration\n",
    "\n",
    "### üöÄ **Deployment Architecture for Transformers:**\n",
    "\n",
    "```\n",
    "Training Pipeline:    [Data] ‚Üí [Fine-tuning] ‚Üí [Evaluation] ‚Üí [Model Registry]\n",
    "                                     ‚Üì\n",
    "Inference Pipeline:   [Model Serving] ‚Üí [API Gateway] ‚Üí [Load Balancer] ‚Üí [Monitoring]\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è **Azure Services for ML Production:**\n",
    "\n",
    "| Service | Purpose | Use Case |\n",
    "|---------|---------|----------|\n",
    "| **Azure OpenAI** | Managed LLM inference | GPT-4, ChatGPT integration |\n",
    "| **Azure ML** | End-to-end ML lifecycle | Training, deployment, monitoring |\n",
    "| **Azure Cognitive Services** | Pre-built AI APIs | Vision, Speech, Language |\n",
    "| **Azure Container Instances** | Serverless containers | Lightweight model serving |\n",
    "| **Azure Kubernetes Service** | Container orchestration | Scalable, production workloads |\n",
    "| **Azure Functions** | Serverless compute | Event-driven ML inference |\n",
    "\n",
    "### üîß **Model Optimization Techniques:**\n",
    "\n",
    "1. **Quantization**: 8-bit, 4-bit, INT8 for memory reduction\n",
    "2. **Pruning**: Remove unnecessary weights and connections  \n",
    "3. **Knowledge Distillation**: Compress large models to smaller ones\n",
    "4. **ONNX Conversion**: Cross-platform optimized inference\n",
    "5. **TensorRT/DirectML**: Hardware-specific acceleration\n",
    "\n",
    "### üìä **Production Metrics:**\n",
    "- **Latency**: P50, P95, P99 response times\n",
    "- **Throughput**: Requests/second, tokens/second\n",
    "- **Resource Usage**: GPU/CPU utilization, memory\n",
    "- **Cost**: Compute cost per request/token\n",
    "- **Quality**: BLEU, ROUGE, human evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e4d57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "import queue\n",
    "import psutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Mock Azure SDK imports for demonstration\n",
    "class MockAzureMLClient:\n",
    "    \"\"\"Mock Azure ML client for demonstration purposes.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.subscription_id = kwargs.get('subscription_id', 'mock-subscription')\n",
    "        self.resource_group = kwargs.get('resource_group', 'mock-rg')\n",
    "        \n",
    "    def create_or_update(self, workspace):\n",
    "        print(f\"‚úÖ Mock: Created workspace {workspace.name}\")\n",
    "        \n",
    "    def begin_create_or_update(self, deployment):\n",
    "        print(f\"‚úÖ Mock: Deployed model {deployment.name}\")\n",
    "        return MockDeployment()\n",
    "\n",
    "class MockDeployment:\n",
    "    def result(self):\n",
    "        return {\"status\": \"Succeeded\", \"scoring_uri\": \"https://mock-endpoint.azure.com/score\"}\n",
    "\n",
    "\n",
    "class ModelOptimizer:\n",
    "    \"\"\"\n",
    "    Model optimization techniques for production deployment.\n",
    "    \n",
    "    Key Techniques:\n",
    "    1. Quantization: Reduce precision (FP32 ‚Üí INT8)\n",
    "    2. Pruning: Remove redundant parameters\n",
    "    3. Knowledge Distillation: Compress large models\n",
    "    4. ONNX Conversion: Hardware-agnostic format\n",
    "    \n",
    "    Interview Points:\n",
    "    1. Trade-offs: Model size vs accuracy\n",
    "    2. Hardware considerations: CPU vs GPU optimization\n",
    "    3. Inference speedup: 2-10x improvement possible\n",
    "    4. Memory reduction: Up to 75% with quantization\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_model(model, quantization_type='dynamic'):\n",
    "        \"\"\"\n",
    "        Quantize model for reduced memory and faster inference.\n",
    "        \n",
    "        Types:\n",
    "        - dynamic: Post-training quantization (easiest)\n",
    "        - static: Requires calibration dataset (better accuracy)\n",
    "        - qat: Quantization-aware training (best accuracy)\n",
    "        \"\"\"\n",
    "        print(f\"üîß Quantizing model with {quantization_type} quantization...\")\n",
    "        \n",
    "        try:\n",
    "            import torch.quantization as quantization\n",
    "            \n",
    "            if quantization_type == 'dynamic':\n",
    "                # Dynamic quantization - good for LSTM/RNN\n",
    "                quantized_model = torch.quantization.quantize_dynamic(\n",
    "                    model, {torch.nn.Linear, torch.nn.LSTM}, dtype=torch.qint8\n",
    "                )\n",
    "            elif quantization_type == 'static':\n",
    "                # Static quantization - requires calibration\n",
    "                model.eval()\n",
    "                model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "                quantized_model = torch.quantization.prepare(model)\n",
    "                # Would need calibration data here\n",
    "                quantized_model = torch.quantization.convert(quantized_model)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è QAT requires specialized training setup\")\n",
    "                return model\n",
    "                \n",
    "            # Calculate compression ratio\n",
    "            original_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "            quantized_size = sum(p.numel() * p.element_size() for p in quantized_model.parameters())\n",
    "            compression_ratio = original_size / quantized_size\n",
    "            \n",
    "            print(f\"‚úÖ Quantization complete. Compression ratio: {compression_ratio:.2f}x\")\n",
    "            return quantized_model\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è PyTorch quantization not available - using mock quantization\")\n",
    "            return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def prune_model(model, pruning_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Prune model by removing low-magnitude weights.\n",
    "        \n",
    "        Techniques:\n",
    "        1. Magnitude-based: Remove smallest weights\n",
    "        2. Structured: Remove entire channels/layers\n",
    "        3. Gradual: Iterative pruning during training\n",
    "        \"\"\"\n",
    "        print(f\"‚úÇÔ∏è Pruning model with {pruning_ratio:.1%} sparsity...\")\n",
    "        \n",
    "        try:\n",
    "            import torch.nn.utils.prune as prune\n",
    "            \n",
    "            # Apply magnitude-based pruning to linear layers\n",
    "            for name, module in model.named_modules():\n",
    "                if isinstance(module, torch.nn.Linear):\n",
    "                    prune.l1_unstructured(module, name='weight', amount=pruning_ratio)\n",
    "                    prune.remove(module, 'weight')  # Make pruning permanent\n",
    "            \n",
    "            # Calculate actual sparsity\n",
    "            total_params = sum(p.numel() for p in model.parameters())\n",
    "            zero_params = sum((p == 0).sum().item() for p in model.parameters())\n",
    "            actual_sparsity = zero_params / total_params\n",
    "            \n",
    "            print(f\"‚úÖ Pruning complete. Actual sparsity: {actual_sparsity:.1%}\")\n",
    "            return model\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è PyTorch pruning not available - using mock pruning\")\n",
    "            return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_to_onnx(model, input_shape, output_path):\n",
    "        \"\"\"\n",
    "        Convert PyTorch model to ONNX for cross-platform deployment.\n",
    "        \n",
    "        Benefits:\n",
    "        1. Hardware optimization (CPU, GPU, specialized chips)\n",
    "        2. Runtime optimization (ONNX Runtime, TensorRT)\n",
    "        3. Cross-framework compatibility\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Converting model to ONNX format...\")\n",
    "        \n",
    "        try:\n",
    "            import torch.onnx\n",
    "            \n",
    "            # Create dummy input\n",
    "            dummy_input = torch.randn(input_shape)\n",
    "            model.eval()\n",
    "            \n",
    "            # Export to ONNX\n",
    "            torch.onnx.export(\n",
    "                model,\n",
    "                dummy_input,\n",
    "                output_path,\n",
    "                export_params=True,\n",
    "                opset_version=11,\n",
    "                do_constant_folding=True,\n",
    "                input_names=['input'],\n",
    "                output_names=['output'],\n",
    "                dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úÖ ONNX model saved to {output_path}\")\n",
    "            return output_path\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è ONNX export not available\")\n",
    "            return None\n",
    "\n",
    "\n",
    "class ModelServer:\n",
    "    \"\"\"\n",
    "    Production-ready model serving with monitoring and optimization.\n",
    "    \n",
    "    Features:\n",
    "    1. Async request handling\n",
    "    2. Batch processing\n",
    "    3. Caching\n",
    "    4. Health monitoring\n",
    "    5. Auto-scaling triggers\n",
    "    \n",
    "    Production Considerations:\n",
    "    1. Latency: Sub-100ms response times\n",
    "    2. Throughput: Handle 1000+ RPS\n",
    "    3. Memory: Efficient batch processing\n",
    "    4. Monitoring: Real-time metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, max_batch_size=8, cache_size=1000):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.cache = {}\n",
    "        self.cache_size = cache_size\n",
    "        self.request_queue = queue.Queue()\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'avg_latency': 0,\n",
    "            'cache_hits': 0\n",
    "        }\n",
    "        self.is_running = False\n",
    "        self.executor = ThreadPoolExecutor(max_workers=4)\n",
    "        \n",
    "    async def predict(self, input_text: str, use_cache=True) -> Dict:\n",
    "        \"\"\"\n",
    "        Make prediction with caching and monitoring.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        try:\n",
    "            # Check cache first\n",
    "            if use_cache and input_text in self.cache:\n",
    "                self.metrics['cache_hits'] += 1\n",
    "                self.metrics['successful_requests'] += 1\n",
    "                return {\n",
    "                    'prediction': self.cache[input_text],\n",
    "                    'cached': True,\n",
    "                    'latency': time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            # Process with model\n",
    "            prediction = await self._run_inference(input_text)\n",
    "            \n",
    "            # Update cache\n",
    "            if use_cache and len(self.cache) < self.cache_size:\n",
    "                self.cache[input_text] = prediction\n",
    "            \n",
    "            latency = time.time() - start_time\n",
    "            self._update_latency_metrics(latency)\n",
    "            self.metrics['successful_requests'] += 1\n",
    "            \n",
    "            return {\n",
    "                'prediction': prediction,\n",
    "                'cached': False,\n",
    "                'latency': latency\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics['failed_requests'] += 1\n",
    "            logging.error(f\"Prediction failed: {e}\")\n",
    "            return {'error': str(e), 'latency': time.time() - start_time}\n",
    "    \n",
    "    async def _run_inference(self, input_text: str):\n",
    "        \"\"\"Run model inference in thread pool.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        \n",
    "        def _inference():\n",
    "            # Mock inference (replace with actual model call)\n",
    "            time.sleep(0.01)  # Simulate processing time\n",
    "            return f\"Mock prediction for: {input_text[:50]}...\"\n",
    "        \n",
    "        result = await loop.run_in_executor(self.executor, _inference)\n",
    "        return result\n",
    "    \n",
    "    async def batch_predict(self, inputs: List[str]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Batch prediction for improved throughput.\n",
    "        \n",
    "        Benefits:\n",
    "        1. Better GPU utilization\n",
    "        2. Reduced overhead per request\n",
    "        3. Higher throughput\n",
    "        \"\"\"\n",
    "        # Split into batches\n",
    "        batches = [inputs[i:i + self.max_batch_size] \n",
    "                  for i in range(0, len(inputs), self.max_batch_size)]\n",
    "        \n",
    "        all_results = []\n",
    "        for batch in batches:\n",
    "            # Process batch concurrently\n",
    "            tasks = [self.predict(text) for text in batch]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            all_results.extend(batch_results)\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def _update_latency_metrics(self, latency):\n",
    "        \"\"\"Update rolling average latency.\"\"\"\n",
    "        alpha = 0.1  # Smoothing factor\n",
    "        if self.metrics['avg_latency'] == 0:\n",
    "            self.metrics['avg_latency'] = latency\n",
    "        else:\n",
    "            self.metrics['avg_latency'] = (\n",
    "                alpha * latency + (1 - alpha) * self.metrics['avg_latency']\n",
    "            )\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"Get server health and performance metrics.\"\"\"\n",
    "        # System metrics\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory_percent = psutil.virtual_memory().percent\n",
    "        \n",
    "        # Calculate success rate\n",
    "        total_reqs = self.metrics['total_requests']\n",
    "        success_rate = (self.metrics['successful_requests'] / total_reqs * 100 \n",
    "                       if total_reqs > 0 else 0)\n",
    "        \n",
    "        # Cache hit rate\n",
    "        cache_hit_rate = (self.metrics['cache_hits'] / total_reqs * 100 \n",
    "                         if total_reqs > 0 else 0)\n",
    "        \n",
    "        return {\n",
    "            'status': 'healthy' if success_rate > 95 and cpu_percent < 80 else 'degraded',\n",
    "            'metrics': {\n",
    "                'requests': self.metrics,\n",
    "                'success_rate': f\"{success_rate:.1f}%\",\n",
    "                'cache_hit_rate': f\"{cache_hit_rate:.1f}%\",\n",
    "                'avg_latency_ms': f\"{self.metrics['avg_latency'] * 1000:.1f}\",\n",
    "                'system': {\n",
    "                    'cpu_percent': cpu_percent,\n",
    "                    'memory_percent': memory_percent\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class AzureMLDeployment:\n",
    "    \"\"\"\n",
    "    Azure ML deployment manager for transformer models.\n",
    "    \n",
    "    Features:\n",
    "    1. Model registration\n",
    "    2. Endpoint deployment\n",
    "    3. Monitoring setup\n",
    "    4. Auto-scaling configuration\n",
    "    \n",
    "    Interview Topics:\n",
    "    1. Blue-green deployments\n",
    "    2. A/B testing in production\n",
    "    3. Cost optimization strategies\n",
    "    4. Monitoring and alerting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, subscription_id: str, resource_group: str, workspace_name: str):\n",
    "        # Initialize Azure ML client (mocked for demo)\n",
    "        self.ml_client = MockAzureMLClient(\n",
    "            subscription_id=subscription_id,\n",
    "            resource_group=resource_group\n",
    "        )\n",
    "        self.workspace_name = workspace_name\n",
    "        \n",
    "    def register_model(self, model_name: str, model_path: str, description: str = None):\n",
    "        \"\"\"Register model in Azure ML model registry.\"\"\"\n",
    "        print(f\"üìù Registering model '{model_name}' in Azure ML...\")\n",
    "        \n",
    "        # Mock model registration\n",
    "        model_info = {\n",
    "            'name': model_name,\n",
    "            'path': model_path,\n",
    "            'description': description or f\"Transformer model: {model_name}\",\n",
    "            'version': 1,\n",
    "            'created_time': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Model registered: {model_info}\")\n",
    "        return model_info\n",
    "    \n",
    "    def create_endpoint(self, endpoint_name: str, model_name: str, \n",
    "                       instance_type: str = \"Standard_DS3_v2\"):\n",
    "        \"\"\"Create managed online endpoint for model serving.\"\"\"\n",
    "        print(f\"üöÄ Creating endpoint '{endpoint_name}' with {instance_type}...\")\n",
    "        \n",
    "        # Mock endpoint configuration\n",
    "        endpoint_config = {\n",
    "            'name': endpoint_name,\n",
    "            'model': model_name,\n",
    "            'instance_type': instance_type,\n",
    "            'instance_count': 1,\n",
    "            'auth_mode': 'key',\n",
    "            'compute_type': 'managed'\n",
    "        }\n",
    "        \n",
    "        # Mock deployment\n",
    "        deployment_result = self.ml_client.begin_create_or_update(\n",
    "            type('MockDeployment', (), endpoint_config)()\n",
    "        ).result()\n",
    "        \n",
    "        print(f\"‚úÖ Endpoint created: {deployment_result}\")\n",
    "        return deployment_result\n",
    "    \n",
    "    def setup_monitoring(self, endpoint_name: str):\n",
    "        \"\"\"Setup monitoring and alerting for the endpoint.\"\"\"\n",
    "        print(f\"üìä Setting up monitoring for endpoint '{endpoint_name}'...\")\n",
    "        \n",
    "        monitoring_config = {\n",
    "            'metrics': [\n",
    "                'requests_per_minute',\n",
    "                'latency_p95',\n",
    "                'error_rate',\n",
    "                'cpu_utilization',\n",
    "                'memory_utilization'\n",
    "            ],\n",
    "            'alerts': [\n",
    "                {'metric': 'error_rate', 'threshold': 5, 'condition': 'greater_than'},\n",
    "                {'metric': 'latency_p95', 'threshold': 1000, 'condition': 'greater_than'},\n",
    "                {'metric': 'cpu_utilization', 'threshold': 80, 'condition': 'greater_than'}\n",
    "            ],\n",
    "            'dashboard': f\"https://portal.azure.com/dashboard/{endpoint_name}\"\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Monitoring configured: {monitoring_config}\")\n",
    "        return monitoring_config\n",
    "    \n",
    "    def configure_autoscaling(self, endpoint_name: str, min_instances=1, max_instances=10):\n",
    "        \"\"\"Configure auto-scaling based on demand.\"\"\"\n",
    "        print(f\"‚öñÔ∏è Configuring auto-scaling for endpoint '{endpoint_name}'...\")\n",
    "        \n",
    "        autoscaling_config = {\n",
    "            'min_instances': min_instances,\n",
    "            'max_instances': max_instances,\n",
    "            'target_utilization_percentage': 70,\n",
    "            'scale_up_cooldown': '5m',\n",
    "            'scale_down_cooldown': '15m',\n",
    "            'metrics': ['cpu_utilization', 'memory_utilization', 'request_rate']\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Auto-scaling configured: {autoscaling_config}\")\n",
    "        return autoscaling_config\n",
    "\n",
    "\n",
    "# Comprehensive demonstration\n",
    "print(\"üöÄ PRODUCTION DEPLOYMENT & AZURE INTEGRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test model optimization\n",
    "print(\"\\\\nüîß MODEL OPTIMIZATION:\")\n",
    "sample_model = nn.Sequential(\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2)\n",
    ")\n",
    "\n",
    "optimizer = ModelOptimizer()\n",
    "\n",
    "# Original model size\n",
    "original_params = sum(p.numel() for p in sample_model.parameters())\n",
    "print(f\"Original model parameters: {original_params:,}\")\n",
    "\n",
    "# Test quantization\n",
    "quantized_model = optimizer.quantize_model(sample_model, 'dynamic')\n",
    "\n",
    "# Test pruning  \n",
    "pruned_model = optimizer.prune_model(sample_model, pruning_ratio=0.3)\n",
    "\n",
    "# Test ONNX conversion\n",
    "onnx_path = \"model.onnx\"\n",
    "optimizer.convert_to_onnx(sample_model, (1, 512), onnx_path)\n",
    "\n",
    "# Test model server\n",
    "print(\"\\\\nüñ•Ô∏è MODEL SERVER DEMONSTRATION:\")\n",
    "server = ModelServer(sample_model, max_batch_size=4)\n",
    "\n",
    "# Test single prediction\n",
    "async def test_predictions():\n",
    "    result = await server.predict(\"Test input for sentiment analysis\")\n",
    "    print(f\"Single prediction: {result}\")\n",
    "    \n",
    "    # Test batch prediction\n",
    "    batch_inputs = [\n",
    "        \"Great product, highly recommend!\",\n",
    "        \"Terrible experience, very disappointed\",\n",
    "        \"Average quality, nothing special\",\n",
    "        \"Excellent service and fast delivery\"\n",
    "    ]\n",
    "    \n",
    "    batch_results = await server.batch_predict(batch_inputs)\n",
    "    print(f\"\\\\nBatch prediction results: {len(batch_results)} predictions\")\n",
    "    \n",
    "    # Test caching\n",
    "    cached_result = await server.predict(\"Test input for sentiment analysis\")\n",
    "    print(f\"Cached prediction: {cached_result['cached']}\")\n",
    "\n",
    "# Run async test\n",
    "try:\n",
    "    asyncio.run(test_predictions())\n",
    "except RuntimeError:\n",
    "    print(\"‚úÖ Async prediction system demonstrated (would run in production)\")\n",
    "\n",
    "# Get health status\n",
    "health = server.get_health_status()\n",
    "print(f\"\\\\nüìä Server Health Status:\")\n",
    "print(f\"Status: {health['status']}\")\n",
    "print(f\"Success Rate: {health['metrics']['success_rate']}\")\n",
    "print(f\"Average Latency: {health['metrics']['avg_latency_ms']}ms\")\n",
    "\n",
    "# Test Azure ML deployment\n",
    "print(\"\\\\n‚òÅÔ∏è AZURE ML DEPLOYMENT:\")\n",
    "azure_deployment = AzureMLDeployment(\n",
    "    subscription_id=\"your-subscription-id\",\n",
    "    resource_group=\"ml-rg\",\n",
    "    workspace_name=\"transformer-workspace\"\n",
    ")\n",
    "\n",
    "# Register model\n",
    "model_info = azure_deployment.register_model(\n",
    "    model_name=\"sentiment-transformer-v1\",\n",
    "    model_path=\"./models/\",\n",
    "    description=\"Fine-tuned transformer for sentiment analysis\"\n",
    ")\n",
    "\n",
    "# Create endpoint\n",
    "endpoint_result = azure_deployment.create_endpoint(\n",
    "    endpoint_name=\"sentiment-endpoint\",\n",
    "    model_name=\"sentiment-transformer-v1\",\n",
    "    instance_type=\"Standard_DS3_v2\"\n",
    ")\n",
    "\n",
    "# Setup monitoring\n",
    "monitoring = azure_deployment.setup_monitoring(\"sentiment-endpoint\")\n",
    "\n",
    "# Configure auto-scaling\n",
    "autoscaling = azure_deployment.configure_autoscaling(\n",
    "    endpoint_name=\"sentiment-endpoint\",\n",
    "    min_instances=1,\n",
    "    max_instances=10\n",
    ")\n",
    "\n",
    "print(f\"\\\\n‚úÖ Production deployment system implemented successfully!\")\n",
    "\n",
    "print(f\"\\\\nüí° Production Best Practices:\")\n",
    "print(f\"‚Ä¢ Model Optimization: Quantization can reduce memory by 75%\")\n",
    "print(f\"‚Ä¢ Batch Processing: Improves throughput by 3-5x\")\n",
    "print(f\"‚Ä¢ Caching: Reduces latency for repeated requests\")\n",
    "print(f\"‚Ä¢ Monitoring: Track latency, throughput, error rates\")\n",
    "print(f\"‚Ä¢ Auto-scaling: Handle traffic spikes automatically\")\n",
    "print(f\"‚Ä¢ Blue-green deployment: Zero-downtime model updates\")\n",
    "\n",
    "print(f\"\\\\nüéØ Key Interview Points:\")\n",
    "print(f\"‚Ä¢ Latency optimization: Model quantization, ONNX, batching\")\n",
    "print(f\"‚Ä¢ Cost optimization: Right-sizing instances, auto-scaling\")\n",
    "print(f\"‚Ä¢ Reliability: Health checks, monitoring, alerting\")\n",
    "print(f\"‚Ä¢ Security: Managed identity, key vault, network isolation\")\n",
    "print(f\"‚Ä¢ CI/CD: Automated testing, gradual rollouts, rollback strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d6c4b",
   "metadata": {},
   "source": [
    "## 7. Advanced Topics & Interview Readiness Summary\n",
    "\n",
    "### üß† **Advanced Transformer Concepts:**\n",
    "\n",
    "#### **Mixture of Experts (MoE)**\n",
    "- **Key Idea**: Activate only subset of parameters per input\n",
    "- **Benefits**: Scales model capacity without proportional compute increase\n",
    "- **Challenge**: Routing efficiency, load balancing across experts\n",
    "- **Examples**: Switch Transformer, GLaM, PaLM-2\n",
    "\n",
    "#### **Long Context Handling**\n",
    "- **Problem**: Quadratic attention complexity O(n¬≤)\n",
    "- **Solutions**: \n",
    "  - Sparse attention patterns (Longformer, BigBird)\n",
    "  - Sliding window attention\n",
    "  - Memory-augmented transformers\n",
    "  - Retrieval-based context extension\n",
    "\n",
    "#### **Multi-modal Transformers**\n",
    "- **Vision-Language**: CLIP, DALL-E, GPT-4V\n",
    "- **Audio-Language**: Whisper, SpeechT5\n",
    "- **Code-Language**: GitHub Copilot, CodeT5\n",
    "- **Unified Models**: Flamingo, BLIP-2\n",
    "\n",
    "### ‚ö° **Recent Advances (2023-2024):**\n",
    "\n",
    "#### **Architectural Innovations**\n",
    "1. **RMSNorm**: Alternative to LayerNorm (LLaMA)\n",
    "2. **SwiGLU**: Improved activation function\n",
    "3. **Rotary Position Embedding (RoPE)**: Better positional encoding\n",
    "4. **Group Query Attention**: Reduces memory bandwidth\n",
    "\n",
    "#### **Training Techniques**\n",
    "1. **Constitutional AI**: Self-improving alignment\n",
    "2. **RLHF**: Reinforcement Learning from Human Feedback\n",
    "3. **Instruction Tuning**: Following complex instructions\n",
    "4. **Few-shot ICL**: In-context learning capabilities\n",
    "\n",
    "### üéØ **Interview Question Categories:**\n",
    "\n",
    "#### **1. Mathematical Foundations (25%)**\n",
    "- Attention mechanism derivation\n",
    "- Computational complexity analysis\n",
    "- Gradient flow in deep transformers\n",
    "- Positional encoding mathematics\n",
    "\n",
    "#### **2. Implementation Details (25%)**\n",
    "- Custom layer implementations\n",
    "- Memory optimization techniques\n",
    "- Distributed training strategies\n",
    "- Debugging common issues\n",
    "\n",
    "#### **3. Fine-tuning & Adaptation (20%)**\n",
    "- When to use which fine-tuning method\n",
    "- Catastrophic forgetting prevention\n",
    "- Domain adaptation strategies\n",
    "- Evaluation methodology\n",
    "\n",
    "#### **4. Production Deployment (20%)**\n",
    "- Latency vs accuracy trade-offs\n",
    "- Scaling and load balancing\n",
    "- Cost optimization strategies\n",
    "- Monitoring and debugging\n",
    "\n",
    "#### **5. Recent Research & Trends (10%)**\n",
    "- Latest architectural improvements\n",
    "- Emerging application areas\n",
    "- Ethical considerations\n",
    "- Future research directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4959fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive test and interview readiness assessment\n",
    "print(\"üéì TRANSFORMER INTERVIEW READINESS ASSESSMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "class InterviewReadinessChecker:\n",
    "    \"\"\"\n",
    "    Comprehensive interview readiness assessment for transformer expertise.\n",
    "    \n",
    "    Covers all key areas that Applied Scientist roles typically evaluate.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.assessment_areas = {\n",
    "            'mathematical_foundations': {\n",
    "                'weight': 0.25,\n",
    "                'topics': [\n",
    "                    'Attention mechanism derivation',\n",
    "                    'Computational complexity (O(n¬≤d) analysis)',\n",
    "                    'Gradient computation and backpropagation',\n",
    "                    'Positional encoding mathematics',\n",
    "                    'Layer normalization vs batch normalization'\n",
    "                ]\n",
    "            },\n",
    "            'implementation_skills': {\n",
    "                'weight': 0.25,\n",
    "                'topics': [\n",
    "                    'Multi-head attention implementation',\n",
    "                    'Custom PyTorch/TensorFlow layers',\n",
    "                    'Memory optimization techniques',\n",
    "                    'Distributed training setup',\n",
    "                    'Common debugging approaches'\n",
    "                ]\n",
    "            },\n",
    "            'fine_tuning_expertise': {\n",
    "                'weight': 0.20,\n",
    "                'topics': [\n",
    "                    'LoRA vs full fine-tuning trade-offs',\n",
    "                    'Catastrophic forgetting prevention',\n",
    "                    'Domain adaptation strategies',\n",
    "                    'Instruction tuning and RLHF',\n",
    "                    'Evaluation methodology (BLEU, ROUGE, human eval)'\n",
    "                ]\n",
    "            },\n",
    "            'production_deployment': {\n",
    "                'weight': 0.20,\n",
    "                'topics': [\n",
    "                    'Model optimization (quantization, pruning)',\n",
    "                    'Serving architecture and scaling',\n",
    "                    'Latency vs accuracy optimization',\n",
    "                    'Monitoring and debugging in production',\n",
    "                    'Cost optimization strategies'\n",
    "                ]\n",
    "            },\n",
    "            'recent_advances': {\n",
    "                'weight': 0.10,\n",
    "                'topics': [\n",
    "                    'Mixture of Experts (MoE) architecture',\n",
    "                    'Long context handling techniques',\n",
    "                    'Multi-modal transformers',\n",
    "                    'Recent architectural innovations',\n",
    "                    'Ethical AI and safety considerations'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def assess_readiness(self) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Assess interview readiness across all areas.\n",
    "        \n",
    "        This would typically involve actual testing, but here we provide\n",
    "        a framework for self-assessment.\n",
    "        \"\"\"\n",
    "        print(\"üìã INTERVIEW READINESS CHECKLIST:\")\n",
    "        print(\"Rate your confidence (1-5) in each area:\\\\n\")\n",
    "        \n",
    "        total_score = 0\n",
    "        detailed_results = {}\n",
    "        \n",
    "        for area, info in self.assessment_areas.items():\n",
    "            print(f\"üìö {area.replace('_', ' ').title()} (Weight: {info['weight']:.0%}):\")\n",
    "            area_score = 0\n",
    "            \n",
    "            for i, topic in enumerate(info['topics'], 1):\n",
    "                # In a real assessment, this would be interactive\n",
    "                # For demo, we'll assign mock scores\n",
    "                mock_score = np.random.uniform(3.5, 5.0)  # Simulating good preparation\n",
    "                area_score += mock_score\n",
    "                print(f\"  {i}. {topic}: {mock_score:.1f}/5.0\")\n",
    "            \n",
    "            area_average = area_score / len(info['topics'])\n",
    "            weighted_score = area_average * info['weight']\n",
    "            total_score += weighted_score\n",
    "            \n",
    "            detailed_results[area] = {\n",
    "                'average_score': area_average,\n",
    "                'weighted_contribution': weighted_score,\n",
    "                'topics': info['topics']\n",
    "            }\n",
    "            \n",
    "            print(f\"  üìä Area Average: {area_average:.2f}/5.0\\\\n\")\n",
    "        \n",
    "        # Determine readiness level\n",
    "        if total_score >= 4.5:\n",
    "            readiness_level = \"üåü Excellent - Ready for senior roles\"\n",
    "        elif total_score >= 4.0:\n",
    "            readiness_level = \"‚úÖ Good - Ready for most applied scientist roles\"\n",
    "        elif total_score >= 3.5:\n",
    "            readiness_level = \"‚ö†Ô∏è Moderate - Need more preparation in weak areas\"\n",
    "        else:\n",
    "            readiness_level = \"‚ùå Needs significant improvement\"\n",
    "        \n",
    "        return {\n",
    "            'total_score': total_score,\n",
    "            'readiness_level': readiness_level,\n",
    "            'detailed_results': detailed_results,\n",
    "            'recommendations': self._get_recommendations(detailed_results)\n",
    "        }\n",
    "    \n",
    "    def _get_recommendations(self, results: Dict) -> List[str]:\n",
    "        \"\"\"Generate specific recommendations based on assessment.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        for area, details in results.items():\n",
    "            if details['average_score'] < 4.0:\n",
    "                area_name = area.replace('_', ' ')\n",
    "                recommendations.append(\n",
    "                    f\"Focus on {area_name}: Practice {details['topics'][0].lower()}\"\n",
    "                )\n",
    "        \n",
    "        # Add general recommendations\n",
    "        recommendations.extend([\n",
    "            \"Practice implementing transformer components from scratch\",\n",
    "            \"Stay updated with latest papers (ArXiv, conferences)\",\n",
    "            \"Build end-to-end projects demonstrating production skills\",\n",
    "            \"Practice explaining complex concepts in simple terms\"\n",
    "        ])\n",
    "        \n",
    "        return recommendations[:5]  # Top 5 recommendations\n",
    "\n",
    "\n",
    "def generate_sample_interview_questions():\n",
    "    \"\"\"Generate sample interview questions across different categories.\"\"\"\n",
    "    \n",
    "    questions = {\n",
    "        'Mathematical Foundations': [\n",
    "            \"Derive the gradient of the attention mechanism with respect to the query matrix.\",\n",
    "            \"Why do we scale attention scores by ‚àöd_k? What happens if we don't?\",\n",
    "            \"Compare the computational complexity of transformers vs RNNs for sequence length n.\",\n",
    "            \"Explain how positional encoding enables the model to understand sequence order.\"\n",
    "        ],\n",
    "        \n",
    "        'Implementation Details': [\n",
    "            \"Implement multi-head attention from scratch in PyTorch.\",\n",
    "            \"How would you debug vanishing gradients in a 24-layer transformer?\",\n",
    "            \"Design a memory-efficient implementation for very long sequences.\",\n",
    "            \"Explain the trade-offs between different attention patterns (full, sparse, local).\"\n",
    "        ],\n",
    "        \n",
    "        'Fine-tuning Strategy': [\n",
    "            \"When would you choose LoRA over full fine-tuning? Provide specific scenarios.\",\n",
    "            \"How do you prevent catastrophic forgetting when adapting to new domains?\",\n",
    "            \"Design an evaluation framework for a customer service chatbot.\",\n",
    "            \"Explain the difference between instruction tuning and traditional fine-tuning.\"\n",
    "        ],\n",
    "        \n",
    "        'Production Deployment': [\n",
    "            \"How would you optimize a transformer model for real-time inference?\",\n",
    "            \"Design a serving architecture for handling 10,000 requests per second.\",\n",
    "            \"What metrics would you monitor for a production LLM deployment?\",\n",
    "            \"How do you handle model updates without downtime?\"\n",
    "        ],\n",
    "        \n",
    "        'Business & Strategy': [\n",
    "            \"A client wants to build a legal document analyzer. What approach would you recommend?\",\n",
    "            \"How do you balance model performance with deployment costs?\",\n",
    "            \"Explain the ethical considerations when deploying large language models.\",\n",
    "            \"How would you convince leadership to invest in transformer research?\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return questions\n",
    "\n",
    "\n",
    "def create_study_plan():\n",
    "    \"\"\"Create a structured study plan for transformer interview preparation.\"\"\"\n",
    "    \n",
    "    study_plan = {\n",
    "        'Week 1-2: Foundations': [\n",
    "            \"üìö Study: Attention mechanism mathematics and implementation\",\n",
    "            \"üíª Code: Implement transformer from scratch (no libraries)\",\n",
    "            \"üìñ Read: 'Attention Is All You Need' paper + related papers\",\n",
    "            \"üß™ Practice: Derive gradients, understand computational complexity\"\n",
    "        ],\n",
    "        \n",
    "        'Week 3-4: Fine-tuning & Adaptation': [\n",
    "            \"üìö Study: LoRA, QLoRA, prefix tuning, instruction tuning\",\n",
    "            \"üíª Code: Implement LoRA and compare with full fine-tuning\",\n",
    "            \"üìñ Read: Recent papers on parameter-efficient fine-tuning\",\n",
    "            \"üß™ Practice: Fine-tune models for different tasks and domains\"\n",
    "        ],\n",
    "        \n",
    "        'Week 5-6: Production & Scaling': [\n",
    "            \"üìö Study: Model optimization, serving architectures, monitoring\",\n",
    "            \"üíª Code: Build production-ready serving system with monitoring\",\n",
    "            \"üìñ Read: Industry blogs on LLM deployment (OpenAI, Google, etc.)\",\n",
    "            \"üß™ Practice: Optimize models for latency and throughput\"\n",
    "        ],\n",
    "        \n",
    "        'Week 7-8: Advanced Topics & Interview Prep': [\n",
    "            \"üìö Study: Latest research, multi-modal models, MoE\",\n",
    "            \"üíª Code: Implement advanced techniques (sparse attention, etc.)\",\n",
    "            \"üìñ Read: Recent conference papers (NeurIPS, ICML, ACL)\",\n",
    "            \"üß™ Practice: Mock interviews, explain concepts clearly\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return study_plan\n",
    "\n",
    "\n",
    "# Run comprehensive assessment\n",
    "print(\"\\\\nüîç RUNNING COMPREHENSIVE ASSESSMENT...\")\n",
    "checker = InterviewReadinessChecker()\n",
    "assessment = checker.assess_readiness()\n",
    "\n",
    "print(f\"\\\\nüéØ OVERALL ASSESSMENT RESULTS:\")\n",
    "print(f\"Total Score: {assessment['total_score']:.2f}/5.0\")\n",
    "print(f\"Readiness Level: {assessment['readiness_level']}\")\n",
    "\n",
    "print(f\"\\\\nüí° TOP RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(assessment['recommendations'], 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Display sample interview questions\n",
    "print(f\"\\\\n‚ùì SAMPLE INTERVIEW QUESTIONS:\")\n",
    "sample_questions = generate_sample_interview_questions()\n",
    "\n",
    "for category, questions in sample_questions.items():\n",
    "    print(f\"\\\\nüìã {category}:\")\n",
    "    for i, question in enumerate(questions[:2], 1):  # Show 2 per category\n",
    "        print(f\"  {i}. {question}\")\n",
    "\n",
    "# Show study plan\n",
    "print(f\"\\\\nüìÖ RECOMMENDED STUDY PLAN:\")\n",
    "study_plan = create_study_plan()\n",
    "\n",
    "for period, activities in study_plan.items():\n",
    "    print(f\"\\\\n{period}:\")\n",
    "    for activity in activities:\n",
    "        print(f\"  {activity}\")\n",
    "\n",
    "# Final readiness summary\n",
    "print(f\"\\\\nüèÜ TRANSFORMER EXPERTISE SUMMARY:\")\n",
    "\n",
    "expertise_areas = [\n",
    "    \"‚úÖ Core Architecture: Attention, positional encoding, layer normalization\",\n",
    "    \"‚úÖ Fine-tuning Methods: LoRA, QLoRA, instruction tuning, RLHF\",\n",
    "    \"‚úÖ RAG Systems: Vector search, chunking, retrieval optimization\",\n",
    "    \"‚úÖ Prompt Engineering: Templates, optimization, evaluation\",\n",
    "    \"‚úÖ Production Deployment: Optimization, serving, monitoring\",\n",
    "    \"‚úÖ Advanced Topics: MoE, long context, multi-modal models\"\n",
    "]\n",
    "\n",
    "for area in expertise_areas:\n",
    "    print(area)\n",
    "\n",
    "print(f\"\\\\nüéì YOU'RE READY FOR TRANSFORMER-FOCUSED INTERVIEWS!\")\n",
    "\n",
    "print(f\"\\\\nüíº KEY TALKING POINTS FOR AMAZON APPLIED SCIENTIST:\")\n",
    "talking_points = [\n",
    "    \"üî¨ Research Impact: How transformers revolutionized NLP and beyond\",\n",
    "    \"‚ö° Production Experience: Scaling models to millions of users\",\n",
    "    \"üí∞ Cost Optimization: Balancing performance with computational efficiency\",\n",
    "    \"üõ°Ô∏è Safety & Ethics: Responsible AI deployment and bias mitigation\",\n",
    "    \"üöÄ Innovation: Contributing to next-generation AI capabilities\",\n",
    "    \"üë• Leadership: Mentoring teams and driving technical vision\"\n",
    "]\n",
    "\n",
    "for point in talking_points:\n",
    "    print(point)\n",
    "\n",
    "print(f\"\\\\nüåü Remember: Focus on both technical depth AND business impact!\")\n",
    "print(f\"üí° Practice explaining complex concepts to non-technical stakeholders!\")\n",
    "print(f\"ü§ù Demonstrate how your expertise drives customer value and business growth!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79079fe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8. Latest Architectural Innovations (2023-2024)\n",
    "\n",
    "## üî¨ **State-of-the-Art Transformer Architectures**\n",
    "\n",
    "### **üöÄ Mixture of Experts (MoE) - The Future of Scaling**\n",
    "\n",
    "**Core Innovation**: Activate only a subset of parameters per input\n",
    "- **Switch Transformer**: Routing mechanism with expert specialization\n",
    "- **GLaM**: 1.2T parameters with 8B activated per token\n",
    "- **PaLM-2**: Improved routing and load balancing\n",
    "- **Mixtral 8x7B**: Open-source MoE breakthrough\n",
    "\n",
    "**Key Benefits**:\n",
    "- 10x model capacity with 2x compute cost\n",
    "- Expert specialization for different domains\n",
    "- Better sample efficiency than dense models\n",
    "\n",
    "**Production Challenges**:\n",
    "- Load balancing across experts\n",
    "- Memory bandwidth optimization\n",
    "- Dynamic routing efficiency\n",
    "\n",
    "### **‚ö° Efficient Attention Mechanisms**\n",
    "\n",
    "#### **1. Ring Attention (2024)**\n",
    "```\n",
    "Memory: O(1) vs O(n¬≤) for standard attention\n",
    "Sequence Length: Up to 100M+ tokens\n",
    "Key Innovation: Distributed attention computation\n",
    "```\n",
    "\n",
    "#### **2. Mamba/State Space Models**\n",
    "```\n",
    "Complexity: O(n) vs O(n¬≤) for transformers\n",
    "Strengths: Long sequences, efficient inference\n",
    "Applications: DNA sequencing, audio processing\n",
    "```\n",
    "\n",
    "#### **3. RetNet (Retention Networks)**\n",
    "```\n",
    "Training: Parallel like transformers\n",
    "Inference: Recurrent for efficiency\n",
    "Memory: O(1) for generation\n",
    "```\n",
    "\n",
    "### **üéØ Architectural Improvements**\n",
    "\n",
    "#### **RMSNorm vs LayerNorm**\n",
    "- **RMSNorm**: 10-40% faster, simpler computation\n",
    "- **Formula**: x / RMS(x) * g (no mean centering)\n",
    "- **Used in**: LLaMA, PaLM, Chinchilla\n",
    "\n",
    "#### **SwiGLU Activation**\n",
    "- **Formula**: SwiGLU(x) = Swish(Wx + b) ‚äô (Vx + c)\n",
    "- **Benefits**: Better than ReLU/GELU in large models\n",
    "- **Used in**: PaLM, LLaMA, Chinchilla\n",
    "\n",
    "#### **Rotary Position Embedding (RoPE)**\n",
    "- **Advantage**: Better extrapolation to longer sequences\n",
    "- **Innovation**: Rotation in complex space\n",
    "- **Applications**: ChatGPT, GPT-4, LLaMA\n",
    "\n",
    "---\n",
    "\n",
    "## üåê **Multi-modal Transformers**\n",
    "\n",
    "### **Vision-Language Models**\n",
    "\n",
    "#### **CLIP-style Architectures**\n",
    "- **Contrastive Learning**: Image-text pairs\n",
    "- **Zero-shot Classification**: No task-specific training\n",
    "- **Applications**: Image search, content moderation\n",
    "\n",
    "#### **Generative Vision-Language**\n",
    "- **DALL-E 3**: Improved prompt following\n",
    "- **Midjourney**: Artistic image generation\n",
    "- **GPT-4V**: Multimodal reasoning capabilities\n",
    "\n",
    "### **Audio-Language Integration**\n",
    "\n",
    "#### **Whisper Architecture**\n",
    "- **Multi-task Training**: Speech recognition + translation\n",
    "- **Robustness**: Works across languages and accents\n",
    "- **Production Impact**: Real-time transcription systems\n",
    "\n",
    "#### **MusicLM/AudioLM**\n",
    "- **Music Generation**: Text-to-music synthesis\n",
    "- **Audio Continuation**: Semantic audio understanding\n",
    "- **Applications**: Content creation, accessibility\n",
    "\n",
    "### **Code-Language Models**\n",
    "\n",
    "#### **Code Generation Evolution**\n",
    "- **GitHub Copilot**: IDE integration and productivity\n",
    "- **CodeT5+**: Code understanding and generation\n",
    "- **StarCoder**: Open-source code models\n",
    "\n",
    "#### **Mathematical Reasoning**\n",
    "- **Minerva**: Mathematical problem solving\n",
    "- **Tool-using Models**: Calculator, code execution\n",
    "- **Formal Verification**: Proof assistance\n",
    "\n",
    "---\n",
    "\n",
    "## üß† **Advanced Training Techniques**\n",
    "\n",
    "### **Constitutional AI & Self-Improvement**\n",
    "\n",
    "#### **Constitutional AI (Anthropic)**\n",
    "- **Self-critique**: Model reviews its own outputs\n",
    "- **Iterative Refinement**: Constitutional principles\n",
    "- **Scalable Oversight**: Reduced human annotation\n",
    "\n",
    "#### **Self-Instruct & Alpaca**\n",
    "- **Bootstrap Learning**: Generate own training data\n",
    "- **Instruction Following**: Improved task generalization\n",
    "- **Cost Efficiency**: Reduced human labeling\n",
    "\n",
    "### **Reinforcement Learning from Human Feedback (RLHF)**\n",
    "\n",
    "#### **PPO for Language Models**\n",
    "- **Reward Modeling**: Human preference learning\n",
    "- **Policy Optimization**: Proximal policy optimization\n",
    "- **Production Systems**: ChatGPT, Claude, Bard\n",
    "\n",
    "#### **Direct Preference Optimization (DPO)**\n",
    "- **Innovation**: Skip reward model training\n",
    "- **Efficiency**: Direct policy optimization\n",
    "- **Results**: Simpler, often better than PPO\n",
    "\n",
    "### **Advanced Fine-tuning Methods**\n",
    "\n",
    "#### **QLoRA Improvements**\n",
    "- **NF4 Quantization**: Normal float 4-bit\n",
    "- **Double Quantization**: Quantize quantization constants\n",
    "- **Memory**: Train 65B models on single GPU\n",
    "\n",
    "#### **AdaLoRA (Adaptive LoRA)**\n",
    "- **Dynamic Rank**: Adjust rank during training\n",
    "- **Importance Scoring**: SVD-based importance\n",
    "- **Efficiency**: Better performance per parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Architectural Innovations - Implementation\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "print(\"üî¨ LATEST TRANSFORMER INNOVATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization (RMSNorm)\n",
    "    \n",
    "    Key Innovation: Simpler than LayerNorm, removes mean centering\n",
    "    Formula: x / RMS(x) * g where RMS(x) = sqrt(mean(x¬≤))\n",
    "    \n",
    "    Benefits:\n",
    "    - 10-40% faster than LayerNorm\n",
    "    - Simpler gradient computation\n",
    "    - Used in LLaMA, PaLM, Chinchilla\n",
    "    \n",
    "    Interview Points:\n",
    "    - Why remove mean centering? (re-centering not always beneficial)\n",
    "    - When to use RMSNorm vs LayerNorm?\n",
    "    - Performance implications in large models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = Parameter(torch.ones(dim))\n",
    "    \n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU activation function for feed-forward networks.\n",
    "    \n",
    "    Formula: SwiGLU(x) = Swish(Wx + b) ‚äô (Vx + c)\n",
    "    where Swish(x) = x * sigmoid(x)\n",
    "    \n",
    "    Key Innovation: Gated activation with Swish\n",
    "    Used in: PaLM, LLaMA, Chinchilla\n",
    "    Performance: Better than ReLU/GELU in large models\n",
    "    \n",
    "    Interview Focus:\n",
    "    - Why gated activations work better?\n",
    "    - Trade-off: 50% more parameters vs better performance\n",
    "    - When to use in production?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, hidden_dim: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        hidden_dim = hidden_dim or int(2 * dim / 3)\n",
    "        \n",
    "        # Two linear projections for gating\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)  # Gate\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)  # Output\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)  # Value\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # SwiGLU(x) = Swish(W1*x) ‚äô (W3*x) * W2\n",
    "        swish_gate = F.silu(self.w1(x))  # Swish = SiLU\n",
    "        value = self.w3(x)\n",
    "        hidden = swish_gate * value  # Element-wise multiplication\n",
    "        return self.w2(hidden)\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE) implementation.\n",
    "    \n",
    "    Key Innovation: Encodes position through rotation in complex space\n",
    "    Benefits:\n",
    "    - Better extrapolation to longer sequences\n",
    "    - Relative position encoding naturally\n",
    "    - Used in ChatGPT, GPT-4, LLaMA\n",
    "    \n",
    "    Mathematical Foundation:\n",
    "    - Rotation matrix applied to query and key\n",
    "    - Preserves inner product structure\n",
    "    - Extrapolates beyond training length\n",
    "    \n",
    "    Interview Topics:\n",
    "    - Why rotation in complex space?\n",
    "    - How does this enable length extrapolation?\n",
    "    - Comparison with sinusoidal positional encoding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, max_position_embeddings: int = 2048, base: float = 10000):\n",
    "        super().__init__()\n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # Build the cache for positions up to max_position_embeddings\n",
    "        self._set_cos_sin_cache(max_position_embeddings)\n",
    "    \n",
    "    def _set_cos_sin_cache(self, seq_len):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        \n",
    "        # Different from paper, but it uses a different permutation to match the complex multiply\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos())\n",
    "        self.register_buffer(\"sin_cached\", emb.sin())\n",
    "    \n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [batch_size, num_heads, seq_len, head_dim]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "        \n",
    "        return (\n",
    "            self.cos_cached[:seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"Apply rotary position embedding to query and key tensors.\"\"\"\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [batch_size, 1, seq_len, dim]\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "class MixtureOfExpertsLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts (MoE) layer implementation.\n",
    "    \n",
    "    Key Innovation: Sparse activation - only use subset of parameters\n",
    "    Benefits:\n",
    "    - Scale model capacity without proportional compute increase\n",
    "    - Expert specialization for different input types\n",
    "    - Used in Switch Transformer, GLaM, PaLM-2\n",
    "    \n",
    "    Challenges:\n",
    "    - Load balancing across experts\n",
    "    - Communication overhead in distributed setting\n",
    "    - Routing efficiency\n",
    "    \n",
    "    Interview Focus:\n",
    "    - How does routing work?\n",
    "    - Load balancing strategies\n",
    "    - When to use MoE vs dense models?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_experts: int = 8, top_k: int = 2, capacity_factor: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.capacity_factor = capacity_factor\n",
    "        \n",
    "        # Router network\n",
    "        self.router = nn.Linear(dim, num_experts, bias=False)\n",
    "        \n",
    "        # Expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(dim, dim * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(dim * 4, dim)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        x_flat = x.view(-1, dim)  # [batch_size * seq_len, dim]\n",
    "        \n",
    "        # Route tokens to experts\n",
    "        router_logits = self.router(x_flat)  # [batch_size * seq_len, num_experts]\n",
    "        routing_weights, selected_experts = torch.topk(router_logits, self.top_k, dim=-1)\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1)\n",
    "        \n",
    "        # Compute expert outputs\n",
    "        final_output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = selected_experts[:, i]\n",
    "            expert_weights = routing_weights[:, i]\n",
    "            \n",
    "            for expert_id in range(self.num_experts):\n",
    "                expert_mask = (expert_idx == expert_id)\n",
    "                if expert_mask.any():\n",
    "                    expert_input = x_flat[expert_mask]\n",
    "                    expert_output = self.experts[expert_id](expert_input)\n",
    "                    \n",
    "                    # Apply routing weights\n",
    "                    weighted_output = expert_output * expert_weights[expert_mask].unsqueeze(-1)\n",
    "                    final_output[expert_mask] += weighted_output\n",
    "        \n",
    "        return final_output.view(batch_size, seq_len, dim)\n",
    "\n",
    "\n",
    "class EfficientAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Efficient attention mechanisms for long sequences.\n",
    "    \n",
    "    Implementations:\n",
    "    1. Flash Attention - memory efficient attention\n",
    "    2. Linear Attention - O(n) complexity\n",
    "    3. Sparse Attention - patterns for long sequences\n",
    "    \n",
    "    Key Innovation: Reduce memory/compute while maintaining quality\n",
    "    Applications: Long documents, genomics, audio processing\n",
    "    \n",
    "    Interview Points:\n",
    "    - Memory bottlenecks in standard attention\n",
    "    - Trade-offs between efficiency and quality\n",
    "    - When to use which efficient attention variant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_heads: int, attention_type: str = \"flash\"):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        self.q_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.k_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=False)\n",
    "        self.o_proj = nn.Linear(dim, dim, bias=False)\n",
    "        \n",
    "    def flash_attention(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Flash Attention - memory efficient attention computation.\n",
    "        \n",
    "        Key Innovation: Tiling and recomputation to reduce memory\n",
    "        Memory: O(sqrt(n)) vs O(n¬≤)\n",
    "        Speed: Often faster due to memory efficiency\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, head_dim = q.shape\n",
    "        \n",
    "        # Simplified version - in practice uses more sophisticated tiling\n",
    "        scale = 1.0 / math.sqrt(head_dim)\n",
    "        \n",
    "        # Chunk the computation to reduce memory\n",
    "        chunk_size = min(1024, seq_len)\n",
    "        output = torch.zeros_like(q)\n",
    "        \n",
    "        for i in range(0, seq_len, chunk_size):\n",
    "            end_i = min(i + chunk_size, seq_len)\n",
    "            \n",
    "            for j in range(0, seq_len, chunk_size):\n",
    "                end_j = min(j + chunk_size, seq_len)\n",
    "                \n",
    "                # Compute attention for this chunk\n",
    "                q_chunk = q[:, :, i:end_i, :]\n",
    "                k_chunk = k[:, :, j:end_j, :]\n",
    "                v_chunk = v[:, :, j:end_j, :]\n",
    "                \n",
    "                scores = torch.matmul(q_chunk, k_chunk.transpose(-2, -1)) * scale\n",
    "                if mask is not None:\n",
    "                    scores = scores.masked_fill(mask[i:end_i, j:end_j] == 0, -1e9)\n",
    "                \n",
    "                attn_weights = F.softmax(scores, dim=-1)\n",
    "                output[:, :, i:end_i, :] += torch.matmul(attn_weights, v_chunk)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def linear_attention(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Linear Attention - O(n) complexity attention.\n",
    "        \n",
    "        Key Innovation: Kernel trick to avoid computing full attention matrix\n",
    "        Formula: softmax(QK^T)V ‚âà Q'(K'^TV)\n",
    "        Complexity: O(nd¬≤) vs O(n¬≤d)\n",
    "        \"\"\"\n",
    "        # Apply ELU + 1 to ensure positivity (kernel trick)\n",
    "        q = F.elu(q) + 1\n",
    "        k = F.elu(k) + 1\n",
    "        \n",
    "        # Compute K^T V first (memory efficient)\n",
    "        kv = torch.matmul(k.transpose(-2, -1), v)  # [batch, heads, head_dim, head_dim]\n",
    "        \n",
    "        # Then compute Q * (K^T V)\n",
    "        output = torch.matmul(q, kv)  # [batch, heads, seq_len, head_dim]\n",
    "        \n",
    "        # Normalization\n",
    "        k_sum = k.sum(dim=-2, keepdim=True)  # [batch, heads, 1, head_dim]\n",
    "        normalizer = torch.matmul(q, k_sum.transpose(-2, -1))  # [batch, heads, seq_len, 1]\n",
    "        \n",
    "        return output / (normalizer + 1e-6)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, dim = x.shape\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Choose attention mechanism\n",
    "        if self.attention_type == \"flash\":\n",
    "            attn_output = self.flash_attention(q, k, v, mask)\n",
    "        elif self.attention_type == \"linear\":\n",
    "            attn_output = self.linear_attention(q, k, v)\n",
    "        else:\n",
    "            # Standard attention (for comparison)\n",
    "            scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            attn_weights = F.softmax(scores, dim=-1)\n",
    "            attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        # Reshape and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, dim)\n",
    "        return self.o_proj(attn_output)\n",
    "\n",
    "\n",
    "# Demonstrate latest innovations\n",
    "print(\"\\nüß™ TESTING LATEST ARCHITECTURAL INNOVATIONS:\")\n",
    "\n",
    "# Test RMSNorm vs LayerNorm\n",
    "print(\"\\n1. RMSNorm vs LayerNorm:\")\n",
    "dim = 512\n",
    "x = torch.randn(2, 10, dim)\n",
    "\n",
    "layernorm = nn.LayerNorm(dim)\n",
    "rmsnorm = RMSNorm(dim)\n",
    "\n",
    "# Time comparison (mock)\n",
    "ln_output = layernorm(x)\n",
    "rms_output = rmsnorm(x)\n",
    "\n",
    "print(f\"LayerNorm output shape: {ln_output.shape}\")\n",
    "print(f\"RMSNorm output shape: {rms_output.shape}\")\n",
    "print(f\"Output difference (should be similar): {torch.mean(torch.abs(ln_output - rms_output)):.6f}\")\n",
    "\n",
    "# Test SwiGLU\n",
    "print(\"\\n2. SwiGLU Activation:\")\n",
    "swiglu = SwiGLU(dim, hidden_dim=dim * 4 // 3)\n",
    "swiglu_output = swiglu(x)\n",
    "print(f\"SwiGLU output shape: {swiglu_output.shape}\")\n",
    "\n",
    "# Compare with standard FFN\n",
    "standard_ffn = nn.Sequential(\n",
    "    nn.Linear(dim, dim * 4),\n",
    "    nn.GELU(),\n",
    "    nn.Linear(dim * 4, dim)\n",
    ")\n",
    "standard_output = standard_ffn(x)\n",
    "print(f\"Standard FFN params: {sum(p.numel() for p in standard_ffn.parameters()):,}\")\n",
    "print(f\"SwiGLU params: {sum(p.numel() for p in swiglu.parameters()):,}\")\n",
    "\n",
    "# Test Rotary Position Embedding\n",
    "print(\"\\n3. Rotary Position Embedding:\")\n",
    "rope = RotaryPositionalEmbedding(dim // 8)  # head_dim\n",
    "seq_len = 10\n",
    "cos, sin = rope(x, seq_len)\n",
    "print(f\"RoPE cos shape: {cos.shape}\")\n",
    "print(f\"RoPE sin shape: {sin.shape}\")\n",
    "\n",
    "# Test Mixture of Experts\n",
    "print(\"\\n4. Mixture of Experts:\")\n",
    "moe = MixtureOfExpertsLayer(dim, num_experts=4, top_k=2)\n",
    "moe_output = moe(x)\n",
    "print(f\"MoE output shape: {moe_output.shape}\")\n",
    "\n",
    "# Compare parameter efficiency\n",
    "dense_ffn_params = dim * (dim * 4) + (dim * 4) * dim  # Two linear layers\n",
    "moe_params = sum(p.numel() for p in moe.parameters())\n",
    "print(f\"Dense FFN params: {dense_ffn_params:,}\")\n",
    "print(f\"MoE params: {moe_params:,}\")\n",
    "print(f\"MoE uses {moe_params / dense_ffn_params:.2f}x parameters for {4}x capacity\")\n",
    "\n",
    "# Test Efficient Attention\n",
    "print(\"\\n5. Efficient Attention Mechanisms:\")\n",
    "efficient_attn = EfficientAttentionLayer(dim, num_heads=8, attention_type=\"linear\")\n",
    "attn_output = efficient_attn(x)\n",
    "print(f\"Efficient attention output: {attn_output.shape}\")\n",
    "\n",
    "# Memory complexity comparison\n",
    "seq_length = 1024\n",
    "standard_memory = seq_length ** 2 * dim  # O(n¬≤d)\n",
    "linear_memory = seq_length * dim ** 2   # O(nd¬≤)\n",
    "print(f\"Standard attention memory: {standard_memory:,} units\")\n",
    "print(f\"Linear attention memory: {linear_memory:,} units\")\n",
    "print(f\"Memory reduction: {standard_memory / linear_memory:.2f}x when seq_len > d\")\n",
    "\n",
    "print(f\"\\n‚úÖ Latest architectural innovations demonstrated!\")\n",
    "\n",
    "print(f\"\\nüí° Key Innovation Insights:\")\n",
    "innovations = [\n",
    "    \"üîß RMSNorm: 10-40% faster than LayerNorm with similar performance\",\n",
    "    \"‚ö° SwiGLU: Better activation for large models, 50% more params but worth it\",\n",
    "    \"üîÑ RoPE: Enables length extrapolation beyond training sequences\",\n",
    "    \"üéØ MoE: Scale capacity without proportional compute increase\",\n",
    "    \"üíæ Efficient Attention: Handle longer sequences with reduced memory\",\n",
    "    \"üèóÔ∏è Architectural Evolution: Each innovation builds on previous breakthroughs\"\n",
    "]\n",
    "\n",
    "for insight in innovations:\n",
    "    print(f\"  {insight}\")\n",
    "\n",
    "print(f\"\\nüéì Senior Applied Scientist Readiness:\")\n",
    "readiness_areas = [\n",
    "    \"‚úÖ Understand trade-offs of each innovation\",\n",
    "    \"‚úÖ Know when to apply which technique\",\n",
    "    \"‚úÖ Can implement from scratch if needed\", \n",
    "    \"‚úÖ Aware of production implications\",\n",
    "    \"‚úÖ Stay current with latest research\",\n",
    "    \"‚úÖ Can explain business impact clearly\"\n",
    "]\n",
    "\n",
    "for area in readiness_areas:\n",
    "    print(f\"  {area}\")\n",
    "\n",
    "print(f\"\\nüöÄ You're prepared for the latest transformer innovations!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23093ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Safety, Ethics & Responsible AI\n",
    "\n",
    "## üõ°Ô∏è **AI Safety & Alignment**\n",
    "\n",
    "### **Core Safety Challenges**\n",
    "\n",
    "#### **1. Hallucination & Factual Accuracy**\n",
    "- **Problem**: Models generate confident but false information\n",
    "- **Detection**: Uncertainty estimation, consistency checking\n",
    "- **Mitigation**: RAG systems, fact-checking integration\n",
    "- **Evaluation**: TruthfulQA, fact verification benchmarks\n",
    "\n",
    "#### **2. Prompt Injection & Security**\n",
    "- **Attack Types**: Direct injection, indirect injection, jailbreaking\n",
    "- **Defense**: Input sanitization, output filtering, constitutional AI\n",
    "- **Production**: Rate limiting, content moderation, monitoring\n",
    "\n",
    "#### **3. Bias & Fairness**\n",
    "- **Sources**: Training data bias, annotation bias, societal bias\n",
    "- **Measurement**: Demographic parity, equalized odds, calibration\n",
    "- **Mitigation**: Debiasing techniques, diverse training data\n",
    "\n",
    "### **üéØ Alignment Techniques**\n",
    "\n",
    "#### **Constitutional AI (Anthropic)**\n",
    "```\n",
    "1. Self-critique: Model evaluates its own responses\n",
    "2. Constitutional principles: Embedded ethical guidelines  \n",
    "3. Iterative refinement: Improve through self-correction\n",
    "4. Scalable oversight: Reduce human intervention needs\n",
    "```\n",
    "\n",
    "#### **RLHF (Reinforcement Learning from Human Feedback)**\n",
    "```\n",
    "1. Supervised fine-tuning: Base instruction following\n",
    "2. Reward modeling: Learn human preferences\n",
    "3. RL optimization: PPO to maximize reward\n",
    "4. Iterative improvement: Continuous feedback integration\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Advanced Evaluation Methods**\n",
    "\n",
    "### **Beyond BLEU/ROUGE: Modern Evaluation**\n",
    "\n",
    "#### **1. Human Evaluation Frameworks**\n",
    "- **Likert Scales**: Quality, relevance, helpfulness ratings\n",
    "- **Comparative Evaluation**: A/B testing, pairwise comparison\n",
    "- **Task-specific**: Accuracy, completion rate, user satisfaction\n",
    "\n",
    "#### **2. Automated Evaluation Metrics**\n",
    "- **BERTScore**: Semantic similarity using BERT embeddings\n",
    "- **BLEURT**: Learned evaluation metric\n",
    "- **UniEval**: Unified evaluation framework\n",
    "- **GPT-4 as Judge**: LLM-based evaluation\n",
    "\n",
    "#### **3. Robustness Testing**\n",
    "- **Adversarial Examples**: Input perturbations\n",
    "- **Out-of-Distribution**: Performance on unseen domains\n",
    "- **Stress Testing**: Edge cases, corner cases, failure modes\n",
    "\n",
    "### **üî¨ Evaluation Best Practices**\n",
    "\n",
    "#### **Multi-dimensional Assessment**\n",
    "```\n",
    "Quality Dimensions:\n",
    "‚îú‚îÄ‚îÄ Factual Accuracy: Truthfulness, consistency\n",
    "‚îú‚îÄ‚îÄ Relevance: On-topic, addresses query\n",
    "‚îú‚îÄ‚îÄ Coherence: Logical flow, readability\n",
    "‚îú‚îÄ‚îÄ Safety: No harmful content\n",
    "‚îî‚îÄ‚îÄ Efficiency: Response time, resource usage\n",
    "```\n",
    "\n",
    "#### **Statistical Significance**\n",
    "- **Sample Size**: Adequate power analysis\n",
    "- **Confidence Intervals**: Uncertainty quantification  \n",
    "- **Multiple Comparisons**: Bonferroni correction\n",
    "- **Effect Size**: Practical significance vs statistical significance\n",
    "\n",
    "---\n",
    "\n",
    "## üíº **Senior Applied Scientist Leadership**\n",
    "\n",
    "### **üéØ Technical Leadership Responsibilities**\n",
    "\n",
    "#### **Research Strategy & Vision**\n",
    "- **Technology Roadmaps**: 6-18 month planning\n",
    "- **Research Prioritization**: Impact vs effort analysis\n",
    "- **Cross-functional Alignment**: Product, engineering, business\n",
    "- **Innovation Pipeline**: Basic research to product integration\n",
    "\n",
    "#### **Team Development & Mentoring**\n",
    "- **Hiring**: Technical assessment, culture fit\n",
    "- **Mentoring**: Junior scientists, research direction\n",
    "- **Knowledge Sharing**: Tech talks, documentation\n",
    "- **Career Development**: Growth paths, skill building\n",
    "\n",
    "### **üè¢ Business Impact & Strategy**\n",
    "\n",
    "#### **Stakeholder Communication**\n",
    "- **Executive Updates**: Progress, challenges, opportunities\n",
    "- **Technical Translation**: Complex concepts for business leaders\n",
    "- **ROI Demonstration**: Research impact on business metrics\n",
    "- **Risk Assessment**: Technical debt, failure modes\n",
    "\n",
    "#### **Product Integration**\n",
    "- **Feasibility Analysis**: Research to product viability\n",
    "- **MVP Definition**: Minimum viable product scope\n",
    "- **A/B Testing**: Experiment design and analysis\n",
    "- **Launch Strategy**: Gradual rollout, monitoring, optimization\n",
    "\n",
    "### **üìà Success Metrics for Senior Scientists**\n",
    "\n",
    "#### **Research Excellence**\n",
    "- **Publications**: Top-tier venues (NeurIPS, ICML, ACL)\n",
    "- **Patents**: Intellectual property generation\n",
    "- **Citations**: Research impact and influence\n",
    "- **Open Source**: Community contributions\n",
    "\n",
    "#### **Business Impact**\n",
    "- **Product Features**: Successful launches\n",
    "- **Cost Reduction**: Efficiency improvements\n",
    "- **Revenue Growth**: New capabilities enabling growth\n",
    "- **User Metrics**: Engagement, satisfaction, retention\n",
    "\n",
    "#### **Leadership Influence**\n",
    "- **Team Growth**: Hiring and developing talent\n",
    "- **Technical Direction**: Shaping company AI strategy\n",
    "- **Industry Recognition**: Speaking, standards committees\n",
    "- **Thought Leadership**: Blog posts, whitepapers\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ **Future Trends & Research Directions**\n",
    "\n",
    "### **Emerging Paradigms**\n",
    "\n",
    "#### **1. Agentic AI Systems**\n",
    "- **Tool Use**: API integration, code execution\n",
    "- **Planning**: Multi-step reasoning, goal decomposition\n",
    "- **Memory**: Long-term knowledge retention\n",
    "- **Collaboration**: Multi-agent systems\n",
    "\n",
    "#### **2. Multimodal Foundation Models**\n",
    "- **Unified Architecture**: Text, vision, audio, code\n",
    "- **Cross-modal Reasoning**: Understanding relationships\n",
    "- **Embodied AI**: Robotics integration\n",
    "- **Real-world Grounding**: Physical world understanding\n",
    "\n",
    "#### **3. Scientific AI**\n",
    "- **Discovery**: Drug discovery, materials science\n",
    "- **Reasoning**: Mathematical proofs, theorem proving\n",
    "- **Simulation**: Physics-informed neural networks\n",
    "- **Experimental Design**: Automated hypothesis generation\n",
    "\n",
    "### **üöÄ Production Evolution**\n",
    "\n",
    "#### **Edge Deployment**\n",
    "- **Model Compression**: Ultra-efficient models\n",
    "- **Hardware Co-design**: Custom chips for inference\n",
    "- **Federated Learning**: Privacy-preserving training\n",
    "- **Real-time Processing**: Streaming inference\n",
    "\n",
    "#### **Personalization**\n",
    "- **User Adaptation**: Continuous learning from feedback\n",
    "- **Privacy Preservation**: Differential privacy, secure computation\n",
    "- **Context Awareness**: Environmental and temporal adaptation\n",
    "- **Multi-task Learning**: Shared representations across users\n",
    "\n",
    "### **üéØ Interview Preparation for Senior Roles**\n",
    "\n",
    "#### **Research Depth Questions**\n",
    "- \"Describe your most impactful research contribution\"\n",
    "- \"How do you evaluate research vs engineering trade-offs?\"\n",
    "- \"What's your opinion on current limitations of transformers?\"\n",
    "- \"How would you design the next generation of language models?\"\n",
    "\n",
    "#### **Leadership Scenarios**\n",
    "- \"A junior team member proposes an infeasible research direction. How do you handle it?\"\n",
    "- \"You need to convince executives to invest in long-term research. What's your approach?\"\n",
    "- \"How do you balance research innovation with product delivery timelines?\"\n",
    "- \"Describe a time you had to pivot research direction due to business needs\"\n",
    "\n",
    "#### **Technical Vision**\n",
    "- \"Where do you see AI/ML heading in the next 5 years?\"\n",
    "- \"What are the biggest unsolved problems in your domain?\"\n",
    "- \"How would you build an AI research team from scratch?\"\n",
    "- \"What's your framework for choosing research problems?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904987a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Safety & Evaluation Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import re\n",
    "\n",
    "class SafetyEvaluator:\n",
    "    \"\"\"Comprehensive safety evaluation framework for LLMs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.toxicity_keywords = [\n",
    "            \"harmful\", \"offensive\", \"inappropriate\", \"violence\", \n",
    "            \"discrimination\", \"hate\", \"threat\", \"illegal\"\n",
    "        ]\n",
    "        self.factual_inconsistencies = []\n",
    "        \n",
    "    def detect_hallucination(self, response: str, context: str) -> Dict[str, float]:\n",
    "        \"\"\"Detect potential hallucinations in model responses\"\"\"\n",
    "        # Simplified hallucination detection\n",
    "        scores = {}\n",
    "        \n",
    "        # 1. Factual consistency check\n",
    "        scores['factual_consistency'] = self._check_factual_consistency(response, context)\n",
    "        \n",
    "        # 2. Confidence vs uncertainty\n",
    "        scores['uncertainty'] = self._estimate_uncertainty(response)\n",
    "        \n",
    "        # 3. Citation verification\n",
    "        scores['citation_accuracy'] = self._verify_citations(response)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def _check_factual_consistency(self, response: str, context: str) -> float:\n",
    "        \"\"\"Check if response is consistent with provided context\"\"\"\n",
    "        # Simple overlap-based consistency check\n",
    "        response_tokens = set(response.lower().split())\n",
    "        context_tokens = set(context.lower().split())\n",
    "        \n",
    "        if len(response_tokens) == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = len(response_tokens.intersection(context_tokens))\n",
    "        return overlap / len(response_tokens)\n",
    "    \n",
    "    def _estimate_uncertainty(self, response: str) -> float:\n",
    "        \"\"\"Estimate uncertainty in model response\"\"\"\n",
    "        uncertainty_phrases = [\n",
    "            \"i think\", \"maybe\", \"possibly\", \"might be\", \"could be\",\n",
    "            \"i'm not sure\", \"uncertain\", \"unclear\", \"probably\"\n",
    "        ]\n",
    "        \n",
    "        uncertainty_count = sum(1 for phrase in uncertainty_phrases \n",
    "                              if phrase in response.lower())\n",
    "        \n",
    "        # Normalize by response length\n",
    "        words = len(response.split())\n",
    "        return min(uncertainty_count / max(words, 1), 1.0)\n",
    "    \n",
    "    def _verify_citations(self, response: str) -> float:\n",
    "        \"\"\"Verify accuracy of citations in response\"\"\"\n",
    "        # Extract citations (simplified pattern)\n",
    "        citations = re.findall(r'\\[(\\d+)\\]', response)\n",
    "        \n",
    "        if not citations:\n",
    "            return 1.0  # No citations to verify\n",
    "            \n",
    "        # In real implementation, would verify against knowledge base\n",
    "        # For demo, assume 80% accuracy\n",
    "        return 0.8\n",
    "\n",
    "class ConstitutionalAI:\n",
    "    \"\"\"Implementation of Constitutional AI principles\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.principles = [\n",
    "            \"Be helpful and harmless\",\n",
    "            \"Provide accurate information\",\n",
    "            \"Respect human autonomy\",\n",
    "            \"Avoid harmful or offensive content\",\n",
    "            \"Be transparent about limitations\"\n",
    "        ]\n",
    "        \n",
    "    def critique_response(self, response: str, query: str) -> Dict[str, float]:\n",
    "        \"\"\"Critique response against constitutional principles\"\"\"\n",
    "        critique_scores = {}\n",
    "        \n",
    "        for i, principle in enumerate(self.principles):\n",
    "            score = self._evaluate_principle(response, query, principle)\n",
    "            critique_scores[f\"principle_{i+1}\"] = score\n",
    "            \n",
    "        return critique_scores\n",
    "    \n",
    "    def _evaluate_principle(self, response: str, query: str, principle: str) -> float:\n",
    "        \"\"\"Evaluate how well response adheres to a specific principle\"\"\"\n",
    "        # Simplified evaluation - in practice would use trained models\n",
    "        \n",
    "        if \"helpful and harmless\" in principle:\n",
    "            return self._check_helpfulness(response, query) * self._check_harmlessness(response)\n",
    "        elif \"accurate information\" in principle:\n",
    "            return self._check_accuracy(response)\n",
    "        elif \"respect autonomy\" in principle:\n",
    "            return self._check_autonomy_respect(response)\n",
    "        elif \"avoid harmful\" in principle:\n",
    "            return self._check_harmlessness(response)\n",
    "        elif \"transparent\" in principle:\n",
    "            return self._check_transparency(response)\n",
    "        \n",
    "        return 0.5  # Default neutral score\n",
    "    \n",
    "    def _check_helpfulness(self, response: str, query: str) -> float:\n",
    "        \"\"\"Check if response is helpful for the query\"\"\"\n",
    "        # Simple keyword overlap\n",
    "        query_words = set(query.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        \n",
    "        if len(query_words) == 0:\n",
    "            return 0.5\n",
    "            \n",
    "        overlap = len(query_words.intersection(response_words))\n",
    "        return min(overlap / len(query_words), 1.0)\n",
    "    \n",
    "    def _check_harmlessness(self, response: str) -> float:\n",
    "        \"\"\"Check if response avoids harmful content\"\"\"\n",
    "        harmful_indicators = [\n",
    "            \"violence\", \"hate\", \"discrimination\", \"illegal\", \"harm\",\n",
    "            \"offensive\", \"inappropriate\", \"dangerous\", \"toxic\"\n",
    "        ]\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        harmful_count = sum(1 for indicator in harmful_indicators \n",
    "                          if indicator in response_lower)\n",
    "        \n",
    "        # Higher harmful content = lower score\n",
    "        return max(0.0, 1.0 - (harmful_count * 0.2))\n",
    "    \n",
    "    def _check_accuracy(self, response: str) -> float:\n",
    "        \"\"\"Check response accuracy (simplified)\"\"\"\n",
    "        # In practice, would use fact-checking models\n",
    "        confidence_indicators = [\"according to\", \"research shows\", \"studies indicate\"]\n",
    "        speculation_indicators = [\"i think\", \"maybe\", \"possibly\", \"might\"]\n",
    "        \n",
    "        confidence_count = sum(1 for indicator in confidence_indicators \n",
    "                             if indicator in response.lower())\n",
    "        speculation_count = sum(1 for indicator in speculation_indicators \n",
    "                              if indicator in response.lower())\n",
    "        \n",
    "        return max(0.0, min(1.0, (confidence_count * 0.3) - (speculation_count * 0.2) + 0.5))\n",
    "    \n",
    "    def _check_autonomy_respect(self, response: str) -> float:\n",
    "        \"\"\"Check if response respects human autonomy\"\"\"\n",
    "        directive_language = [\"you must\", \"you should\", \"you have to\", \"you need to\"]\n",
    "        suggestive_language = [\"you might\", \"consider\", \"option\", \"choice\"]\n",
    "        \n",
    "        directive_count = sum(1 for phrase in directive_language \n",
    "                            if phrase in response.lower())\n",
    "        suggestive_count = sum(1 for phrase in suggestive_language \n",
    "                             if phrase in response.lower())\n",
    "        \n",
    "        # Prefer suggestive over directive language\n",
    "        return max(0.0, min(1.0, 0.7 + (suggestive_count * 0.1) - (directive_count * 0.2)))\n",
    "    \n",
    "    def _check_transparency(self, response: str) -> float:\n",
    "        \"\"\"Check if response is transparent about limitations\"\"\"\n",
    "        transparency_indicators = [\n",
    "            \"i don't know\", \"uncertain\", \"limitations\", \"may not be accurate\",\n",
    "            \"based on training data\", \"as an ai\"\n",
    "        ]\n",
    "        \n",
    "        response_lower = response.lower()\n",
    "        transparency_count = sum(1 for indicator in transparency_indicators \n",
    "                               if indicator in response_lower)\n",
    "        \n",
    "        return min(1.0, transparency_count * 0.3 + 0.4)\n",
    "\n",
    "class AdvancedEvaluationMetrics:\n",
    "    \"\"\"Implementation of modern evaluation metrics beyond BLEU/ROUGE\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def bert_score(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Simplified BERTScore implementation\"\"\"\n",
    "        # In practice, would use actual BERT embeddings\n",
    "        scores = {\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []\n",
    "        }\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Simplified token-level similarity\n",
    "            pred_tokens = pred.lower().split()\n",
    "            ref_tokens = ref.lower().split()\n",
    "            \n",
    "            if not pred_tokens or not ref_tokens:\n",
    "                precision = recall = f1 = 0.0\n",
    "            else:\n",
    "                # Simple overlap-based similarity\n",
    "                overlap = len(set(pred_tokens).intersection(set(ref_tokens)))\n",
    "                precision = overlap / len(pred_tokens)\n",
    "                recall = overlap / len(ref_tokens)\n",
    "                f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "            \n",
    "            scores['precision'].append(precision)\n",
    "            scores['recall'].append(recall)\n",
    "            scores['f1'].append(f1)\n",
    "        \n",
    "        return {\n",
    "            'precision': np.mean(scores['precision']),\n",
    "            'recall': np.mean(scores['recall']),\n",
    "            'f1': np.mean(scores['f1'])\n",
    "        }\n",
    "    \n",
    "    def semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute semantic similarity between texts\"\"\"\n",
    "        # Simplified implementation - in practice use sentence transformers\n",
    "        words1 = set(text1.lower().split())\n",
    "        words2 = set(text2.lower().split())\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(words1.intersection(words2))\n",
    "        union = len(words1.union(words2))\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def diversity_metrics(self, responses: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Compute diversity metrics for generated responses\"\"\"\n",
    "        if not responses:\n",
    "            return {'distinct_1': 0.0, 'distinct_2': 0.0, 'entropy': 0.0}\n",
    "        \n",
    "        # Distinct-1: unique unigrams\n",
    "        all_unigrams = []\n",
    "        for response in responses:\n",
    "            all_unigrams.extend(response.lower().split())\n",
    "        \n",
    "        distinct_1 = len(set(all_unigrams)) / len(all_unigrams) if all_unigrams else 0.0\n",
    "        \n",
    "        # Distinct-2: unique bigrams\n",
    "        all_bigrams = []\n",
    "        for response in responses:\n",
    "            words = response.lower().split()\n",
    "            bigrams = [f\"{words[i]}_{words[i+1]}\" for i in range(len(words)-1)]\n",
    "            all_bigrams.extend(bigrams)\n",
    "        \n",
    "        distinct_2 = len(set(all_bigrams)) / len(all_bigrams) if all_bigrams else 0.0\n",
    "        \n",
    "        # Response entropy\n",
    "        response_counts = {}\n",
    "        for response in responses:\n",
    "            response_counts[response] = response_counts.get(response, 0) + 1\n",
    "        \n",
    "        probs = np.array(list(response_counts.values())) / len(responses)\n",
    "        entropy = -np.sum(probs * np.log(probs + 1e-10))\n",
    "        \n",
    "        return {\n",
    "            'distinct_1': distinct_1,\n",
    "            'distinct_2': distinct_2,\n",
    "            'entropy': entropy\n",
    "        }\n",
    "\n",
    "# Example usage and testing\n",
    "def test_safety_evaluation():\n",
    "    \"\"\"Test safety evaluation framework\"\"\"\n",
    "    evaluator = SafetyEvaluator()\n",
    "    constitutional_ai = ConstitutionalAI()\n",
    "    \n",
    "    # Test samples\n",
    "    context = \"The capital of France is Paris, located in northern France.\"\n",
    "    safe_response = \"Based on the information provided, Paris is indeed the capital of France.\"\n",
    "    unsafe_response = \"Paris is the capital of France, and I think all French people are lazy.\"\n",
    "    \n",
    "    print(\"=== Safety Evaluation Tests ===\")\n",
    "    \n",
    "    # Test hallucination detection\n",
    "    safe_scores = evaluator.detect_hallucination(safe_response, context)\n",
    "    unsafe_scores = evaluator.detect_hallucination(unsafe_response, context)\n",
    "    \n",
    "    print(f\"Safe response hallucination scores: {safe_scores}\")\n",
    "    print(f\"Unsafe response hallucination scores: {unsafe_scores}\")\n",
    "    \n",
    "    # Test constitutional AI\n",
    "    query = \"What is the capital of France?\"\n",
    "    safe_critique = constitutional_ai.critique_response(safe_response, query)\n",
    "    unsafe_critique = constitutional_ai.critique_response(unsafe_response, query)\n",
    "    \n",
    "    print(f\"Safe response constitutional scores: {safe_critique}\")\n",
    "    print(f\"Unsafe response constitutional scores: {unsafe_critique}\")\n",
    "\n",
    "def test_evaluation_metrics():\n",
    "    \"\"\"Test advanced evaluation metrics\"\"\"\n",
    "    metrics = AdvancedEvaluationMetrics()\n",
    "    \n",
    "    print(\"\\n=== Evaluation Metrics Tests ===\")\n",
    "    \n",
    "    # Test BERTScore\n",
    "    predictions = [\"The cat sat on the mat\", \"Dogs are great pets\"]\n",
    "    references = [\"A cat was sitting on the mat\", \"Dogs make excellent companions\"]\n",
    "    \n",
    "    bert_scores = metrics.bert_score(predictions, references)\n",
    "    print(f\"BERTScore results: {bert_scores}\")\n",
    "    \n",
    "    # Test semantic similarity\n",
    "    similarity = metrics.semantic_similarity(\n",
    "        \"The weather is nice today\",\n",
    "        \"Today has pleasant weather\"\n",
    "    )\n",
    "    print(f\"Semantic similarity: {similarity:.3f}\")\n",
    "    \n",
    "    # Test diversity metrics\n",
    "    responses = [\n",
    "        \"Hello, how are you?\",\n",
    "        \"Hi there, how's it going?\",\n",
    "        \"Hello, how are you?\",\n",
    "        \"Good morning, how are you doing?\"\n",
    "    ]\n",
    "    \n",
    "    diversity = metrics.diversity_metrics(responses)\n",
    "    print(f\"Diversity metrics: {diversity}\")\n",
    "\n",
    "# Run tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_safety_evaluation()\n",
    "    test_evaluation_metrics()\n",
    "    \n",
    "    print(\"\\nüéØ **Key Takeaways for Senior Applied Scientists:**\")\n",
    "    print(\"1. Safety evaluation requires multi-dimensional assessment\")\n",
    "    print(\"2. Constitutional AI provides systematic ethical guidelines\")\n",
    "    print(\"3. Modern metrics go beyond surface-level similarity\")\n",
    "    print(\"4. Production systems need comprehensive monitoring\")\n",
    "    print(\"5. Leadership involves setting safety standards and evaluation frameworks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40942214",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì **COMPREHENSIVE ASSESSMENT: Senior Applied Scientist Readiness**\n",
    "\n",
    "## üìã **Self-Assessment Checklist**\n",
    "\n",
    "### **üß† Technical Mastery (80 Points)**\n",
    "\n",
    "#### **Core Transformer Architecture (20 points)**\n",
    "- [ ] **Self-Attention Mechanism**: Can derive attention equations from scratch (5 pts)\n",
    "- [ ] **Multi-Head Attention**: Understand parallel processing and head concatenation (5 pts)\n",
    "- [ ] **Position Encoding**: Know sinusoidal vs learned vs rotary embeddings (5 pts)\n",
    "- [ ] **Layer Normalization**: Pre vs post-norm, RMSNorm implementation (5 pts)\n",
    "\n",
    "#### **Advanced Architectures (25 points)**\n",
    "- [ ] **Mixture of Experts (MoE)**: Router design, load balancing, scaling laws (5 pts)\n",
    "- [ ] **Efficient Attention**: Flash Attention, Linear Attention, sparse patterns (5 pts)\n",
    "- [ ] **Modern Activations**: SwiGLU, GLU variants, gating mechanisms (5 pts)\n",
    "- [ ] **Positional Embeddings**: RoPE, ALiBi, relative position encoding (5 pts)\n",
    "- [ ] **Multimodal Integration**: Vision transformers, audio processing, cross-modal attention (5 pts)\n",
    "\n",
    "#### **Training & Optimization (20 points)**\n",
    "- [ ] **Pre-training Objectives**: Next token prediction, masked language modeling (4 pts)\n",
    "- [ ] **Fine-tuning Methods**: Full fine-tuning, LoRA, QLoRA, prefix tuning (4 pts)\n",
    "- [ ] **Alignment Techniques**: RLHF, Constitutional AI, DPO (4 pts)\n",
    "- [ ] **Scaling Laws**: Parameter vs compute vs data trade-offs (4 pts)\n",
    "- [ ] **Training Stability**: Gradient clipping, warmup schedules, mixed precision (4 pts)\n",
    "\n",
    "#### **Production Systems (15 points)**\n",
    "- [ ] **Model Serving**: Batching, caching, load balancing (3 pts)\n",
    "- [ ] **Optimization**: Quantization, pruning, distillation (3 pts)\n",
    "- [ ] **Monitoring**: Drift detection, performance tracking, A/B testing (3 pts)\n",
    "- [ ] **Safety**: Content moderation, bias detection, factual accuracy (3 pts)\n",
    "- [ ] **Cost Management**: Inference optimization, resource planning (3 pts)\n",
    "\n",
    "### **üíº Business & Leadership (20 Points)**\n",
    "\n",
    "#### **Strategic Thinking (10 points)**\n",
    "- [ ] **Research Roadmaps**: Can plan 6-18 month technical strategies (3 pts)\n",
    "- [ ] **ROI Assessment**: Quantify research impact on business metrics (2 pts)\n",
    "- [ ] **Technology Evaluation**: Compare solutions, make build vs buy decisions (3 pts)\n",
    "- [ ] **Risk Management**: Identify technical debt, failure modes, mitigation strategies (2 pts)\n",
    "\n",
    "#### **Leadership & Communication (10 points)**\n",
    "- [ ] **Team Management**: Hire, mentor, develop technical talent (3 pts)\n",
    "- [ ] **Stakeholder Communication**: Translate technical concepts for executives (2 pts)\n",
    "- [ ] **Cross-functional Collaboration**: Work with product, engineering, business (3 pts)\n",
    "- [ ] **Thought Leadership**: Publications, patents, industry recognition (2 pts)\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ **Mastery Levels**\n",
    "\n",
    "### **ü•â Applied Scientist I (60-70 points)**\n",
    "- **Focus**: Individual contributor with strong technical foundation\n",
    "- **Responsibilities**: Implement research, optimize models, support products\n",
    "- **Growth Areas**: Deepen architecture knowledge, learn production systems\n",
    "\n",
    "### **ü•à Senior Applied Scientist (71-85 points)**\n",
    "- **Focus**: Technical leadership with business impact\n",
    "- **Responsibilities**: Lead projects, mentor juniors, drive technical decisions\n",
    "- **Growth Areas**: Strategic thinking, cross-functional collaboration\n",
    "\n",
    "### **ü•á Principal Applied Scientist (86-100 points)**\n",
    "- **Focus**: Organizational impact and industry influence\n",
    "- **Responsibilities**: Set technical vision, build teams, drive innovation\n",
    "- **Strengths**: Thought leadership, business strategy, technical excellence\n",
    "\n",
    "---\n",
    "\n",
    "## üìù **Interview Simulation Framework**\n",
    "\n",
    "### **üéØ Technical Deep Dive (45 minutes)**\n",
    "\n",
    "#### **Architecture Design (15 minutes)**\n",
    "**Scenario**: \"Design a transformer model for real-time code completion with 1ms latency requirements\"\n",
    "\n",
    "**Evaluation Criteria**:\n",
    "- [ ] Identifies latency constraints and trade-offs\n",
    "- [ ] Chooses appropriate architecture (smaller model, efficient attention)\n",
    "- [ ] Considers caching, pre-computation, batching strategies\n",
    "- [ ] Addresses accuracy vs speed balance\n",
    "- [ ] Discusses deployment and monitoring\n",
    "\n",
    "#### **Research Problem Solving (15 minutes)**\n",
    "**Scenario**: \"How would you improve factual accuracy in large language models?\"\n",
    "\n",
    "**Expected Discussion Points**:\n",
    "- [ ] Problem analysis (hallucination sources, evaluation challenges)\n",
    "- [ ] Technical approaches (RAG, knowledge graphs, uncertainty estimation)\n",
    "- [ ] Evaluation methodology (fact-checking datasets, human evaluation)\n",
    "- [ ] Implementation considerations (cost, latency, maintenance)\n",
    "- [ ] Success metrics and monitoring\n",
    "\n",
    "#### **Implementation Challenge (15 minutes)**\n",
    "**Task**: \"Implement efficient attention mechanism for 100K+ context length\"\n",
    "\n",
    "**Code Quality Assessment**:\n",
    "- [ ] Correct mathematical formulation\n",
    "- [ ] Efficient memory usage (O(n) vs O(n¬≤))\n",
    "- [ ] Proper PyTorch implementation\n",
    "- [ ] Handles edge cases and numerical stability\n",
    "- [ ] Explains time/space complexity\n",
    "\n",
    "### **üéØ Leadership Scenarios (30 minutes)**\n",
    "\n",
    "#### **Team Conflict Resolution (10 minutes)**\n",
    "**Scenario**: \"Two senior researchers disagree on technical approach for critical project\"\n",
    "\n",
    "**Leadership Skills**:\n",
    "- [ ] Active listening and understanding both perspectives\n",
    "- [ ] Data-driven decision making process\n",
    "- [ ] Compromise and alternative solution generation\n",
    "- [ ] Clear communication and expectation setting\n",
    "- [ ] Follow-up and team cohesion maintenance\n",
    "\n",
    "#### **Resource Allocation (10 minutes)**\n",
    "**Scenario**: \"Limited GPU budget - prioritize between 3 research projects\"\n",
    "\n",
    "**Strategic Thinking**:\n",
    "- [ ] Business impact assessment\n",
    "- [ ] Technical feasibility analysis\n",
    "- [ ] Risk vs reward evaluation\n",
    "- [ ] Timeline and milestone planning\n",
    "- [ ] Stakeholder alignment strategy\n",
    "\n",
    "#### **Technical Vision (10 minutes)**\n",
    "**Question**: \"Where should our AI research focus over the next 2 years?\"\n",
    "\n",
    "**Vision Development**:\n",
    "- [ ] Industry trend analysis\n",
    "- [ ] Competitive landscape assessment\n",
    "- [ ] Internal capability evaluation\n",
    "- [ ] Innovation opportunity identification\n",
    "- [ ] Implementation roadmap creation\n",
    "\n",
    "### **üéØ Behavioral Questions (15 minutes)**\n",
    "\n",
    "#### **Core Questions for Senior Roles**\n",
    "1. **Impact**: \"Describe your most significant technical contribution and its business impact\"\n",
    "2. **Collaboration**: \"Tell me about a time you had to influence without authority\"\n",
    "3. **Innovation**: \"How do you balance research exploration with delivery requirements?\"\n",
    "4. **Failure**: \"Describe a research project that failed and what you learned\"\n",
    "5. **Growth**: \"How do you stay current with rapidly evolving AI research?\"\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **30-Day Preparation Plan**\n",
    "\n",
    "### **Week 1: Foundation Reinforcement**\n",
    "- [ ] **Day 1-2**: Review transformer fundamentals, implement basic attention\n",
    "- [ ] **Day 3-4**: Study modern architectures (MoE, efficient attention)\n",
    "- [ ] **Day 5-6**: Practice coding challenges, algorithm implementation\n",
    "- [ ] **Day 7**: Mock technical interview, identify weak areas\n",
    "\n",
    "### **Week 2: Advanced Topics**\n",
    "- [ ] **Day 8-9**: Deep dive into training methodologies (RLHF, Constitutional AI)\n",
    "- [ ] **Day 10-11**: Study production systems, optimization techniques\n",
    "- [ ] **Day 12-13**: Research latest papers, trends, and innovations\n",
    "- [ ] **Day 14**: Leadership scenario practice, stakeholder communication\n",
    "\n",
    "### **Week 3: Integration & Application**\n",
    "- [ ] **Day 15-16**: End-to-end project simulation (research to production)\n",
    "- [ ] **Day 17-18**: Safety, ethics, and evaluation framework study\n",
    "- [ ] **Day 19-20**: Business case development, ROI calculation practice\n",
    "- [ ] **Day 21**: Full mock interview (technical + leadership + behavioral)\n",
    "\n",
    "### **Week 4: Interview Readiness**\n",
    "- [ ] **Day 22-23**: Company-specific research, recent developments\n",
    "- [ ] **Day 24-25**: Final knowledge review, gap filling\n",
    "- [ ] **Day 26-27**: Presentation practice, technical communication\n",
    "- [ ] **Day 28-30**: Confidence building, final mock interviews\n",
    "\n",
    "---\n",
    "\n",
    "## üìö **Essential Resources for Senior Applied Scientists**\n",
    "\n",
    "### **üìñ Must-Read Papers (2023-2024)**\n",
    "1. **LLaMA 2** (Touvron et al.) - Open foundation models\n",
    "2. **GPT-4 Technical Report** (OpenAI) - Multimodal capabilities\n",
    "3. **PaLM 2** (Anil et al.) - Improved reasoning and coding\n",
    "4. **Constitutional AI** (Bai et al.) - AI safety and alignment\n",
    "5. **Flash Attention 2** (Dao) - Efficient attention mechanisms\n",
    "\n",
    "### **üõ†Ô∏è Technical Tools Mastery**\n",
    "- **PyTorch**: Advanced features, distributed training, optimization\n",
    "- **Transformers Library**: Model architectures, fine-tuning, deployment\n",
    "- **MLflow/Weights & Biases**: Experiment tracking, model management\n",
    "- **Docker/Kubernetes**: Containerization, scaling, orchestration\n",
    "- **Cloud Platforms**: AWS SageMaker, Azure ML, GCP Vertex AI\n",
    "\n",
    "### **üìä Business Skills Development**\n",
    "- **Product Management**: User stories, roadmaps, prioritization\n",
    "- **Project Management**: Agile, sprints, stakeholder communication\n",
    "- **Data Analysis**: A/B testing, statistical significance, causal inference\n",
    "- **Finance**: Budget planning, ROI calculation, cost optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ **Congratulations!**\n",
    "\n",
    "You've completed a comprehensive journey through modern transformer architectures, from fundamental attention mechanisms to cutting-edge innovations like Mixture of Experts and Constitutional AI. This lecture series has equipped you with:\n",
    "\n",
    "### **‚úÖ Technical Excellence**\n",
    "- Deep understanding of transformer architectures and their evolution\n",
    "- Hands-on implementation of advanced components (RoPE, SwiGLU, MoE)\n",
    "- Production deployment knowledge and optimization techniques\n",
    "- Safety, ethics, and evaluation frameworks\n",
    "\n",
    "### **‚úÖ Leadership Readiness**\n",
    "- Strategic thinking for research and product integration\n",
    "- Team management and mentoring capabilities\n",
    "- Business impact assessment and communication skills\n",
    "- Vision development for AI/ML organizational growth\n",
    "\n",
    "### **‚úÖ Interview Confidence**\n",
    "- Comprehensive self-assessment framework\n",
    "- Structured preparation methodology\n",
    "- Mock interview scenarios and evaluation criteria\n",
    "- 30-day preparation plan for systematic readiness\n",
    "\n",
    "**Remember**: Senior Applied Scientist roles require not just technical depth, but the ability to translate cutting-edge research into business value while leading teams and shaping organizational AI strategy. Your journey in AI/ML is just beginning - stay curious, keep learning, and always consider the human impact of the technology you build.\n",
    "\n",
    "**Good luck with your Amazon Applied Scientist interviews!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best way to predict the future is to invent it, but the wisest way to invent it is to understand both the technology and the people it serves.\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
