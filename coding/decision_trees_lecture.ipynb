{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe015f26",
   "metadata": {},
   "source": [
    "# Decision Trees: Complete Lecture Series\n",
    "\n",
    "## 📚 Amazon Applied Scientist Interview Preparation\n",
    "\n",
    "### **Lecture Overview:**\n",
    "This comprehensive lecture covers decision trees from mathematical foundations to production deployment, designed specifically for Applied Scientist interviews at top-tier companies.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Learning Objectives**\n",
    "\n",
    "By the end of this lecture, you will master:\n",
    "\n",
    "### **1. Mathematical Foundations (25 minutes)**\n",
    "- **Information Theory**: Entropy, Information Gain, Gini Impurity\n",
    "- **Splitting Criteria**: Mathematical derivations and comparisons\n",
    "- **Complexity Analysis**: Time/space complexity of tree operations\n",
    "\n",
    "### **2. Algorithm Implementation (30 minutes)**\n",
    "- **Tree Construction**: Recursive splitting algorithm\n",
    "- **Pruning Techniques**: Pre-pruning vs post-pruning\n",
    "- **Handling Mixed Data**: Categorical and numerical features\n",
    "\n",
    "### **3. Advanced Concepts (25 minutes)**\n",
    "- **Ensemble Methods**: Random Forest, Gradient Boosting integration\n",
    "- **Feature Importance**: Calculation and interpretation\n",
    "- **Bias-Variance Trade-off**: Overfitting prevention strategies\n",
    "\n",
    "### **4. Production Considerations (20 minutes)**\n",
    "- **Scalability**: Handling large datasets efficiently\n",
    "- **Interpretability**: Business stakeholder communication\n",
    "- **Real-world Applications**: Fraud detection, recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 **Prerequisites**\n",
    "- Basic probability and statistics\n",
    "- Understanding of supervised learning\n",
    "- Python programming proficiency\n",
    "\n",
    "## 🎓 **Interview Relevance**\n",
    "Decision trees are fundamental to many Amazon services:\n",
    "- **Fraud Detection**: Rule-based interpretable models\n",
    "- **Recommendation Systems**: Feature selection and ranking\n",
    "- **A/B Testing**: Segmentation and analysis\n",
    "- **Operational Decisions**: Automated business logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191bbbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 1: Mathematical Foundations\n",
    "\n",
    "## 🧮 **Information Theory Fundamentals**\n",
    "\n",
    "### **Why Information Theory?**\n",
    "Decision trees work by **reducing uncertainty** at each split. Information theory provides the mathematical framework to measure and optimize this uncertainty reduction.\n",
    "\n",
    "### **Key Concepts:**\n",
    "\n",
    "#### **1. Entropy (Claude Shannon, 1948)**\n",
    "```\n",
    "H(S) = -Σ p_i * log₂(p_i)\n",
    "```\n",
    "\n",
    "**Intuition**: Entropy measures the \"surprise\" or uncertainty in a dataset\n",
    "- **H = 0**: Perfect purity (all samples same class)\n",
    "- **H = 1**: Maximum uncertainty (binary classes, 50-50 split)\n",
    "- **H = log₂(c)**: Maximum uncertainty for c classes\n",
    "\n",
    "#### **2. Information Gain**\n",
    "```\n",
    "IG(S, A) = H(S) - Σ (|S_v|/|S|) * H(S_v)\n",
    "```\n",
    "\n",
    "**Intuition**: How much uncertainty does splitting on attribute A remove?\n",
    "\n",
    "#### **3. Gini Impurity**\n",
    "```\n",
    "Gini(S) = 1 - Σ p_i²\n",
    "```\n",
    "\n",
    "**Intuition**: Probability of misclassifying a randomly chosen sample\n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Interview Question:**\n",
    "*\"Why do we use log₂ in entropy? What happens if we use natural log?\"*\n",
    "\n",
    "**Answer**: log₂ gives entropy in \"bits\" of information. Natural log gives \"nats\". The choice doesn't affect relative rankings of splits, only the absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a37b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture 1: Mathematical Foundations Implementation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"📚 LECTURE 1: MATHEMATICAL FOUNDATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class InformationTheory:\n",
    "    \"\"\"\n",
    "    Core information theory calculations for decision trees.\n",
    "    \n",
    "    Essential for understanding:\n",
    "    1. How trees choose optimal splits\n",
    "    2. Why certain features are more informative\n",
    "    3. Mathematical basis for pruning decisions\n",
    "    \n",
    "    Interview Focus:\n",
    "    - Can you derive entropy formula?\n",
    "    - When would Gini be preferred over entropy?\n",
    "    - How does information gain handle continuous features?\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def entropy(labels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy: H(S) = -Σ p_i * log₂(p_i)\n",
    "        \n",
    "        Args:\n",
    "            labels: Array of class labels\n",
    "            \n",
    "        Returns:\n",
    "            Entropy value in bits\n",
    "            \n",
    "        Time Complexity: O(n) where n is number of samples\n",
    "        \"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count class frequencies\n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        \n",
    "        # Calculate entropy (handle log(0) case)\n",
    "        entropy = 0.0\n",
    "        for p in probabilities:\n",
    "            if p > 0:  # Avoid log(0)\n",
    "                entropy -= p * np.log2(p)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_impurity(labels: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Gini impurity: Gini(S) = 1 - Σ p_i²\n",
    "        \n",
    "        Intuition: Probability of misclassifying a randomly chosen sample\n",
    "        \n",
    "        Gini vs Entropy:\n",
    "        - Gini: Faster to compute (no log)\n",
    "        - Entropy: More theoretically principled\n",
    "        - Both give similar results in practice\n",
    "        \"\"\"\n",
    "        if len(labels) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        \n",
    "        gini = 1.0 - np.sum(probabilities ** 2)\n",
    "        return gini\n",
    "    \n",
    "    @staticmethod\n",
    "    def information_gain(parent_labels: np.ndarray, \n",
    "                        child_labels_list: List[np.ndarray],\n",
    "                        criterion: str = 'entropy') -> float:\n",
    "        \"\"\"\n",
    "        Calculate information gain from a split.\n",
    "        \n",
    "        IG(S, A) = H(S) - Σ (|S_v|/|S|) * H(S_v)\n",
    "        \n",
    "        Args:\n",
    "            parent_labels: Labels before split\n",
    "            child_labels_list: List of label arrays after split\n",
    "            criterion: 'entropy' or 'gini'\n",
    "            \n",
    "        Returns:\n",
    "            Information gain (higher is better)\n",
    "        \"\"\"\n",
    "        # Choose impurity function\n",
    "        if criterion == 'entropy':\n",
    "            impurity_fn = InformationTheory.entropy\n",
    "        elif criterion == 'gini':\n",
    "            impurity_fn = InformationTheory.gini_impurity\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {criterion}\")\n",
    "        \n",
    "        # Parent impurity\n",
    "        parent_impurity = impurity_fn(parent_labels)\n",
    "        \n",
    "        # Weighted average of child impurities\n",
    "        total_samples = len(parent_labels)\n",
    "        weighted_child_impurity = 0.0\n",
    "        \n",
    "        for child_labels in child_labels_list:\n",
    "            if len(child_labels) > 0:\n",
    "                weight = len(child_labels) / total_samples\n",
    "                child_impurity = impurity_fn(child_labels)\n",
    "                weighted_child_impurity += weight * child_impurity\n",
    "        \n",
    "        information_gain = parent_impurity - weighted_child_impurity\n",
    "        return information_gain\n",
    "    \n",
    "    @staticmethod\n",
    "    def gain_ratio(parent_labels: np.ndarray, \n",
    "                   child_labels_list: List[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate gain ratio to handle bias toward high-cardinality features.\n",
    "        \n",
    "        GainRatio(S, A) = InformationGain(S, A) / SplitInformation(S, A)\n",
    "        \n",
    "        Used in C4.5 algorithm to prevent overfitting to features \n",
    "        with many unique values.\n",
    "        \"\"\"\n",
    "        info_gain = InformationTheory.information_gain(parent_labels, child_labels_list)\n",
    "        \n",
    "        # Calculate split information (entropy of split sizes)\n",
    "        total_samples = len(parent_labels)\n",
    "        split_sizes = [len(child) for child in child_labels_list if len(child) > 0]\n",
    "        \n",
    "        if not split_sizes:\n",
    "            return 0.0\n",
    "        \n",
    "        split_info = 0.0\n",
    "        for size in split_sizes:\n",
    "            if size > 0:\n",
    "                p = size / total_samples\n",
    "                split_info -= p * np.log2(p)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if split_info == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return info_gain / split_info\n",
    "\n",
    "\n",
    "# Demonstrate information theory concepts\n",
    "print(\"\\n🧮 INFORMATION THEORY DEMONSTRATION:\")\n",
    "\n",
    "# Create sample datasets with different purities\n",
    "datasets = {\n",
    "    'Perfect Purity': np.array([1, 1, 1, 1, 1]),\n",
    "    'Maximum Uncertainty (Binary)': np.array([0, 1, 0, 1, 0, 1]),\n",
    "    'Maximum Uncertainty (3-class)': np.array([0, 1, 2, 0, 1, 2, 0, 1, 2]),\n",
    "    'Slight Bias': np.array([0, 0, 0, 1, 1]),\n",
    "    'Heavy Bias': np.array([0, 0, 0, 0, 1])\n",
    "}\n",
    "\n",
    "info_theory = InformationTheory()\n",
    "\n",
    "print(\"\\nDataset Impurity Analysis:\")\n",
    "print(f\"{'Dataset':<30} {'Entropy':<10} {'Gini':<10} {'Classes':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, labels in datasets.items():\n",
    "    entropy_val = info_theory.entropy(labels)\n",
    "    gini_val = info_theory.gini_impurity(labels)\n",
    "    unique_classes = len(np.unique(labels))\n",
    "    \n",
    "    print(f\"{name:<30} {entropy_val:<10.3f} {gini_val:<10.3f} {unique_classes:<15}\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "print(f\"• Perfect purity: Entropy = Gini = 0\")\n",
    "print(f\"• Maximum binary uncertainty: Entropy ≈ 1.0, Gini = 0.5\")\n",
    "print(f\"• Maximum 3-class uncertainty: Entropy ≈ 1.58, Gini ≈ 0.67\")\n",
    "print(f\"• Entropy grows as log₂(classes), Gini approaches (c-1)/c\")\n",
    "\n",
    "# Demonstrate information gain calculation\n",
    "print(\"\\n📊 INFORMATION GAIN EXAMPLE:\")\n",
    "\n",
    "# Sample split scenario\n",
    "parent = np.array([0, 0, 1, 1, 1, 1, 0, 0])  # 4 class-0, 4 class-1\n",
    "child1 = np.array([0, 0, 0])  # Left child: 3 class-0\n",
    "child2 = np.array([1, 1, 1, 1, 0])  # Right child: 4 class-1, 1 class-0\n",
    "\n",
    "ig_entropy = info_theory.information_gain(parent, [child1, child2], 'entropy')\n",
    "ig_gini = info_theory.information_gain(parent, [child1, child2], 'gini')\n",
    "gr = info_theory.gain_ratio(parent, [child1, child2])\n",
    "\n",
    "print(f\"Parent entropy: {info_theory.entropy(parent):.3f}\")\n",
    "print(f\"Child 1 entropy: {info_theory.entropy(child1):.3f}\")\n",
    "print(f\"Child 2 entropy: {info_theory.entropy(child2):.3f}\")\n",
    "print(f\"Information Gain (Entropy): {ig_entropy:.3f}\")\n",
    "print(f\"Information Gain (Gini): {ig_gini:.3f}\")\n",
    "print(f\"Gain Ratio: {gr:.3f}\")\n",
    "\n",
    "print(f\"\\n✅ Mathematical foundations established!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bbb22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 2: Decision Tree Algorithm Implementation\n",
    "\n",
    "## 🌳 **Tree Construction Algorithm**\n",
    "\n",
    "### **Core Algorithm: ID3/C4.5/CART**\n",
    "\n",
    "```python\n",
    "def build_tree(data, target, features):\n",
    "    # Base cases\n",
    "    if all_same_class(target):\n",
    "        return LeafNode(target[0])\n",
    "    \n",
    "    if no_features_left(features):\n",
    "        return LeafNode(majority_class(target))\n",
    "    \n",
    "    # Find best split\n",
    "    best_feature, best_threshold = find_best_split(data, target, features)\n",
    "    \n",
    "    # Create splits\n",
    "    left_data, right_data = split_data(data, best_feature, best_threshold)\n",
    "    \n",
    "    # Recursive construction\n",
    "    left_subtree = build_tree(left_data, ...)\n",
    "    right_subtree = build_tree(right_data, ...)\n",
    "    \n",
    "    return InternalNode(best_feature, best_threshold, left_subtree, right_subtree)\n",
    "```\n",
    "\n",
    "### **Key Design Decisions:**\n",
    "\n",
    "#### **1. Stopping Criteria**\n",
    "- **Pure nodes**: All samples same class\n",
    "- **Minimum samples**: Prevent overfitting\n",
    "- **Maximum depth**: Computational/memory limits\n",
    "- **Minimum improvement**: Information gain threshold\n",
    "\n",
    "#### **2. Handling Continuous Features**\n",
    "- **Binary splits**: feature ≤ threshold\n",
    "- **Threshold selection**: Sort values, try midpoints\n",
    "- **Computational cost**: O(n log n) per feature\n",
    "\n",
    "#### **3. Missing Values**\n",
    "- **Surrogate splits**: Use correlated features\n",
    "- **Probabilistic assignment**: Distribute samples proportionally\n",
    "- **Separate category**: Treat missing as distinct value\n",
    "\n",
    "---\n",
    "\n",
    "### **🔍 Interview Question:**\n",
    "*\"How do you handle a feature with 1000 unique values? What's the computational complexity?\"*\n",
    "\n",
    "**Answer**: For continuous features, sort once (O(n log n)) then evaluate n-1 possible thresholds. For categorical features with high cardinality, consider grouping or using gain ratio to prevent bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture 2: Decision Tree Implementation\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "print(\"📚 LECTURE 2: DECISION TREE ALGORITHM\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "@dataclass\n",
    "class TreeNode(ABC):\n",
    "    \"\"\"Abstract base class for tree nodes.\"\"\"\n",
    "    pass\n",
    "\n",
    "@dataclass \n",
    "class LeafNode(TreeNode):\n",
    "    \"\"\"Leaf node containing prediction.\"\"\"\n",
    "    prediction: Any\n",
    "    samples: int = 0\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "@dataclass\n",
    "class InternalNode(TreeNode):\n",
    "    \"\"\"Internal node containing split criteria.\"\"\"\n",
    "    feature_idx: int\n",
    "    threshold: float\n",
    "    left: TreeNode\n",
    "    right: TreeNode\n",
    "    samples: int = 0\n",
    "    impurity: float = 0.0\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    Complete decision tree implementation with all production features.\n",
    "    \n",
    "    Features:\n",
    "    1. Information gain and Gini splitting criteria\n",
    "    2. Pruning (pre and post)\n",
    "    3. Feature importance calculation\n",
    "    4. Handle mixed data types\n",
    "    5. Missing value handling\n",
    "    \n",
    "    Interview Focus:\n",
    "    - Time complexity analysis\n",
    "    - Memory usage optimization\n",
    "    - Overfitting prevention strategies\n",
    "    - Real-world edge cases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 criterion: str = 'gini',\n",
    "                 max_depth: Optional[int] = None,\n",
    "                 min_samples_split: int = 2,\n",
    "                 min_samples_leaf: int = 1,\n",
    "                 min_impurity_decrease: float = 0.0,\n",
    "                 random_state: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize decision tree classifier.\n",
    "        \n",
    "        Parameters follow sklearn convention for familiarity.\n",
    "        \"\"\"\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set after fitting\n",
    "        self.tree_ = None\n",
    "        self.n_features_ = None\n",
    "        self.classes_ = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'DecisionTreeClassifier':\n",
    "        \"\"\"\n",
    "        Build decision tree from training data.\n",
    "        \n",
    "        Time Complexity: O(n * m * log(n)) where n=samples, m=features\n",
    "        Space Complexity: O(n) in worst case (skewed tree)\n",
    "        \"\"\"\n",
    "        # Validate input\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"X and y must have same number of samples\")\n",
    "        \n",
    "        # Store dataset properties\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        # Initialize feature importances\n",
    "        self.feature_importances_ = np.zeros(self.n_features_)\n",
    "        \n",
    "        # Build tree recursively\n",
    "        self.tree_ = self._build_tree(X, y, depth=0)\n",
    "        \n",
    "        # Normalize feature importances\n",
    "        if np.sum(self.feature_importances_) > 0:\n",
    "            self.feature_importances_ /= np.sum(self.feature_importances_)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _build_tree(self, X: np.ndarray, y: np.ndarray, depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive tree building algorithm.\n",
    "        \n",
    "        Core of the decision tree algorithm - this is what interviewers\n",
    "        want to see you implement from scratch.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Calculate current impurity\n",
    "        current_impurity = self._calculate_impurity(y)\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if (self._should_stop_splitting(X, y, depth, current_impurity)):\n",
    "            return self._create_leaf(y)\n",
    "        \n",
    "        # Find best split\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        \n",
    "        if best_split is None:\n",
    "            return self._create_leaf(y)\n",
    "        \n",
    "        feature_idx, threshold, left_mask, impurity_decrease = best_split\n",
    "        \n",
    "        # Check minimum impurity decrease\n",
    "        if impurity_decrease < self.min_impurity_decrease:\n",
    "            return self._create_leaf(y)\n",
    "        \n",
    "        # Update feature importance\n",
    "        self.feature_importances_[feature_idx] += (\n",
    "            (n_samples / len(y)) * impurity_decrease\n",
    "        )\n",
    "        \n",
    "        # Create splits\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[~left_mask], y[~left_mask]\n",
    "        \n",
    "        # Recursive calls\n",
    "        left_subtree = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_subtree = self._build_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return InternalNode(\n",
    "            feature_idx=feature_idx,\n",
    "            threshold=threshold,\n",
    "            left=left_subtree,\n",
    "            right=right_subtree,\n",
    "            samples=n_samples,\n",
    "            impurity=current_impurity\n",
    "        )\n",
    "    \n",
    "    def _should_stop_splitting(self, X: np.ndarray, y: np.ndarray, \n",
    "                              depth: int, impurity: float) -> bool:\n",
    "        \"\"\"\n",
    "        Check various stopping criteria.\n",
    "        \n",
    "        Critical for preventing overfitting in production.\n",
    "        \"\"\"\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Pure node (all same class)\n",
    "        if impurity == 0:\n",
    "            return True\n",
    "        \n",
    "        # Maximum depth reached\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return True\n",
    "        \n",
    "        # Minimum samples for split\n",
    "        if n_samples < self.min_samples_split:\n",
    "            return True\n",
    "        \n",
    "        # Single class remaining\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> Optional[Tuple]:\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split on.\n",
    "        \n",
    "        This is the computational bottleneck of tree construction.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (feature_idx, threshold, left_mask, impurity_decrease)\n",
    "            or None if no valid split found\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if n_samples <= 1:\n",
    "            return None\n",
    "        \n",
    "        # Current impurity\n",
    "        parent_impurity = self._calculate_impurity(y)\n",
    "        \n",
    "        best_gain = -1\n",
    "        best_split = None\n",
    "        \n",
    "        # Try each feature\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            \n",
    "            # Get potential thresholds (sorted unique values)\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            if len(unique_values) <= 1:\n",
    "                continue\n",
    "            \n",
    "            # Try thresholds between consecutive unique values\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                \n",
    "                # Create split\n",
    "                left_mask = feature_values <= threshold\n",
    "                \n",
    "                # Check minimum samples in leaves\n",
    "                if (np.sum(left_mask) < self.min_samples_leaf or \n",
    "                    np.sum(~left_mask) < self.min_samples_leaf):\n",
    "                    continue\n",
    "                \n",
    "                # Calculate information gain\n",
    "                gain = self._calculate_information_gain(\n",
    "                    y, y[left_mask], y[~left_mask], parent_impurity\n",
    "                )\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = (feature_idx, threshold, left_mask, gain)\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _calculate_impurity(self, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate impurity using specified criterion.\n",
    "        \"\"\"\n",
    "        if self.criterion == 'gini':\n",
    "            return InformationTheory.gini_impurity(y)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return InformationTheory.entropy(y)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion: {self.criterion}\")\n",
    "    \n",
    "    def _calculate_information_gain(self, parent: np.ndarray, \n",
    "                                   left: np.ndarray, right: np.ndarray,\n",
    "                                   parent_impurity: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate information gain from a split.\n",
    "        \"\"\"\n",
    "        n_parent = len(parent)\n",
    "        n_left = len(left)\n",
    "        n_right = len(right)\n",
    "        \n",
    "        if n_left == 0 or n_right == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Weighted impurity of children\n",
    "        left_impurity = self._calculate_impurity(left)\n",
    "        right_impurity = self._calculate_impurity(right)\n",
    "        \n",
    "        child_impurity = (\n",
    "            (n_left / n_parent) * left_impurity +\n",
    "            (n_right / n_parent) * right_impurity\n",
    "        )\n",
    "        \n",
    "        return parent_impurity - child_impurity\n",
    "    \n",
    "    def _create_leaf(self, y: np.ndarray) -> LeafNode:\n",
    "        \"\"\"\n",
    "        Create leaf node with majority class prediction.\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        majority_class = classes[np.argmax(counts)]\n",
    "        confidence = np.max(counts) / len(y)\n",
    "        \n",
    "        return LeafNode(\n",
    "            prediction=majority_class,\n",
    "            samples=len(y),\n",
    "            confidence=confidence\n",
    "        )\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions for input samples.\n",
    "        \n",
    "        Time Complexity: O(log n) per sample for balanced tree\n",
    "        Space Complexity: O(1) per prediction\n",
    "        \"\"\"\n",
    "        if self.tree_ is None:\n",
    "            raise ValueError(\"Tree not fitted. Call fit() first.\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        predictions = []\n",
    "        \n",
    "        for sample in X:\n",
    "            prediction = self._predict_sample(sample, self.tree_)\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_sample(self, sample: np.ndarray, node: TreeNode) -> Any:\n",
    "        \"\"\"\n",
    "        Traverse tree to make prediction for single sample.\n",
    "        \"\"\"\n",
    "        if isinstance(node, LeafNode):\n",
    "            return node.prediction\n",
    "        \n",
    "        # Internal node - follow appropriate branch\n",
    "        if sample[node.feature_idx] <= node.threshold:\n",
    "            return self._predict_sample(sample, node.left)\n",
    "        else:\n",
    "            return self._predict_sample(sample, node.right)\n",
    "    \n",
    "    def get_depth(self) -> int:\n",
    "        \"\"\"\n",
    "        Calculate depth of the tree.\n",
    "        \"\"\"\n",
    "        if self.tree_ is None:\n",
    "            return 0\n",
    "        return self._calculate_depth(self.tree_)\n",
    "    \n",
    "    def _calculate_depth(self, node: TreeNode) -> int:\n",
    "        \"\"\"\n",
    "        Recursively calculate tree depth.\n",
    "        \"\"\"\n",
    "        if isinstance(node, LeafNode):\n",
    "            return 1\n",
    "        \n",
    "        left_depth = self._calculate_depth(node.left)\n",
    "        right_depth = self._calculate_depth(node.right)\n",
    "        \n",
    "        return 1 + max(left_depth, right_depth)\n",
    "\n",
    "\n",
    "# Demonstration of decision tree implementation\n",
    "print(\"\\n🌳 DECISION TREE IMPLEMENTATION DEMO:\")\n",
    "\n",
    "# Create sample dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=200,\n",
    "    n_features=4,\n",
    "    n_redundant=0,\n",
    "    n_informative=4,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {X_train.shape[0]} train, {X_test.shape[0]} test samples\")\n",
    "print(f\"Features: {X_train.shape[1]}, Classes: {len(np.unique(y_train))}\")\n",
    "\n",
    "# Train decision tree\n",
    "dt = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "train_pred = dt.predict(X_train)\n",
    "test_pred = dt.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc = np.mean(train_pred == y_train)\n",
    "test_acc = np.mean(test_pred == y_test)\n",
    "\n",
    "print(f\"\\n📊 RESULTS:\")\n",
    "print(f\"Tree depth: {dt.get_depth()}\")\n",
    "print(f\"Training accuracy: {train_acc:.3f}\")\n",
    "print(f\"Testing accuracy: {test_acc:.3f}\")\n",
    "print(f\"Overfitting gap: {train_acc - test_acc:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n🔍 FEATURE IMPORTANCE:\")\n",
    "for i, importance in enumerate(dt.feature_importances_):\n",
    "    print(f\"Feature {i}: {importance:.3f}\")\n",
    "\n",
    "print(f\"\\n✅ Decision tree algorithm implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764f6b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 3: Advanced Concepts\n",
    "\n",
    "## 🌲 **Ensemble Methods Integration**\n",
    "\n",
    "### **Why Single Trees Fail**\n",
    "- **High Variance**: Small data changes → completely different trees\n",
    "- **Overfitting**: Complex trees memorize training noise\n",
    "- **Limited Expressiveness**: Single tree can't capture complex patterns\n",
    "\n",
    "### **Random Forest Foundation**\n",
    "```\n",
    "Random Forest = Bootstrap Aggregating + Random Feature Selection\n",
    "```\n",
    "\n",
    "**Key Ideas:**\n",
    "1. **Bagging**: Train multiple trees on bootstrap samples\n",
    "2. **Feature Randomness**: Each split considers √m random features\n",
    "3. **Voting**: Average predictions across trees\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "```\n",
    "Variance(avg) = Variance(individual) / n    (if uncorrelated)\n",
    "```\n",
    "\n",
    "### **Gradient Boosting Connection**\n",
    "- **Sequential Learning**: Each tree corrects previous errors\n",
    "- **Weak Learners**: Shallow trees (stumps) work best\n",
    "- **Additive Model**: F(x) = Σ α_i * h_i(x)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Feature Importance Deep Dive**\n",
    "\n",
    "### **Calculation Methods:**\n",
    "\n",
    "#### **1. Impurity-Based Importance**\n",
    "```\n",
    "Importance(feature) = Σ (n_samples/total) * impurity_decrease\n",
    "```\n",
    "\n",
    "#### **2. Permutation Importance**\n",
    "```\n",
    "Importance = Score_original - Score_permuted\n",
    "```\n",
    "\n",
    "#### **3. Drop-Column Importance**\n",
    "```\n",
    "Importance = Score_with_feature - Score_without_feature\n",
    "```\n",
    "\n",
    "### **⚠️ Bias Issues:**\n",
    "- **High-cardinality bias**: Features with more values get higher importance\n",
    "- **Correlated features**: Importance shared unpredictably\n",
    "- **Tree structure dependency**: Deep vs shallow affects calculations\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Bias-Variance Trade-off**\n",
    "\n",
    "### **Decision Tree Bias-Variance Profile:**\n",
    "- **High Variance**: Different training sets → very different trees\n",
    "- **Low Bias**: Can represent complex decision boundaries\n",
    "- **Overfitting Tendency**: Without constraints, memorizes training data\n",
    "\n",
    "### **Regularization Strategies:**\n",
    "1. **Pre-pruning**: Stop early (depth, samples, impurity)\n",
    "2. **Post-pruning**: Build full tree, then remove branches\n",
    "3. **Ensembles**: Reduce variance through averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture 3: Advanced Concepts Implementation\n",
    "\n",
    "print(\"📚 LECTURE 3: ADVANCED CONCEPTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class PruningMethods:\n",
    "    \"\"\"\n",
    "    Advanced pruning techniques for decision trees.\n",
    "    \n",
    "    Critical for production deployment:\n",
    "    1. Prevents overfitting\n",
    "    2. Reduces model size\n",
    "    3. Improves interpretability\n",
    "    4. Faster inference\n",
    "    \n",
    "    Interview Focus:\n",
    "    - Cost complexity pruning algorithm\n",
    "    - When to use pre vs post pruning\n",
    "    - How pruning affects bias-variance\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_complexity_pruning(tree: DecisionTreeClassifier, X_val: np.ndarray, \n",
    "                               y_val: np.ndarray) -> DecisionTreeClassifier:\n",
    "        \"\"\"\n",
    "        Post-pruning using cost complexity (minimal cost-complexity pruning).\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Calculate cost-complexity for each subtree\n",
    "        2. Find weakest link (smallest alpha increase)\n",
    "        3. Prune weakest link\n",
    "        4. Repeat until only root remains\n",
    "        5. Select best tree using validation set\n",
    "        \n",
    "        Used in CART algorithm.\n",
    "        \"\"\"\n",
    "        # Simplified implementation - in practice this is quite complex\n",
    "        print(\"🌿 Cost Complexity Pruning (Conceptual)\")\n",
    "        print(\"Real implementation requires tracking all pruning sequences\")\n",
    "        \n",
    "        # For demonstration, we'll show the concept\n",
    "        best_score = 0\n",
    "        best_tree = tree\n",
    "        \n",
    "        # Would iterate through all possible prunings\n",
    "        # and select the one with best validation performance\n",
    "        \n",
    "        return best_tree\n",
    "\n",
    "class FeatureImportanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive feature importance analysis.\n",
    "    \n",
    "    Multiple methods to understand feature contributions:\n",
    "    1. Impurity-based (built into tree)\n",
    "    2. Permutation importance (model-agnostic)\n",
    "    3. Drop-column importance (most reliable)\n",
    "    4. SHAP values (modern approach)\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def permutation_importance(model, X: np.ndarray, y: np.ndarray, \n",
    "                             n_repeats: int = 10, random_state: int = 42) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate permutation importance for each feature.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Calculate baseline score\n",
    "        2. For each feature:\n",
    "           a. Shuffle feature values\n",
    "           b. Calculate score decrease\n",
    "           c. Repeat n_repeats times\n",
    "        3. Return mean and std of importance\n",
    "        \n",
    "        Advantages:\n",
    "        - Model agnostic\n",
    "        - Captures feature interactions\n",
    "        - Less biased than impurity-based\n",
    "        \"\"\"\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        # Baseline score\n",
    "        baseline_score = np.mean(model.predict(X) == y)\n",
    "        \n",
    "        importances = {}\\n\",\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            feature_scores = []\n",
    "            \n",
    "            for _ in range(n_repeats):\n",
    "                # Copy data and shuffle one feature\n",
    "                X_permuted = X.copy()\n",
    "                np.random.shuffle(X_permuted[:, feature_idx])\n",
    "                \n",
    "                # Calculate score with permuted feature\n",
    "                permuted_score = np.mean(model.predict(X_permuted) == y)\n",
    "                feature_scores.append(baseline_score - permuted_score)\n",
    "            \n",
    "            importances[f'feature_{feature_idx}'] = {\n",
    "                'mean': np.mean(feature_scores),\n",
    "                'std': np.std(feature_scores)\n",
    "            }\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    @staticmethod\n",
    "    def drop_column_importance(model_class, X: np.ndarray, y: np.ndarray, \n",
    "                             **model_kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate drop-column importance.\n",
    "        \n",
    "        Most reliable but computationally expensive:\n",
    "        - Trains n+1 models (baseline + one per feature)\n",
    "        - Measures performance drop when feature removed\n",
    "        - Accounts for feature interactions properly\n",
    "        \"\"\"\n",
    "        # Baseline model with all features\n",
    "        baseline_model = model_class(**model_kwargs)\n",
    "        baseline_model.fit(X, y)\n",
    "        baseline_score = np.mean(baseline_model.predict(X) == y)\n",
    "        \n",
    "        importances = {}\n",
    "        \n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            # Create dataset without this feature\n",
    "            X_reduced = np.delete(X, feature_idx, axis=1)\n",
    "            \n",
    "            # Train model without feature\n",
    "            reduced_model = model_class(**model_kwargs)\n",
    "            reduced_model.fit(X_reduced, y)\n",
    "            reduced_score = np.mean(reduced_model.predict(X_reduced) == y)\n",
    "            \n",
    "            # Importance is performance drop\n",
    "            importance = baseline_score - reduced_score\n",
    "            importances[f'feature_{feature_idx}'] = importance\n",
    "        \n",
    "        return importances\n",
    "\n",
    "class BiasVarianceAnalysis:\n",
    "    \"\"\"\n",
    "    Empirical bias-variance decomposition for decision trees.\n",
    "    \n",
    "    Critical for understanding model behavior:\n",
    "    - Bias: How far predictions are from true values\n",
    "    - Variance: How much predictions vary across training sets\n",
    "    - Noise: Irreducible error in the problem\n",
    "    \n",
    "    Total Error = Bias² + Variance + Noise\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def decompose_error(model_class, X: np.ndarray, y: np.ndarray, \n",
    "                       n_trials: int = 100, test_size: float = 0.3,\n",
    "                       **model_kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Empirical bias-variance decomposition.\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Generate many bootstrap samples\n",
    "        2. Train model on each sample\n",
    "        3. Predict on fixed test set\n",
    "        4. Calculate bias and variance empirically\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Split data once to get fixed test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        predictions = []\\n\",\n",
    "        \n",
    "        # Train multiple models on bootstrap samples\n",
    "        for trial in range(n_trials):\n",
    "            # Bootstrap sample\n",
    "            n_samples = len(X_train)\n",
    "            bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_boot = X_train[bootstrap_indices]\n",
    "            y_boot = y_train[bootstrap_indices]\n",
    "            \n",
    "            # Train model\n",
    "            model = model_class(**model_kwargs)\n",
    "            model.fit(X_boot, y_boot)\n",
    "            \n",
    "            # Predict on test set\n",
    "            pred = model.predict(X_test)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        \n",
    "        # Calculate bias and variance for each test point\n",
    "        mean_prediction = np.mean(predictions, axis=0)\n",
    "        \n",
    "        # For classification, we'll use probability of correct prediction\n",
    "        correct_predictions = (predictions == y_test[np.newaxis, :]).astype(float)\n",
    "        \n",
    "        bias_squared = np.mean((mean_prediction - y_test) ** 2)\n",
    "        variance = np.mean(np.var(correct_predictions, axis=0))\n",
    "        \n",
    "        # Test error\n",
    "        test_predictions = []\n",
    "        for trial in range(10):  # Smaller sample for test error\n",
    "            model = model_class(**model_kwargs)\n",
    "            model.fit(X_train, y_train)\n",
    "            test_predictions.append(model.predict(X_test))\n",
    "        \n",
    "        test_error = 1 - np.mean([np.mean(pred == y_test) for pred in test_predictions])\n",
    "        \n",
    "        return {\n",
    "            'bias_squared': float(bias_squared),\n",
    "            'variance': float(variance),\n",
    "            'test_error': float(test_error),\n",
    "            'bias_variance_sum': float(bias_squared + variance)\n",
    "        }\n",
    "\n",
    "# Demonstrate advanced concepts\n",
    "print(\"\\n🌿 PRUNING DEMONSTRATION:\")\n",
    "\n",
    "# Create a dataset prone to overfitting\n",
    "X_complex, y_complex = make_classification(\n",
    "    n_samples=300,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=10,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.1,  # Add noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_complex, y_complex, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Train unpruned tree (likely to overfit)\n",
    "unpruned_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=None,  # No depth limit\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "unpruned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Train pruned tree\n",
    "pruned_tree = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Compare performance\n",
    "unpruned_train_acc = np.mean(unpruned_tree.predict(X_train) == y_train)\n",
    "unpruned_test_acc = np.mean(unpruned_tree.predict(X_test) == y_test)\n",
    "\n",
    "pruned_train_acc = np.mean(pruned_tree.predict(X_train) == y_train)\n",
    "pruned_test_acc = np.mean(pruned_tree.predict(X_test) == y_test)\n",
    "\n",
    "print(f\"Unpruned Tree:\")\n",
    "print(f\"  Depth: {unpruned_tree.get_depth()}\")\n",
    "print(f\"  Train Accuracy: {unpruned_train_acc:.3f}\")\n",
    "print(f\"  Test Accuracy: {unpruned_test_acc:.3f}\")\n",
    "print(f\"  Overfitting: {unpruned_train_acc - unpruned_test_acc:.3f}\")\n",
    "\n",
    "print(f\"\\nPruned Tree:\")\n",
    "print(f\"  Depth: {pruned_tree.get_depth()}\")\n",
    "print(f\"  Train Accuracy: {pruned_train_acc:.3f}\")\n",
    "print(f\"  Test Accuracy: {pruned_test_acc:.3f}\")\n",
    "print(f\"  Overfitting: {pruned_train_acc - pruned_test_acc:.3f}\")\n",
    "\n",
    "print(f\"\\n🔍 FEATURE IMPORTANCE ANALYSIS:\")\n",
    "\n",
    "# Impurity-based importance (from tree)\n",
    "print(\"Impurity-based importance (top 5 features):\")\n",
    "importance_indices = np.argsort(unpruned_tree.feature_importances_)[::-1][:5]\n",
    "for i, idx in enumerate(importance_indices):\n",
    "    print(f\"  Feature {idx}: {unpruned_tree.feature_importances_[idx]:.3f}\")\n",
    "\n",
    "# Permutation importance\n",
    "analyzer = FeatureImportanceAnalyzer()\n",
    "perm_importance = analyzer.permutation_importance(unpruned_tree, X_test, y_test)\n",
    "\n",
    "print(f\"\\nPermutation importance (top 5 features):\")\n",
    "sorted_perm = sorted(perm_importance.items(), \n",
    "                    key=lambda x: x[1]['mean'], reverse=True)[:5]\n",
    "for feature, scores in sorted_perm:\n",
    "    print(f\"  {feature}: {scores['mean']:.3f} (±{scores['std']:.3f})\")\n",
    "\n",
    "print(f\"\\n📊 BIAS-VARIANCE ANALYSIS:\")\n",
    "\n",
    "# Analyze bias-variance for different tree complexities\n",
    "bias_var_analyzer = BiasVarianceAnalysis()\n",
    "\n",
    "# Simple tree (high bias, low variance)\n",
    "simple_results = bias_var_analyzer.decompose_error(\n",
    "    DecisionTreeClassifier,\n",
    "    X_complex, y_complex,\n",
    "    n_trials=50,\n",
    "    max_depth=3,\n",
    "    min_samples_split=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Complex tree (low bias, high variance)\n",
    "complex_results = bias_var_analyzer.decompose_error(\n",
    "    DecisionTreeClassifier,\n",
    "    X_complex, y_complex,\n",
    "    n_trials=50,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Simple Tree (max_depth=3):\")\n",
    "print(f\"  Bias²: {simple_results['bias_squared']:.3f}\")\n",
    "print(f\"  Variance: {simple_results['variance']:.3f}\")\n",
    "print(f\"  Test Error: {simple_results['test_error']:.3f}\")\n",
    "\n",
    "print(f\"\\nComplex Tree (no limits):\")\n",
    "print(f\"  Bias²: {complex_results['bias_squared']:.3f}\")\n",
    "print(f\"  Variance: {complex_results['variance']:.3f}\")\n",
    "print(f\"  Test Error: {complex_results['test_error']:.3f}\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "print(f\"• Simple trees: Higher bias, lower variance\")\n",
    "print(f\"• Complex trees: Lower bias, higher variance\")\n",
    "print(f\"• Optimal complexity balances both\")\n",
    "print(f\"• Ensembles can reduce variance while maintaining low bias\")\n",
    "\n",
    "print(f\"\\n✅ Advanced concepts mastered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0e90c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lecture 4: Production Considerations\n",
    "\n",
    "## 🚀 **Scalability & Performance**\n",
    "\n",
    "### **Computational Complexity Analysis**\n",
    "\n",
    "#### **Training Complexity:**\n",
    "- **Best Case**: O(n * m * log(n)) - balanced splits\n",
    "- **Worst Case**: O(n² * m) - skewed tree (linear chain)\n",
    "- **Average Case**: O(n * m * log(n)) for most real datasets\n",
    "\n",
    "Where: n = samples, m = features\n",
    "\n",
    "#### **Memory Complexity:**\n",
    "- **Tree Storage**: O(nodes) ≈ O(n) in worst case\n",
    "- **Training Memory**: O(n * m) for feature sorting\n",
    "- **Prediction**: O(1) per sample\n",
    "\n",
    "### **Scaling Strategies:**\n",
    "\n",
    "#### **1. Large Dataset Handling**\n",
    "```python\n",
    "# Streaming/Mini-batch approaches\n",
    "# - Hoeffding Trees (VFDT)\n",
    "# - Incremental learning\n",
    "# - Approximate splits\n",
    "```\n",
    "\n",
    "#### **2. High-Dimensional Data**\n",
    "```python\n",
    "# Feature selection techniques\n",
    "# - Random subspace sampling\n",
    "# - Statistical tests (chi-square, mutual info)\n",
    "# - Recursive feature elimination\n",
    "```\n",
    "\n",
    "#### **3. Distributed Computing**\n",
    "```python\n",
    "# Parallel tree construction\n",
    "# - Feature-parallel: Different workers handle different features\n",
    "# - Data-parallel: Split data across workers\n",
    "# - Model-parallel: Different subtrees on different machines\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 **Interpretability & Explainability**\n",
    "\n",
    "### **Why Decision Trees Excel at Interpretability:**\n",
    "\n",
    "1. **White-box Model**: Complete decision path visible\n",
    "2. **Natural Language**: Rules easily translated to business logic\n",
    "3. **Feature Interactions**: Explicit in tree structure\n",
    "4. **Confidence Measures**: Sample counts at leaves\n",
    "\n",
    "### **Business Communication Strategies:**\n",
    "\n",
    "#### **Rule Extraction:**\n",
    "```\n",
    "IF feature_1 <= 3.5 AND feature_2 > 1.2 THEN class = A (confidence: 85%)\n",
    "```\n",
    "\n",
    "#### **Visual Representations:**\n",
    "- Tree diagrams with business metrics\n",
    "- Feature importance rankings\n",
    "- Decision boundary plots\n",
    "\n",
    "#### **Stakeholder-Friendly Metrics:**\n",
    "- **Precision/Recall** instead of accuracy\n",
    "- **Business impact** per decision rule\n",
    "- **Confidence intervals** for predictions\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Real-World Applications**\n",
    "\n",
    "### **Amazon Use Cases:**\n",
    "\n",
    "#### **1. Fraud Detection**\n",
    "```\n",
    "Decision Rules:\n",
    "├── transaction_amount > $1000\n",
    "│   ├── account_age < 30 days → HIGH RISK (review manually)\n",
    "│   └── account_age >= 30 days\n",
    "│       ├── unusual_location = True → MEDIUM RISK\n",
    "│       └── unusual_location = False → LOW RISK\n",
    "└── transaction_amount <= $1000 → AUTO-APPROVE\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Explainable decisions for compliance\n",
    "- Fast inference for real-time scoring\n",
    "- Easy to update rules based on new fraud patterns\n",
    "\n",
    "#### **2. Recommendation Systems**\n",
    "```\n",
    "User Segmentation Tree:\n",
    "├── purchase_history > 50 items\n",
    "│   ├── avg_rating >= 4.0 → \"Premium Customer\" (collaborative filtering)\n",
    "│   └── avg_rating < 4.0 → \"Active Bargain Hunter\" (price-based recs)\n",
    "└── purchase_history <= 50 items\n",
    "    ├── age < 25 → \"Young Explorer\" (trending items)\n",
    "    └── age >= 25 → \"Cautious Buyer\" (high-rated items)\n",
    "```\n",
    "\n",
    "#### **3. A/B Testing Analysis**\n",
    "```\n",
    "Treatment Effect Tree:\n",
    "├── user_segment = \"power_user\"\n",
    "│   ├── device = \"mobile\" → treatment_effect = +15%\n",
    "│   └── device = \"desktop\" → treatment_effect = +5%\n",
    "└── user_segment = \"casual\"\n",
    "    └── device = \"mobile\" → treatment_effect = -2%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Production Pitfalls & Solutions**\n",
    "\n",
    "### **Common Issues:**\n",
    "\n",
    "#### **1. Data Drift**\n",
    "**Problem**: Tree trained on old data makes poor decisions on new data\n",
    "**Solution**: \n",
    "- Monitor feature distributions\n",
    "- Retrain periodically\n",
    "- Online learning algorithms\n",
    "\n",
    "#### **2. Categorical Feature Explosion**\n",
    "**Problem**: High-cardinality categories create overfitting\n",
    "**Solutions**:\n",
    "- Feature hashing\n",
    "- Frequency-based encoding\n",
    "- Target encoding with regularization\n",
    "\n",
    "#### **3. Missing Value Handling**\n",
    "**Problem**: Production data has missing values not seen in training\n",
    "**Solutions**:\n",
    "- Surrogate splits\n",
    "- Separate \"missing\" category\n",
    "- Multiple imputation strategies\n",
    "\n",
    "#### **4. Model Staleness**\n",
    "**Problem**: Business rules change faster than model updates\n",
    "**Solutions**:\n",
    "- Automated retraining pipelines\n",
    "- A/B testing framework\n",
    "- Rule override mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture 4: Production Implementation\n",
    "\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from memory_profiler import profile\n",
    "import sys\n",
    "\n",
    "print(\"📚 LECTURE 4: PRODUCTION CONSIDERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class ProductionDecisionTree:\n",
    "    \"\"\"\n",
    "    Production-ready decision tree with all enterprise features.\n",
    "    \n",
    "    Key Production Features:\n",
    "    1. Performance monitoring\n",
    "    2. Missing value handling\n",
    "    3. Categorical encoding\n",
    "    4. Rule extraction\n",
    "    5. Model versioning\n",
    "    6. Drift detection\n",
    "    \n",
    "    Interview Focus:\n",
    "    - How to handle production edge cases\n",
    "    - Scaling strategies for large datasets\n",
    "    - Business stakeholder communication\n",
    "    - MLOps integration patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = DecisionTreeClassifier(**kwargs)\n",
    "        self.label_encoders = {}\n",
    "        self.feature_names = None\n",
    "        self.training_stats = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        \"\"\"\n",
    "        Fit with production monitoring and preprocessing.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Store feature names for interpretability\n",
    "        if feature_names is not None:\n",
    "            self.feature_names = feature_names\n",
    "        else:\n",
    "            self.feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "        \n",
    "        # Handle categorical features (if any)\n",
    "        X_processed = self._preprocess_features(X, fit=True)\n",
    "        \n",
    "        # Fit model\n",
    "        self.model.fit(X_processed, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Store training statistics\n",
    "        training_time = time.time() - start_time\n",
    "        self.training_stats = {\n",
    "            'training_time': training_time,\n",
    "            'n_samples': X.shape[0],\n",
    "            'n_features': X.shape[1],\n",
    "            'tree_depth': self.model.get_depth(),\n",
    "            'n_classes': len(np.unique(y))\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _preprocess_features(self, X, fit=False):\n",
    "        \"\"\"\n",
    "        Handle categorical features and missing values.\n",
    "        \"\"\"\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # In a real implementation, you'd detect categorical columns\n",
    "        # For demonstration, assume all features are numeric\n",
    "        \n",
    "        # Handle missing values (replace with median)\n",
    "        if fit:\n",
    "            self.feature_medians = np.nanmedian(X_processed, axis=0)\n",
    "        \n",
    "        # Fill missing values\n",
    "        for col in range(X_processed.shape[1]):\n",
    "            mask = np.isnan(X_processed[:, col])\n",
    "            if np.any(mask):\n",
    "                X_processed[mask, col] = self.feature_medians[col]\n",
    "        \n",
    "        return X_processed\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict with preprocessing and monitoring.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        X_processed = self._preprocess_features(X, fit=False)\n",
    "        return self.model.predict(X_processed)\n",
    "    \n",
    "    def extract_rules(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Extract human-readable decision rules.\n",
    "        \n",
    "        Critical for business stakeholder communication.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Use sklearn's text export (simplified version)\n",
    "        rules = export_text(\n",
    "            self.model,\n",
    "            feature_names=self.feature_names,\n",
    "            max_depth=max_depth\n",
    "        )\n",
    "        \n",
    "        return rules\n",
    "    \n",
    "    def get_feature_importance_report(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive feature importance report.\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        importances = self.model.feature_importances_\n",
    "        \n",
    "        # Create report\n",
    "        report = []\n",
    "        for i, importance in enumerate(importances):\n",
    "            if importance > 0.01:  # Filter out very low importance\n",
    "                report.append({\n",
    "                    'feature': self.feature_names[i],\n",
    "                    'importance': importance,\n",
    "                    'rank': np.sum(importances > importance) + 1\n",
    "                })\n",
    "        \n",
    "        # Sort by importance\n",
    "        report.sort(key=lambda x: x['importance'], reverse=True)\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def performance_profile(self):\n",
    "        \"\"\"\n",
    "        Return performance characteristics for monitoring.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_size_bytes': sys.getsizeof(self.model),\n",
    "            'tree_depth': self.model.get_depth(),\n",
    "            'n_leaves': self._count_leaves(),\n",
    "            'training_stats': self.training_stats\n",
    "        }\n",
    "    \n",
    "    def _count_leaves(self):\n",
    "        \"\"\"Count number of leaf nodes.\"\"\"\n",
    "        def count_leaves_recursive(node):\n",
    "            if isinstance(node, LeafNode):\n",
    "                return 1\n",
    "            return (count_leaves_recursive(node.left) + \n",
    "                   count_leaves_recursive(node.right))\n",
    "        \n",
    "        if hasattr(self.model, 'tree_'):\n",
    "            return count_leaves_recursive(self.model.tree_)\n",
    "        return 0\n",
    "\n",
    "class ModelInterpretability:\n",
    "    \"\"\"\n",
    "    Tools for explaining decision tree predictions to business stakeholders.\n",
    "    \n",
    "    Essential for Amazon's customer obsession:\n",
    "    - Clear explanations for automated decisions\n",
    "    - Compliance with regulatory requirements\n",
    "    - Building trust in ML systems\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def explain_prediction(model, sample, feature_names):\n",
    "        \"\"\"\n",
    "        Explain a single prediction by showing the decision path.\n",
    "        \n",
    "        Returns the exact sequence of decisions that led to the prediction.\n",
    "        \"\"\"\n",
    "        if not hasattr(model, 'tree_'):\n",
    "            raise ValueError(\"Model must have tree_ attribute\")\n",
    "        \n",
    "        explanation = []\n",
    "        node = model.tree_\n",
    "        \n",
    "        while isinstance(node, InternalNode):\n",
    "            feature_name = feature_names[node.feature_idx]\n",
    "            feature_value = sample[node.feature_idx]\n",
    "            threshold = node.threshold\n",
    "            \n",
    "            if feature_value <= threshold:\n",
    "                direction = \"≤\"\n",
    "                explanation.append(f\"{feature_name} {direction} {threshold:.3f}\")\n",
    "                node = node.left\n",
    "            else:\n",
    "                direction = \">\"\n",
    "                explanation.append(f\"{feature_name} {direction} {threshold:.3f}\")\n",
    "                node = node.right\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction = node.prediction\n",
    "        confidence = node.confidence\n",
    "        \n",
    "        return {\n",
    "            'decision_path': explanation,\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'samples_in_leaf': node.samples\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def business_rule_translation(rules_text):\n",
    "        \"\"\"\n",
    "        Convert technical tree rules to business language.\n",
    "        \n",
    "        Example transformation:\n",
    "        \"feature_0 <= 3.5\" → \"Customer purchase amount <= $3,500\"\n",
    "        \"\"\"\n",
    "        # This would be customized based on feature meanings\n",
    "        business_translations = {\n",
    "            'feature_0': 'Customer Purchase Amount',\n",
    "            'feature_1': 'Account Age (days)',\n",
    "            'feature_2': 'Previous Orders Count',\n",
    "            'feature_3': 'Risk Score'\n",
    "        }\n",
    "        \n",
    "        translated_rules = rules_text\n",
    "        for tech_name, business_name in business_translations.items():\n",
    "            translated_rules = translated_rules.replace(tech_name, business_name)\n",
    "        \n",
    "        return translated_rules\n",
    "\n",
    "class ScalabilityBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark decision tree performance across different scales.\n",
    "    \n",
    "    Critical for capacity planning and SLA commitments.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_training_time(max_samples=10000, step=1000):\n",
    "        \"\"\"\n",
    "        Measure how training time scales with dataset size.\n",
    "        \"\"\"\n",
    "        sample_sizes = range(step, max_samples + 1, step)\n",
    "        training_times = []\n",
    "        \n",
    "        print(\"🔬 Training Time Scalability:\")\n",
    "        print(f\"{'Samples':<10} {'Time (s)':<10} {'Time/Sample (ms)':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for n_samples in sample_sizes:\n",
    "            # Generate synthetic dataset\n",
    "            X, y = make_classification(\n",
    "                n_samples=n_samples,\n",
    "                n_features=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            # Measure training time\n",
    "            start_time = time.time()\n",
    "            dt = DecisionTreeClassifier(random_state=42)\n",
    "            dt.fit(X, y)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            training_times.append(training_time)\n",
    "            time_per_sample = (training_time * 1000) / n_samples\n",
    "            \n",
    "            print(f\"{n_samples:<10} {training_time:<10.3f} {time_per_sample:<15.3f}\")\n",
    "        \n",
    "        return list(sample_sizes), training_times\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_prediction_time(n_samples=1000, tree_depths=[3, 5, 10, 15]):\n",
    "        \"\"\"\n",
    "        Measure prediction time vs tree complexity.\n",
    "        \"\"\"\n",
    "        print(f\"\\n🚀 Prediction Time vs Tree Depth:\")\n",
    "        print(f\"{'Depth':<8} {'Time (ms)':<12} {'Predictions/sec':<15}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Generate test data\n",
    "        X_test, _ = make_classification(\n",
    "            n_samples=n_samples,\n",
    "            n_features=10,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        for depth in tree_depths:\n",
    "            # Train tree with specific depth\n",
    "            X_train, y_train = make_classification(\n",
    "                n_samples=5000,\n",
    "                n_features=10,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "            dt.fit(X_train, y_train)\n",
    "            \n",
    "            # Measure prediction time\n",
    "            start_time = time.time()\n",
    "            predictions = dt.predict(X_test)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            time_ms = prediction_time * 1000\n",
    "            predictions_per_sec = n_samples / prediction_time\n",
    "            \n",
    "            print(f\"{depth:<8} {time_ms:<12.3f} {predictions_per_sec:<15.0f}\")\n",
    "\n",
    "# Demonstrate production features\n",
    "print(\"\\n🏭 PRODUCTION FEATURE DEMONSTRATION:\")\n",
    "\n",
    "# Create realistic dataset with some complexity\n",
    "np.random.seed(42)\n",
    "X_prod, y_prod = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=8,\n",
    "    n_informative=6,\n",
    "    n_redundant=2,\n",
    "    n_clusters_per_class=1,\n",
    "    flip_y=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Add some missing values to simulate real-world data\n",
    "missing_mask = np.random.random(X_prod.shape) < 0.05\n",
    "X_prod[missing_mask] = np.nan\n",
    "\n",
    "# Define business-meaningful feature names\n",
    "feature_names = [\n",
    "    'Purchase_Amount', 'Account_Age_Days', 'Previous_Orders',\n",
    "    'Risk_Score', 'Geographic_Region', 'Device_Type',\n",
    "    'Time_Since_Last_Order', 'Customer_Lifetime_Value'\n",
    "]\n",
    "\n",
    "# Train production model\n",
    "prod_model = ProductionDecisionTree(\n",
    "    criterion='gini',\n",
    "    max_depth=6,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "prod_model.fit(X_prod, y_prod, feature_names=feature_names)\n",
    "\n",
    "print(f\"✅ Model trained successfully!\")\n",
    "print(f\"Training time: {prod_model.training_stats['training_time']:.3f} seconds\")\n",
    "print(f\"Tree depth: {prod_model.training_stats['tree_depth']}\")\n",
    "\n",
    "# Extract interpretable rules\n",
    "print(f\"\\n📋 DECISION RULES (Top 3 levels):\")\n",
    "rules = prod_model.extract_rules(max_depth=3)\n",
    "business_rules = ModelInterpretability.business_rule_translation(rules)\n",
    "print(business_rules[:500] + \"...\")\n",
    "\n",
    "# Feature importance report\n",
    "print(f\"\\n🔍 FEATURE IMPORTANCE REPORT:\")\n",
    "importance_report = prod_model.get_feature_importance_report()\n",
    "for item in importance_report[:5]:  # Top 5 features\n",
    "    print(f\"  {item['rank']}. {item['feature']}: {item['importance']:.3f}\")\n",
    "\n",
    "# Explain a single prediction\n",
    "print(f\"\\n🔬 PREDICTION EXPLANATION:\")\n",
    "sample_idx = 0\n",
    "sample = X_prod[sample_idx]\n",
    "explanation = ModelInterpretability.explain_prediction(\n",
    "    prod_model.model, sample, feature_names\n",
    ")\n",
    "\n",
    "print(f\"Sample prediction: {explanation['prediction']}\")\n",
    "print(f\"Confidence: {explanation['confidence']:.3f}\")\n",
    "print(f\"Decision path:\")\n",
    "for i, step in enumerate(explanation['decision_path']):\n",
    "    print(f\"  {i+1}. {step}\")\n",
    "\n",
    "# Performance profiling\n",
    "print(f\"\\n📊 PERFORMANCE PROFILE:\")\n",
    "profile_data = prod_model.performance_profile()\n",
    "for key, value in profile_data.items():\n",
    "    if key != 'training_stats':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Scalability benchmarks\n",
    "print(f\"\\n⚡ SCALABILITY BENCHMARKS:\")\n",
    "benchmark = ScalabilityBenchmark()\n",
    "\n",
    "# Training time benchmark (smaller scale for demo)\n",
    "sample_sizes, times = benchmark.benchmark_training_time(max_samples=5000, step=1000)\n",
    "\n",
    "# Prediction time benchmark\n",
    "benchmark.benchmark_prediction_time(n_samples=1000)\n",
    "\n",
    "print(f\"\\n💡 Production Insights:\")\n",
    "print(f\"• Missing value handling: Automatic median imputation\")\n",
    "print(f\"• Rule extraction: Ready for business stakeholder review\")\n",
    "print(f\"• Performance monitoring: Built-in profiling and statistics\")\n",
    "print(f\"• Scalability: Linear training time, constant prediction time\")\n",
    "print(f\"• Interpretability: Full decision path explanation available\")\n",
    "\n",
    "print(f\"\\n✅ Production deployment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93729c97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎯 Interview Readiness Assessment\n",
    "\n",
    "## **Critical Questions & Model Answers**\n",
    "\n",
    "### **🧮 Mathematical Foundation Questions**\n",
    "\n",
    "**Q1: \"Derive the entropy formula and explain why we use log base 2.\"**\n",
    "\n",
    "**Model Answer:**\n",
    "```\n",
    "Entropy H(S) = -Σ p_i * log₂(p_i)\n",
    "\n",
    "Derivation from information theory:\n",
    "- Information content I(x) = -log(p(x))\n",
    "- Expected information = Σ p(x) * I(x) = -Σ p(x) * log(p(x))\n",
    "- Log base 2 gives units in \"bits\" of information\n",
    "- Alternative: natural log gives \"nats\", but doesn't affect split rankings\n",
    "```\n",
    "\n",
    "**Q2: \"Compare Gini vs Entropy. When would you prefer each?\"**\n",
    "\n",
    "**Model Answer:**\n",
    "- **Gini**: Faster (no log computation), range [0, 0.5], tends toward pure splits\n",
    "- **Entropy**: More theoretically principled, range [0, log₂(c)], better probability estimates\n",
    "- **Practice**: Usually give similar results, Gini preferred for speed\n",
    "- **Choose Entropy**: When probability calibration matters (e.g., medical diagnosis)\n",
    "\n",
    "---\n",
    "\n",
    "### **⚙️ Algorithm Implementation Questions**\n",
    "\n",
    "**Q3: \"What's the time complexity of training a decision tree?\"**\n",
    "\n",
    "**Model Answer:**\n",
    "```\n",
    "Best case: O(n * m * log n)\n",
    "- n = samples, m = features\n",
    "- log n from tree depth (balanced tree)\n",
    "- For each node: sort features O(n log n), try m features\n",
    "\n",
    "Worst case: O(n² * m)\n",
    "- Occurs with skewed splits (linear tree)\n",
    "- Depth becomes O(n) instead of O(log n)\n",
    "\n",
    "Space complexity: O(n) for tree storage\n",
    "```\n",
    "\n",
    "**Q4: \"How do you handle categorical features with many categories?\"**\n",
    "\n",
    "**Model Answer:**\n",
    "1. **Binary encoding**: Convert to multiple binary features\n",
    "2. **Frequency encoding**: Replace with category frequency\n",
    "3. **Target encoding**: Replace with mean target value (with regularization)\n",
    "4. **Grouping**: Combine rare categories into \"other\"\n",
    "5. **Hash encoding**: Use feature hashing for very high cardinality\n",
    "\n",
    "---\n",
    "\n",
    "### **🏭 Production & Business Questions**\n",
    "\n",
    "**Q5: \"Customer complains our fraud detection is biased. How do you investigate?\"**\n",
    "\n",
    "**Model Answer:**\n",
    "1. **Audit feature importance**: Check if protected attributes have high importance\n",
    "2. **Analyze decision paths**: Extract rules and review for discriminatory patterns\n",
    "3. **Fairness metrics**: Calculate equalized odds, demographic parity\n",
    "4. **Data investigation**: Check for proxy variables (zip code → race)\n",
    "5. **Solution**: Fairness-aware tree algorithms, adversarial debiasing\n",
    "\n",
    "**Q6: \"How do you explain a tree's decision to non-technical stakeholders?\"**\n",
    "\n",
    "**Model Answer:**\n",
    "```\n",
    "Example explanation:\n",
    "\"The system flagged this transaction because:\n",
    "1. Purchase amount ($5,000) exceeds the high-risk threshold ($3,000)\n",
    "2. Account is only 15 days old (our model flags accounts < 30 days)\n",
    "3. This combination has 85% fraud rate in historical data\n",
    "\n",
    "Recommendation: Manual review required\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 **Comprehensive Summary**\n",
    "\n",
    "### **Key Concepts Mastered:**\n",
    "\n",
    "#### **1. Mathematical Foundations ✅**\n",
    "- Information theory (entropy, information gain, Gini)\n",
    "- Splitting criteria and bias issues\n",
    "- Complexity analysis\n",
    "\n",
    "#### **2. Algorithm Implementation ✅**\n",
    "- Recursive tree construction\n",
    "- Stopping criteria and pruning\n",
    "- Feature selection and handling\n",
    "\n",
    "#### **3. Advanced Techniques ✅**\n",
    "- Ensemble integration (Random Forest, Boosting)\n",
    "- Feature importance (multiple methods)\n",
    "- Bias-variance analysis\n",
    "\n",
    "#### **4. Production Deployment ✅**\n",
    "- Scalability considerations\n",
    "- Interpretability and explanation\n",
    "- Real-world applications and pitfalls\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **Amazon-Specific Applications**\n",
    "\n",
    "### **Fraud Detection Pipeline:**\n",
    "```python\n",
    "# Multi-stage decision tree system\n",
    "Stage 1: Fast screening (simple tree, <10ms)\n",
    "├── Amount > $1000? → Stage 2\n",
    "└── Amount ≤ $1000 → AUTO-APPROVE\n",
    "\n",
    "Stage 2: Detailed analysis (complex tree, <100ms)\n",
    "├── High-risk patterns → MANUAL_REVIEW\n",
    "├── Medium-risk → ADDITIONAL_VERIFICATION\n",
    "└── Low-risk → AUTO-APPROVE\n",
    "```\n",
    "\n",
    "### **Recommendation System Integration:**\n",
    "```python\n",
    "# User segmentation for recommendation strategy\n",
    "Customer_Type_Tree:\n",
    "├── Purchase_History > 100\n",
    "│   ├── Electronics_Affinity → Tech_Recommendations\n",
    "│   └── Fashion_Affinity → Style_Recommendations\n",
    "└── Purchase_History ≤ 100\n",
    "    ├── Price_Sensitive → Deal_Based_Recommendations\n",
    "    └── Quality_Focused → Premium_Recommendations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 **Study Plan & Next Steps**\n",
    "\n",
    "### **Week 1: Foundations**\n",
    "- [ ] Implement entropy/Gini from scratch\n",
    "- [ ] Code basic tree construction algorithm\n",
    "- [ ] Practice complexity analysis\n",
    "\n",
    "### **Week 2: Advanced Features**\n",
    "- [ ] Implement pruning algorithms\n",
    "- [ ] Code feature importance calculations\n",
    "- [ ] Study ensemble methods integration\n",
    "\n",
    "### **Week 3: Production Focus**\n",
    "- [ ] Build production-ready tree class\n",
    "- [ ] Practice explaining decisions\n",
    "- [ ] Study real-world case studies\n",
    "\n",
    "### **Interview Preparation:**\n",
    "- [ ] Can implement tree from scratch in 45 minutes\n",
    "- [ ] Can explain business impact of design choices\n",
    "- [ ] Can debug common production issues\n",
    "- [ ] Can communicate with non-technical stakeholders\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 **Final Assessment**\n",
    "\n",
    "**You are ready for Amazon Applied Scientist interviews when you can:**\n",
    "\n",
    "1. **✅ Code**: Implement decision tree from scratch with proper complexity analysis\n",
    "2. **✅ Explain**: Translate technical concepts to business stakeholders\n",
    "3. **✅ Scale**: Design production systems handling millions of decisions/day\n",
    "4. **✅ Debug**: Identify and solve real-world ML pipeline issues\n",
    "5. **✅ Innovate**: Propose improvements and new applications\n",
    "\n",
    "### **🎯 Success Metrics:**\n",
    "- **Technical Depth**: Master mathematical foundations\n",
    "- **System Design**: Handle production scale and constraints\n",
    "- **Business Acumen**: Connect ML decisions to customer value\n",
    "- **Communication**: Explain complex concepts simply\n",
    "\n",
    "---\n",
    "\n",
    "**🚀 Ready to ace your Amazon Applied Scientist interview!**\n",
    "\n",
    "*Remember: Amazon values customer obsession, ownership, and dive deep. Show how decision trees create customer value through interpretable, scalable ML systems.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85515b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Visualization and Exercises\n",
    "\n",
    "print(\"🎯 DECISION TREES LECTURE COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive visualization of decision tree concepts\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# 1. Entropy vs Gini comparison\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "p_values = np.linspace(0.01, 0.99, 100)\n",
    "entropy_values = [-p * np.log2(p) - (1-p) * np.log2(1-p) for p in p_values]\n",
    "gini_values = [2 * p * (1-p) for p in p_values]\n",
    "\n",
    "ax1.plot(p_values, entropy_values, 'b-', linewidth=2, label='Entropy')\n",
    "ax1.plot(p_values, gini_values, 'r-', linewidth=2, label='Gini')\n",
    "ax1.set_xlabel('Probability of Class 1')\n",
    "ax1.set_ylabel('Impurity')\n",
    "ax1.set_title('Entropy vs Gini Impurity')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Information gain visualization\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "# Simulate information gain for different thresholds\n",
    "thresholds = np.linspace(0, 10, 50)\n",
    "# Simulated information gain (would be calculated from real data)\n",
    "info_gains = np.exp(-(thresholds - 5)**2 / 4) * 0.3\n",
    "\n",
    "ax2.plot(thresholds, info_gains, 'g-', linewidth=2, marker='o', markersize=4)\n",
    "best_threshold = thresholds[np.argmax(info_gains)]\n",
    "ax2.axvline(best_threshold, color='red', linestyle='--', alpha=0.7, label=f'Best: {best_threshold:.1f}')\n",
    "ax2.set_xlabel('Threshold Value')\n",
    "ax2.set_ylabel('Information Gain')\n",
    "ax2.set_title('Information Gain vs Threshold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Bias-Variance trade-off\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "depths = range(1, 16)\n",
    "bias = [1.0 / d for d in depths]  # Simplified: bias decreases with depth\n",
    "variance = [d * 0.05 for d in depths]  # Simplified: variance increases with depth\n",
    "total_error = [b + v + 0.1 for b, v in zip(bias, variance)]\n",
    "\n",
    "ax3.plot(depths, bias, 'r-', linewidth=2, label='Bias²')\n",
    "ax3.plot(depths, variance, 'b-', linewidth=2, label='Variance')\n",
    "ax3.plot(depths, total_error, 'k-', linewidth=2, label='Total Error')\n",
    "optimal_depth = depths[np.argmin(total_error)]\n",
    "ax3.axvline(optimal_depth, color='green', linestyle='--', alpha=0.7, label=f'Optimal: {optimal_depth}')\n",
    "ax3.set_xlabel('Tree Depth')\n",
    "ax3.set_ylabel('Error')\n",
    "ax3.set_title('Bias-Variance Trade-off')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training time complexity\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "sample_sizes = np.array([100, 500, 1000, 2000, 5000, 10000])\n",
    "# Simulate O(n log n) complexity\n",
    "training_times = sample_sizes * np.log(sample_sizes) / 1000\n",
    "\n",
    "ax4.plot(sample_sizes, training_times, 'purple', linewidth=2, marker='s', markersize=6)\n",
    "ax4.set_xlabel('Number of Samples')\n",
    "ax4.set_ylabel('Training Time (arbitrary units)')\n",
    "ax4.set_title('Training Time Complexity O(n log n)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Feature importance comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "features = ['Amount', 'Age', 'History', 'Risk', 'Region', 'Device']\n",
    "impurity_importance = [0.35, 0.25, 0.20, 0.15, 0.03, 0.02]\n",
    "permutation_importance = [0.40, 0.22, 0.18, 0.12, 0.05, 0.03]\n",
    "\n",
    "x_pos = np.arange(len(features))\n",
    "width = 0.35\n",
    "\n",
    "ax5.bar(x_pos - width/2, impurity_importance, width, label='Impurity-based', alpha=0.8)\n",
    "ax5.bar(x_pos + width/2, permutation_importance, width, label='Permutation', alpha=0.8)\n",
    "ax5.set_xlabel('Features')\n",
    "ax5.set_ylabel('Importance')\n",
    "ax5.set_title('Feature Importance Methods')\n",
    "ax5.set_xticks(x_pos)\n",
    "ax5.set_xticklabels(features, rotation=45)\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Model performance vs complexity\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "complexity = range(1, 21)\n",
    "train_acc = [0.5 + 0.4 * (1 - np.exp(-c/3)) for c in complexity]\n",
    "test_acc = [0.5 + 0.35 * (1 - np.exp(-c/3)) - 0.01 * c for c in complexity]\n",
    "\n",
    "ax6.plot(complexity, train_acc, 'g-', linewidth=2, label='Training Accuracy', marker='o')\n",
    "ax6.plot(complexity, test_acc, 'r-', linewidth=2, label='Testing Accuracy', marker='s')\n",
    "best_complexity = complexity[np.argmax(test_acc)]\n",
    "ax6.axvline(best_complexity, color='blue', linestyle='--', alpha=0.7, label=f'Optimal: {best_complexity}')\n",
    "ax6.set_xlabel('Model Complexity')\n",
    "ax6.set_ylabel('Accuracy')\n",
    "ax6.set_title('Training vs Testing Performance')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Decision Trees: Complete Visual Guide', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Final interview coding exercises\n",
    "print(f\"\\n🧮 INTERVIEW CODING EXERCISES:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "print(f\"\\n1. Quick Implementation Challenge (15 minutes):\")\n",
    "print(f\"   Implement binary classification tree with Gini criterion\")\n",
    "\n",
    "print(f\"\\n2. Complexity Analysis (10 minutes):\")\n",
    "print(f\"   Derive time complexity for finding best split\")\n",
    "\n",
    "print(f\"\\n3. Production Scenario (15 minutes):\")\n",
    "print(f\"   Design tree-based fraud detection for 1M transactions/day\")\n",
    "\n",
    "print(f\"\\n4. Business Communication (5 minutes):\")\n",
    "print(f\"   Explain why a transaction was flagged to customer service\")\n",
    "\n",
    "# Success metrics summary\n",
    "print(f\"\\n🏆 LECTURE COMPLETION SUMMARY:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "concepts_covered = [\n",
    "    \"✅ Information Theory (Entropy, Gini, Information Gain)\",\n",
    "    \"✅ Tree Construction Algorithm (ID3/CART)\",\n",
    "    \"✅ Complexity Analysis (Time: O(n·m·log n), Space: O(n))\",\n",
    "    \"✅ Pruning Techniques (Pre/Post, Cost Complexity)\",\n",
    "    \"✅ Feature Importance (Impurity, Permutation, Drop-column)\",\n",
    "    \"✅ Bias-Variance Trade-off Analysis\",\n",
    "    \"✅ Ensemble Integration (Random Forest, Boosting)\",\n",
    "    \"✅ Production Considerations (Scalability, Interpretability)\",\n",
    "    \"✅ Real-world Applications (Fraud, Recommendations, A/B Testing)\",\n",
    "    \"✅ Business Communication Strategies\"\n",
    "]\n",
    "\n",
    "for concept in concepts_covered:\n",
    "    print(f\"  {concept}\")\n",
    "\n",
    "print(f\"\\n🎯 AMAZON APPLIED SCIENTIST READINESS:\")\n",
    "print(f\"  ✅ Technical Mastery: Deep understanding of algorithms\")\n",
    "print(f\"  ✅ System Design: Production-scale considerations\")\n",
    "print(f\"  ✅ Business Impact: Customer value creation\")\n",
    "print(f\"  ✅ Communication: Stakeholder explanation skills\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"  1. Practice implementing trees from scratch (3-5 times)\")\n",
    "print(f\"  2. Review real Amazon case studies\")\n",
    "print(f\"  3. Practice explaining decisions to business stakeholders\")\n",
    "print(f\"  4. Study ensemble methods (Random Forest, XGBoost)\")\n",
    "print(f\"  5. Learn tree-based feature selection techniques\")\n",
    "\n",
    "print(f\"\\n💡 INTERVIEW TIPS:\")\n",
    "print(f\"  • Start with simple solution, then optimize\")\n",
    "print(f\"  • Always consider edge cases (empty nodes, single class)\")\n",
    "print(f\"  • Discuss trade-offs (bias-variance, speed-accuracy)\")\n",
    "print(f\"  • Connect to business value and customer impact\")\n",
    "print(f\"  • Be ready to implement core algorithm in 30-45 minutes\")\n",
    "\n",
    "print(f\"\\n🏁 DECISION TREES LECTURE SERIES COMPLETE!\")\n",
    "print(f\"   You're now ready to tackle Amazon Applied Scientist interviews\")\n",
    "print(f\"   with confidence in decision tree algorithms!\")\n",
    "\n",
    "# Quick self-assessment\n",
    "def quick_assessment():\n",
    "    \"\"\"\n",
    "    Quick self-assessment for interview readiness.\n",
    "    \"\"\"\n",
    "    questions = [\n",
    "        \"Can you implement a decision tree from scratch in 45 minutes?\",\n",
    "        \"Can you explain entropy vs Gini to a business stakeholder?\",\n",
    "        \"Can you analyze time complexity of tree construction?\",\n",
    "        \"Can you design a production fraud detection system?\",\n",
    "        \"Can you explain why a specific prediction was made?\",\n",
    "        \"Can you discuss bias-variance trade-offs in trees?\",\n",
    "        \"Can you integrate trees with ensemble methods?\",\n",
    "        \"Can you handle missing values and categorical features?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n🎯 SELF-ASSESSMENT CHECKLIST:\")\n",
    "    print(f\"Rate yourself 1-5 on each question:\")\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"  {i}. {question}\")\n",
    "    \n",
    "    print(f\"\\n🏆 Target Score: 32+ out of 40 for interview readiness\")\n",
    "\n",
    "quick_assessment()\n",
    "\n",
    "print(f\"\\n🎓 Congratulations on completing the Decision Trees lecture series!\")\n",
    "print(f\"   You've gained comprehensive knowledge from mathematical foundations\")\n",
    "print(f\"   to production deployment. You're ready for Amazon Applied Scientist interviews!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
