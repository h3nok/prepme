{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262927e6",
   "metadata": {},
   "source": [
    "# Loss Functions: Mathematical Foundations & Custom Implementations\n",
    "\n",
    "## üéØ **Amazon Applied Scientist Interview Preparation**\n",
    "\n",
    "This notebook provides **comprehensive coverage** of loss functions essential for ML interviews, with focus on:\n",
    "\n",
    "### üìö **What You'll Master:**\n",
    "1. **Mathematical derivations** of each loss function\n",
    "2. **Custom implementations** from scratch (no libraries)\n",
    "3. **Gradient computations** for optimization\n",
    "4. **When to use** each loss function\n",
    "5. **Numerical stability** considerations\n",
    "6. **Business applications** and trade-offs\n",
    "\n",
    "### üî• **Loss Functions Covered:**\n",
    "- **Regression**: MSE, MAE, Huber, Quantile\n",
    "- **Classification**: Binary Cross-Entropy, Categorical CE, Focal, Hinge\n",
    "- **Advanced**: Triplet, Contrastive, Wasserstein, KL Divergence\n",
    "- **Custom**: Weighted losses, Label smoothing\n",
    "\n",
    "### ‚ö†Ô∏è **Interview Focus Areas:**\n",
    "1. ‚úÖ Derive gradients by hand\n",
    "2. ‚úÖ Implement numerical stability fixes\n",
    "3. ‚úÖ Explain when each loss is optimal\n",
    "4. ‚úÖ Connect to business metrics\n",
    "5. ‚úÖ Handle edge cases and overflow\n",
    "6. ‚úÖ Compare computational complexity\n",
    "\n",
    "---\n",
    "**üö® Key Interview Insight**: Loss function choice can make or break model performance. Understanding the \"why\" behind each formula is crucial for Amazon's technical depth rounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93454b",
   "metadata": {},
   "source": [
    "## 1. Essential Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411674b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Union, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Mathematical constants\n",
    "EPSILON = 1e-15  # Numerical stability constant\n",
    "LOG_EPSILON = 1e-8  # For log operations\n",
    "\n",
    "print(\"üì¶ IMPORTS COMPLETE\")\n",
    "print(\"üéØ Ready for loss function implementations!\")\n",
    "print(f\"üìä NumPy version: {np.__version__}\")\n",
    "print(f\"üî¢ Numerical epsilon: {EPSILON}\")\n",
    "print(f\"üìà Matplotlib backend: {plt.get_backend()}\")\n",
    "\n",
    "# Quick test of numerical stability\n",
    "print(f\"\\nüß™ NUMERICAL STABILITY TESTS:\")\n",
    "print(f\"   log(EPSILON) = {np.log(EPSILON):.2f}\")\n",
    "print(f\"   exp(700) = {np.exp(700) if 700 < 709 else 'overflow!'}\")\n",
    "print(f\"   exp(-700) = {np.exp(-700):.2e}\")\n",
    "print(\"‚úÖ Environment ready for stable implementations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64d1a1e",
   "metadata": {},
   "source": [
    "## 2. Regression Loss Functions\n",
    "\n",
    "### üéØ **Mathematical Foundation**\n",
    "\n",
    "Regression losses measure the difference between predicted continuous values and true targets. Key considerations:\n",
    "\n",
    "| **Loss Function** | **Formula** | **Gradient** | **Robustness** | **Use Case** |\n",
    "|-------------------|-------------|--------------|----------------|---------------|\n",
    "| **MSE (L2)** | `¬Ω(y - ≈∑)¬≤` | `-(y - ≈∑)` | Sensitive to outliers | Gaussian noise, smooth optimization |\n",
    "| **MAE (L1)** | `|y - ≈∑|` | `sign(≈∑ - y)` | Robust to outliers | Heavy-tailed noise, median regression |\n",
    "| **Huber** | Hybrid L1/L2 | Smooth transition | Balanced robustness | Robust regression with smooth gradients |\n",
    "| **Quantile** | Asymmetric L1 | Weighted sign | Robust, asymmetric | Risk modeling, confidence intervals |\n",
    "\n",
    "### üö® **Critical Interview Insights:**\n",
    "- **MSE** penalizes large errors quadratically ‚Üí sensitive to outliers\n",
    "- **MAE** treats all errors equally ‚Üí robust but non-differentiable at 0\n",
    "- **Huber** combines both ‚Üí robust with smooth gradients\n",
    "- **Quantile** enables uncertainty quantification ‚Üí essential for risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f447eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionLosses:\n",
    "    \"\"\"\n",
    "    Custom implementations of regression loss functions.\n",
    "    \n",
    "    üéØ INTERVIEW FOCUS: Understand mathematical properties and implementation trade-offs\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_loss(y_true: np.ndarray, y_pred: np.ndarray, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Mean Squared Error (L2 Loss)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L(y, ≈∑) = ¬Ω(y - ≈∑)¬≤\n",
    "        \n",
    "        üîç GRADIENT:\n",
    "        ‚àÇL/‚àÇ≈∑ = -(y - ≈∑) = (≈∑ - y)\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Differentiable everywhere\n",
    "        ‚Ä¢ Convex (single global minimum)\n",
    "        ‚Ä¢ Sensitive to outliers (quadratic penalty)\n",
    "        ‚Ä¢ Optimal for Gaussian noise\n",
    "        \n",
    "        Args:\n",
    "            y_true: Ground truth values\n",
    "            y_pred: Predicted values\n",
    "            reduction: 'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        diff = y_pred - y_true\n",
    "        squared_error = 0.5 * diff**2\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(squared_error)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(squared_error)\n",
    "        else:\n",
    "            return squared_error\n",
    "    \n",
    "    @staticmethod\n",
    "    def mse_gradient(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of MSE w.r.t. predictions\n",
    "        \n",
    "        ‚àÇL/‚àÇ≈∑ = (≈∑ - y)\n",
    "        \"\"\"\n",
    "        return y_pred - y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def mae_loss(y_true: np.ndarray, y_pred: np.ndarray, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Mean Absolute Error (L1 Loss)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L(y, ≈∑) = |y - ≈∑|\n",
    "        \n",
    "        üîç GRADIENT:\n",
    "        ‚àÇL/‚àÇ≈∑ = sign(≈∑ - y)  [undefined at ≈∑ = y]\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Non-differentiable at ≈∑ = y\n",
    "        ‚Ä¢ Robust to outliers (linear penalty)\n",
    "        ‚Ä¢ Optimal for Laplace noise\n",
    "        ‚Ä¢ Promotes sparsity\n",
    "        \n",
    "        üö® CAVEAT: Gradient is discontinuous at zero!\n",
    "        \"\"\"\n",
    "        abs_error = np.abs(y_pred - y_true)\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(abs_error)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(abs_error)\n",
    "        else:\n",
    "            return abs_error\n",
    "    \n",
    "    @staticmethod\n",
    "    def mae_gradient(y_true: np.ndarray, y_pred: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of MAE w.r.t. predictions (with smoothing for numerical stability)\n",
    "        \n",
    "        ‚àÇL/‚àÇ≈∑ = sign(≈∑ - y)\n",
    "        \n",
    "        üîß NUMERICAL FIX: Use smooth approximation near zero\n",
    "        \"\"\"\n",
    "        diff = y_pred - y_true\n",
    "        # Smooth sign function to avoid undefined gradient at 0\n",
    "        return np.tanh(diff / epsilon)\n",
    "    \n",
    "    @staticmethod\n",
    "    def huber_loss(y_true: np.ndarray, y_pred: np.ndarray, delta: float = 1.0, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Huber Loss (Smooth L1 Loss)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L(y, ≈∑) = {\n",
    "            ¬Ω(y - ≈∑)¬≤           if |y - ≈∑| ‚â§ Œ¥\n",
    "            Œ¥|y - ≈∑| - ¬ΩŒ¥¬≤      if |y - ≈∑| > Œ¥\n",
    "        }\n",
    "        \n",
    "        üîç GRADIENT:\n",
    "        ‚àÇL/‚àÇ≈∑ = {\n",
    "            (≈∑ - y)           if |y - ≈∑| ‚â§ Œ¥\n",
    "            Œ¥¬∑sign(≈∑ - y)     if |y - ≈∑| > Œ¥\n",
    "        }\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Smooth everywhere (differentiable)\n",
    "        ‚Ä¢ Quadratic for small errors (MSE-like)\n",
    "        ‚Ä¢ Linear for large errors (MAE-like)\n",
    "        ‚Ä¢ Balanced robustness\n",
    "        \n",
    "        Args:\n",
    "            delta: Threshold between quadratic and linear regions\n",
    "        \"\"\"\n",
    "        diff = y_pred - y_true\n",
    "        abs_diff = np.abs(diff)\n",
    "        \n",
    "        # Quadratic region: |error| ‚â§ Œ¥\n",
    "        quadratic_mask = abs_diff <= delta\n",
    "        quadratic_loss = 0.5 * diff**2\n",
    "        \n",
    "        # Linear region: |error| > Œ¥\n",
    "        linear_mask = abs_diff > delta\n",
    "        linear_loss = delta * abs_diff - 0.5 * delta**2\n",
    "        \n",
    "        loss = quadratic_mask * quadratic_loss + linear_mask * linear_loss\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def huber_gradient(y_true: np.ndarray, y_pred: np.ndarray, delta: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of Huber loss w.r.t. predictions\n",
    "        \"\"\"\n",
    "        diff = y_pred - y_true\n",
    "        abs_diff = np.abs(diff)\n",
    "        \n",
    "        # Quadratic region gradient: (≈∑ - y)\n",
    "        quadratic_mask = abs_diff <= delta\n",
    "        quadratic_grad = diff\n",
    "        \n",
    "        # Linear region gradient: Œ¥¬∑sign(≈∑ - y)\n",
    "        linear_mask = abs_diff > delta\n",
    "        linear_grad = delta * np.sign(diff)\n",
    "        \n",
    "        return quadratic_mask * quadratic_grad + linear_mask * linear_grad\n",
    "\n",
    "# Test the implementations\n",
    "print(\"üß™ TESTING REGRESSION LOSSES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "y_true = np.array([1.0, 2.0, 3.0, 100.0])  # Note: 100.0 is an outlier\n",
    "y_pred = np.array([1.1, 2.2, 2.8, 10.0])   # Outlier prediction is way off\n",
    "\n",
    "losses = RegressionLosses()\n",
    "\n",
    "# Calculate losses\n",
    "mse = losses.mse_loss(y_true, y_pred)\n",
    "mae = losses.mae_loss(y_true, y_pred)\n",
    "huber = losses.huber_loss(y_true, y_pred, delta=1.0)\n",
    "\n",
    "print(f\"Test data:\")\n",
    "print(f\"  y_true: {y_true}\")\n",
    "print(f\"  y_pred: {y_pred}\")\n",
    "print(f\"  errors: {y_pred - y_true}\")\n",
    "print(f\"\\nLoss comparison:\")\n",
    "print(f\"  MSE:    {mse:.4f} (sensitive to outlier)\")\n",
    "print(f\"  MAE:    {mae:.4f} (robust to outlier)\")\n",
    "print(f\"  Huber:  {huber:.4f} (balanced)\")\n",
    "\n",
    "# Test gradients\n",
    "mse_grad = losses.mse_gradient(y_true, y_pred)\n",
    "mae_grad = losses.mae_gradient(y_true, y_pred)\n",
    "huber_grad = losses.huber_gradient(y_true, y_pred)\n",
    "\n",
    "print(f\"\\nGradient comparison:\")\n",
    "print(f\"  MSE grad:   {mse_grad}\")\n",
    "print(f\"  MAE grad:   {mae_grad}\")\n",
    "print(f\"  Huber grad: {huber_grad}\")\n",
    "\n",
    "print(f\"\\nüí° INTERVIEW INSIGHT:\")\n",
    "print(f\"   Notice how MSE gradient grows linearly with error size,\")\n",
    "print(f\"   while MAE gradient is constant (¬±1) regardless of error magnitude.\")\n",
    "print(f\"   Huber combines both: smooth near zero, clipped for large errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e21fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Quantile Loss to RegressionLosses class\n",
    "def quantile_loss(y_true: np.ndarray, y_pred: np.ndarray, quantile: float = 0.5, reduction: str = 'mean') -> float:\n",
    "    \"\"\"\n",
    "    Quantile Loss (Pinball Loss)\n",
    "    \n",
    "    üìö MATHEMATICAL FOUNDATION:\n",
    "    L_œÑ(y, ≈∑) = {\n",
    "        œÑ(y - ≈∑)       if y ‚â• ≈∑  (under-prediction)\n",
    "        (œÑ-1)(y - ≈∑)   if y < ≈∑  (over-prediction)\n",
    "    }\n",
    "    \n",
    "    üîç GRADIENT:\n",
    "    ‚àÇL/‚àÇ≈∑ = {\n",
    "        -œÑ      if y ‚â• ≈∑\n",
    "        -(œÑ-1)  if y < ≈∑\n",
    "    }\n",
    "    \n",
    "    ‚ö†Ô∏è PROPERTIES:\n",
    "    ‚Ä¢ Asymmetric penalty (different costs for over/under-prediction)\n",
    "    ‚Ä¢ œÑ = 0.5 ‚Üí MAE (median regression)\n",
    "    ‚Ä¢ œÑ ‚â† 0.5 ‚Üí Biased toward quantile\n",
    "    ‚Ä¢ Enables uncertainty quantification\n",
    "    \n",
    "    üéØ BUSINESS APPLICATIONS:\n",
    "    ‚Ä¢ Risk management (Value at Risk)\n",
    "    ‚Ä¢ Inventory optimization\n",
    "    ‚Ä¢ Demand forecasting with asymmetric costs\n",
    "    \n",
    "    Args:\n",
    "        quantile: Target quantile œÑ ‚àà (0, 1)\n",
    "    \"\"\"\n",
    "    error = y_true - y_pred\n",
    "    \n",
    "    # Asymmetric penalty\n",
    "    over_pred_mask = error < 0  # y < ≈∑ (over-prediction)\n",
    "    under_pred_mask = error >= 0  # y ‚â• ≈∑ (under-prediction)\n",
    "    \n",
    "    loss = (under_pred_mask * quantile * error + \n",
    "            over_pred_mask * (quantile - 1) * error)\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return np.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        return np.sum(loss)\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "def quantile_gradient(y_true: np.ndarray, y_pred: np.ndarray, quantile: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"Gradient of quantile loss w.r.t. predictions\"\"\"\n",
    "    error = y_true - y_pred\n",
    "    over_pred_mask = error < 0\n",
    "    under_pred_mask = error >= 0\n",
    "    \n",
    "    return -(under_pred_mask * quantile + over_pred_mask * (quantile - 1))\n",
    "\n",
    "# Add methods to the class\n",
    "RegressionLosses.quantile_loss = staticmethod(quantile_loss)\n",
    "RegressionLosses.quantile_gradient = staticmethod(quantile_gradient)\n",
    "\n",
    "# Visualize regression losses\n",
    "def visualize_regression_losses():\n",
    "    \"\"\"Visualize how different regression losses behave\"\"\"\n",
    "    errors = np.linspace(-5, 5, 1000)\n",
    "    y_true = np.zeros_like(errors)\n",
    "    y_pred = errors  # So error = y_pred - y_true = errors\n",
    "    \n",
    "    losses = RegressionLosses()\n",
    "    \n",
    "    mse_vals = [losses.mse_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in errors]\n",
    "    mae_vals = [losses.mae_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in errors]\n",
    "    huber_vals = [losses.huber_loss(np.array([0]), np.array([e]), delta=1.0, reduction='none')[0] for e in errors]\n",
    "    quantile_05_vals = [losses.quantile_loss(np.array([0]), np.array([e]), quantile=0.05, reduction='none')[0] for e in errors]\n",
    "    quantile_95_vals = [losses.quantile_loss(np.array([0]), np.array([e]), quantile=0.95, reduction='none')[0] for e in errors]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss functions\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(errors, mse_vals, 'b-', linewidth=2, label='MSE (L2)')\n",
    "    plt.plot(errors, mae_vals, 'r-', linewidth=2, label='MAE (L1)')\n",
    "    plt.plot(errors, huber_vals, 'g-', linewidth=2, label='Huber (Œ¥=1)')\n",
    "    plt.xlabel('Prediction Error (≈∑ - y)')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Regression Loss Functions\\n(MSE vs MAE vs Huber)', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Gradients\n",
    "    plt.subplot(2, 2, 2)\n",
    "    mse_grads = errors  # Gradient of MSE is just the error\n",
    "    mae_grads = np.sign(errors)  # Gradient of MAE is sign function\n",
    "    huber_grads = [losses.huber_gradient(np.array([0]), np.array([e]))[0] for e in errors]\n",
    "    \n",
    "    plt.plot(errors, mse_grads, 'b-', linewidth=2, label='MSE Gradient')\n",
    "    plt.plot(errors, mae_grads, 'r-', linewidth=2, label='MAE Gradient')\n",
    "    plt.plot(errors, huber_grads, 'g-', linewidth=2, label='Huber Gradient')\n",
    "    plt.xlabel('Prediction Error (≈∑ - y)')\n",
    "    plt.ylabel('Gradient Value')\n",
    "    plt.title('Loss Function Gradients', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Quantile losses\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(errors, quantile_05_vals, 'purple', linewidth=2, label='Quantile 0.05 (5th percentile)')\n",
    "    plt.plot(errors, mae_vals, 'orange', linewidth=2, label='Quantile 0.5 (median = MAE)')\n",
    "    plt.plot(errors, quantile_95_vals, 'brown', linewidth=2, label='Quantile 0.95 (95th percentile)')\n",
    "    plt.xlabel('Prediction Error (≈∑ - y)')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Quantile Loss Functions\\n(Asymmetric Penalties)', fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Outlier sensitivity comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    outlier_errors = np.array([-10, -5, -1, 0, 1, 5, 10])\n",
    "    mse_outlier = [losses.mse_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in outlier_errors]\n",
    "    mae_outlier = [losses.mae_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in outlier_errors]\n",
    "    huber_outlier = [losses.huber_loss(np.array([0]), np.array([e]), delta=1.0, reduction='none')[0] for e in outlier_errors]\n",
    "    \n",
    "    x_pos = np.arange(len(outlier_errors))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.bar(x_pos - width, mse_outlier, width, label='MSE', alpha=0.8)\n",
    "    plt.bar(x_pos, mae_outlier, width, label='MAE', alpha=0.8)\n",
    "    plt.bar(x_pos + width, huber_outlier, width, label='Huber', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.title('Outlier Sensitivity Comparison', fontweight='bold')\n",
    "    plt.xticks(x_pos, outlier_errors)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run visualization\n",
    "visualize_regression_losses()\n",
    "\n",
    "print(\"üìä VISUALIZATION ANALYSIS:\")\n",
    "print(\"1. MSE grows quadratically ‚Üí heavily penalizes outliers\")\n",
    "print(\"2. MAE grows linearly ‚Üí robust to outliers\")\n",
    "print(\"3. Huber combines both ‚Üí smooth gradients + robustness\")\n",
    "print(\"4. Quantile losses ‚Üí asymmetric penalties for risk modeling\")\n",
    "print(\"\\nüéØ INTERVIEW TAKEAWAY:\")\n",
    "print(\"   Choose loss function based on noise distribution and business requirements!\")\n",
    "\n",
    "# Test quantile loss\n",
    "print(f\"\\nüß™ QUANTILE LOSS DEMONSTRATION:\")\n",
    "y_true = np.array([10.0])\n",
    "y_pred_under = np.array([8.0])  # Under-prediction\n",
    "y_pred_over = np.array([12.0])  # Over-prediction\n",
    "\n",
    "for q in [0.1, 0.5, 0.9]:\n",
    "    under_loss = losses.quantile_loss(y_true, y_pred_under, quantile=q)\n",
    "    over_loss = losses.quantile_loss(y_true, y_pred_over, quantile=q)\n",
    "    print(f\"   Quantile {q}: Under-pred loss = {under_loss:.3f}, Over-pred loss = {over_loss:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Notice: Low quantiles (0.1) penalize under-prediction more heavily\")\n",
    "print(f\"           High quantiles (0.9) penalize over-prediction more heavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec189a",
   "metadata": {},
   "source": [
    "## 3. Classification Loss Functions\n",
    "\n",
    "### üéØ **Mathematical Foundation**\n",
    "\n",
    "Classification losses measure prediction quality for discrete labels. Critical for understanding optimization dynamics:\n",
    "\n",
    "| **Loss Function** | **Formula** | **Gradient** | **Use Case** | **Key Properties** |\n",
    "|-------------------|-------------|--------------|---------------|-------------------|\n",
    "| **Binary Cross-Entropy** | `-[y log(p) + (1-y) log(1-p)]` | `p - y` | Binary classification | Probabilistic, smooth gradients |\n",
    "| **Categorical Cross-Entropy** | `-Œ£ y_i log(p_i)` | `p_i - y_i` | Multi-class | One-hot encoded targets |\n",
    "| **Sparse Cross-Entropy** | `-log(p_target)` | `p_i - Œ¥_i` | Multi-class | Integer targets |\n",
    "| **Focal Loss** | `-Œ±(1-p)^Œ≥ log(p)` | Complex | Imbalanced data | Down-weights easy examples |\n",
    "| **Hinge Loss** | `max(0, 1 - y¬∑f(x))` | `‚àÇ/‚àÇf` | SVM-style | Large margin, sparse |\n",
    "\n",
    "### üö® **Numerical Stability Challenges:**\n",
    "\n",
    "1. **log(0) = -‚àû**: Must clip probabilities\n",
    "2. **exp overflow**: Use log-sum-exp trick\n",
    "3. **Gradient explosion**: Careful with logits\n",
    "4. **Class imbalance**: Weight adjustments needed\n",
    "\n",
    "### üéØ **Amazon Interview Focus:**\n",
    "- When to use each loss function\n",
    "- Numerical stability implementations\n",
    "- Gradient derivations by hand\n",
    "- Business impact of loss choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLosses:\n",
    "    \"\"\"\n",
    "    Custom implementations of classification loss functions.\n",
    "    \n",
    "    üéØ INTERVIEW FOCUS: Numerical stability and gradient derivations\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Numerically stable sigmoid function\n",
    "        \n",
    "        üö® CRITICAL: Prevents overflow for large |logits|\n",
    "        \"\"\"\n",
    "        return np.where(logits >= 0,\n",
    "                        1 / (1 + np.exp(-logits)),\n",
    "                        np.exp(logits) / (1 + np.exp(logits)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(logits: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Numerically stable softmax function\n",
    "        \n",
    "        üö® CRITICAL: Subtracts max to prevent overflow\n",
    "        \n",
    "        üìö FORMULA: softmax(x_i) = exp(x_i) / Œ£ exp(x_j)\n",
    "        üîß STABLE: softmax(x_i) = exp(x_i - max(x)) / Œ£ exp(x_j - max(x))\n",
    "        \"\"\"\n",
    "        # Subtract max for numerical stability\n",
    "        shifted_logits = logits - np.max(logits, axis=axis, keepdims=True)\n",
    "        exp_logits = np.exp(shifted_logits)\n",
    "        return exp_logits / np.sum(exp_logits, axis=axis, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_crossentropy(y_true: np.ndarray, y_pred: np.ndarray, \n",
    "                           epsilon: float = EPSILON, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy Loss (from probabilities)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = -[y log(p) + (1-y) log(1-p)]\n",
    "        \n",
    "        üîç GRADIENT w.r.t. probabilities:\n",
    "        ‚àÇL/‚àÇp = -(y/p) + (1-y)/(1-p) = (p-y)/[p(1-p)]\n",
    "        \n",
    "        üîç GRADIENT w.r.t. logits (z = log(p/(1-p))):\n",
    "        ‚àÇL/‚àÇz = p - y  (much cleaner!)\n",
    "        \n",
    "        ‚ö†Ô∏è NUMERICAL ISSUES:\n",
    "        ‚Ä¢ log(0) = -‚àû ‚Üí clip probabilities\n",
    "        ‚Ä¢ p ‚àà [Œµ, 1-Œµ] prevents overflow\n",
    "        \n",
    "        üéØ INTERVIEW INSIGHT:\n",
    "        Working in logit space (before sigmoid) is more stable!\n",
    "        \"\"\"\n",
    "        # Clip probabilities to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Binary cross-entropy formula\n",
    "        loss = -(y_true * np.log(y_pred_clipped) + \n",
    "                 (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_crossentropy_with_logits(y_true: np.ndarray, logits: np.ndarray,\n",
    "                                       reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Binary Cross-Entropy with Logits (PREFERRED!)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = max(z, 0) - z¬∑y + log(1 + exp(-|z|))\n",
    "        \n",
    "        Where z = logits, y = labels\n",
    "        \n",
    "        üîß NUMERICAL STABILITY:\n",
    "        For z ‚â• 0: log(1 + exp(-z)) = log(1 + exp(-z))\n",
    "        For z < 0:  log(1 + exp(z)) = z + log(1 + exp(-z))\n",
    "        \n",
    "        ‚ö†Ô∏è WHY BETTER:\n",
    "        ‚Ä¢ No explicit sigmoid computation\n",
    "        ‚Ä¢ No probability clipping needed\n",
    "        ‚Ä¢ Numerically stable for all logit values\n",
    "        ‚Ä¢ Gradients are well-behaved\n",
    "        \n",
    "        üéØ AMAZON INSIGHT: Always prefer logit-based losses!\n",
    "        \"\"\"\n",
    "        # Numerically stable implementation\n",
    "        # L = max(z, 0) - z*y + log(1 + exp(-|z|))\n",
    "        \n",
    "        # Separate positive and negative logits for stability\n",
    "        positive_logits = np.maximum(logits, 0)\n",
    "        negative_part = np.maximum(-logits, 0)\n",
    "        \n",
    "        loss = (positive_logits - \n",
    "                logits * y_true + \n",
    "                np.log(1 + np.exp(-negative_part)) + \n",
    "                np.where(logits < 0, -logits, 0))\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def binary_crossentropy_gradient(y_true: np.ndarray, logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of BCE w.r.t. logits\n",
    "        \n",
    "        üîç BEAUTIFUL RESULT: ‚àÇL/‚àÇz = œÉ(z) - y = p - y\n",
    "        \n",
    "        This is why BCE + sigmoid is so popular in neural networks!\n",
    "        \"\"\"\n",
    "        probabilities = ClassificationLosses.sigmoid(logits)\n",
    "        return probabilities - y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_crossentropy(y_true: np.ndarray, y_pred: np.ndarray,\n",
    "                                epsilon: float = EPSILON, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy Loss (from probabilities)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = -Œ£ y_i log(p_i)\n",
    "        \n",
    "        üîç GRADIENT w.r.t. probabilities:\n",
    "        ‚àÇL/‚àÇp_i = -y_i / p_i\n",
    "        \n",
    "        üîç GRADIENT w.r.t. logits (before softmax):\n",
    "        ‚àÇL/‚àÇz_i = p_i - y_i  (clean and simple!)\n",
    "        \n",
    "        Args:\n",
    "            y_true: One-hot encoded labels (n_samples, n_classes)\n",
    "            y_pred: Predicted probabilities (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        # Clip probabilities to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Categorical cross-entropy: sum over classes, mean over samples\n",
    "        loss = -np.sum(y_true * np.log(y_pred_clipped), axis=1)\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_crossentropy_with_logits(y_true: np.ndarray, logits: np.ndarray,\n",
    "                                           reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy with Logits (PREFERRED!)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = -Œ£ y_i (z_i - log_sum_exp(z))\n",
    "        L = log_sum_exp(z) - Œ£ y_i z_i\n",
    "        \n",
    "        üîß NUMERICAL STABILITY: Uses log-sum-exp trick\n",
    "        \"\"\"\n",
    "        # Compute log_sum_exp(logits) for each sample\n",
    "        max_logits = np.max(logits, axis=1, keepdims=True)\n",
    "        shifted_logits = logits - max_logits\n",
    "        log_sum_exp = max_logits.squeeze() + np.log(np.sum(np.exp(shifted_logits), axis=1))\n",
    "        \n",
    "        # Compute -Œ£ y_i z_i for each sample\n",
    "        target_logits = np.sum(y_true * logits, axis=1)\n",
    "        \n",
    "        loss = log_sum_exp - target_logits\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def categorical_crossentropy_gradient(y_true: np.ndarray, logits: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gradient of categorical CE w.r.t. logits\n",
    "        \n",
    "        üîç BEAUTIFUL RESULT: ‚àÇL/‚àÇz_i = softmax(z_i) - y_i\n",
    "        \"\"\"\n",
    "        probabilities = ClassificationLosses.softmax(logits)\n",
    "        return probabilities - y_true\n",
    "    \n",
    "    @staticmethod\n",
    "    def sparse_categorical_crossentropy(y_true: np.ndarray, logits: np.ndarray,\n",
    "                                      reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Sparse Categorical Cross-Entropy (integer labels)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        Same as categorical CE, but y_true are integers, not one-hot\n",
    "        \n",
    "        L = -log(p_target) = -(z_target - log_sum_exp(z))\n",
    "        \n",
    "        Args:\n",
    "            y_true: Integer labels (n_samples,)\n",
    "            logits: Raw logits (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        n_samples = logits.shape[0]\n",
    "        \n",
    "        # Extract logits for true classes\n",
    "        target_logits = logits[np.arange(n_samples), y_true.astype(int)]\n",
    "        \n",
    "        # Compute log_sum_exp for each sample\n",
    "        max_logits = np.max(logits, axis=1)\n",
    "        log_sum_exp = max_logits + np.log(np.sum(np.exp(logits - max_logits[:, np.newaxis]), axis=1))\n",
    "        \n",
    "        loss = log_sum_exp - target_logits\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# Test classification losses\n",
    "print(\"üß™ TESTING CLASSIFICATION LOSSES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate test data\n",
    "np.random.seed(42)\n",
    "n_samples, n_classes = 5, 3\n",
    "\n",
    "# Binary classification test\n",
    "y_binary = np.array([0, 1, 1, 0, 1])\n",
    "logits_binary = np.array([-2.0, 1.5, 0.5, -1.0, 3.0])\n",
    "probs_binary = ClassificationLosses.sigmoid(logits_binary)\n",
    "\n",
    "print(\"BINARY CLASSIFICATION TEST:\")\n",
    "print(f\"True labels: {y_binary}\")\n",
    "print(f\"Logits:      {logits_binary}\")\n",
    "print(f\"Probabilities: {probs_binary}\")\n",
    "\n",
    "cls_losses = ClassificationLosses()\n",
    "\n",
    "# Compare BCE implementations\n",
    "bce_from_probs = cls_losses.binary_crossentropy(y_binary, probs_binary)\n",
    "bce_from_logits = cls_losses.binary_crossentropy_with_logits(y_binary, logits_binary)\n",
    "\n",
    "print(f\"\\nBinary Cross-Entropy:\")\n",
    "print(f\"  From probabilities: {bce_from_probs:.6f}\")\n",
    "print(f\"  From logits:        {bce_from_logits:.6f}\")\n",
    "print(f\"  Difference:         {abs(bce_from_probs - bce_from_logits):.8f}\")\n",
    "\n",
    "# Test gradients\n",
    "bce_grad = cls_losses.binary_crossentropy_gradient(y_binary, logits_binary)\n",
    "print(f\"  BCE gradient:       {bce_grad}\")\n",
    "\n",
    "# Multi-class classification test\n",
    "print(f\"\\nMULTI-CLASS CLASSIFICATION TEST:\")\n",
    "y_categorical = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]])  # One-hot\n",
    "y_sparse = np.array([0, 1, 2, 0, 1])  # Integer labels\n",
    "logits_multi = np.random.randn(n_samples, n_classes)\n",
    "probs_multi = cls_losses.softmax(logits_multi)\n",
    "\n",
    "print(f\"True labels (one-hot): {y_categorical}\")\n",
    "print(f\"True labels (sparse):  {y_sparse}\")\n",
    "print(f\"Logits shape: {logits_multi.shape}\")\n",
    "print(f\"Probabilities sum: {np.sum(probs_multi, axis=1)}\")  # Should be ~1.0\n",
    "\n",
    "# Compare CE implementations\n",
    "ce_from_probs = cls_losses.categorical_crossentropy(y_categorical, probs_multi)\n",
    "ce_from_logits = cls_losses.categorical_crossentropy_with_logits(y_categorical, logits_multi)\n",
    "sparse_ce = cls_losses.sparse_categorical_crossentropy(y_sparse, logits_multi)\n",
    "\n",
    "print(f\"\\nCategorical Cross-Entropy:\")\n",
    "print(f\"  From probabilities: {ce_from_probs:.6f}\")\n",
    "print(f\"  From logits:        {ce_from_logits:.6f}\")\n",
    "print(f\"  Sparse version:     {sparse_ce:.6f}\")\n",
    "\n",
    "print(f\"\\nüí° INTERVIEW INSIGHT:\")\n",
    "print(f\"   All three should give the same result!\")\n",
    "print(f\"   Differences: {abs(ce_from_probs - ce_from_logits):.8f}, {abs(ce_from_logits - sparse_ce):.8f}\")\n",
    "\n",
    "# Test numerical stability\n",
    "print(f\"\\nüî• NUMERICAL STABILITY TEST:\")\n",
    "extreme_logits = np.array([-1000, 0, 1000])  # Extreme values\n",
    "stable_softmax = cls_losses.softmax(extreme_logits)\n",
    "print(f\"Extreme logits: {extreme_logits}\")\n",
    "print(f\"Stable softmax: {stable_softmax}\")\n",
    "print(f\"Sum check: {np.sum(stable_softmax):.10f}\")  # Should be 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Focal Loss and Hinge Loss to ClassificationLosses\n",
    "def focal_loss(y_true: np.ndarray, logits: np.ndarray, alpha: float = 0.25, \n",
    "               gamma: float = 2.0, reduction: str = 'mean') -> float:\n",
    "    \"\"\"\n",
    "    Focal Loss for Addressing Class Imbalance\n",
    "    \n",
    "    üìö MATHEMATICAL FOUNDATION:\n",
    "    FL(p_t) = -Œ±_t (1 - p_t)^Œ≥ log(p_t)\n",
    "    \n",
    "    Where:\n",
    "    ‚Ä¢ p_t = p if y=1, else (1-p)\n",
    "    ‚Ä¢ Œ±_t = Œ± if y=1, else (1-Œ±)\n",
    "    \n",
    "    üîç GRADIENT (complex, involves chain rule):\n",
    "    ‚àÇFL/‚àÇz = Œ±_t * [(1-p_t)^Œ≥ (Œ≥p_t log(p_t) + p_t - y) - Œ≥(1-p_t)^(Œ≥-1) p_t log(p_t)]\n",
    "    \n",
    "    ‚ö†Ô∏è KEY PROPERTIES:\n",
    "    ‚Ä¢ Œ≥ = 0 ‚Üí Standard CE loss\n",
    "    ‚Ä¢ Œ≥ > 0 ‚Üí Down-weights easy examples\n",
    "    ‚Ä¢ Œ± balances positive/negative classes\n",
    "    ‚Ä¢ Helps with extreme class imbalance\n",
    "    \n",
    "    üéØ BUSINESS APPLICATIONS:\n",
    "    ‚Ä¢ Fraud detection (rare positives)\n",
    "    ‚Ä¢ Medical diagnosis (rare diseases)\n",
    "    ‚Ä¢ Object detection (background vs objects)\n",
    "    \n",
    "    Args:\n",
    "        alpha: Weighting factor for positive class\n",
    "        gamma: Focusing parameter (typically 2.0)\n",
    "    \"\"\"\n",
    "    # Get probabilities\n",
    "    probabilities = ClassificationLosses.sigmoid(logits)\n",
    "    \n",
    "    # Compute p_t (probability of true class)\n",
    "    p_t = y_true * probabilities + (1 - y_true) * (1 - probabilities)\n",
    "    \n",
    "    # Compute Œ±_t (class-specific weighting)\n",
    "    alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "    \n",
    "    # Focal loss formula\n",
    "    focal_weight = alpha_t * np.power(1 - p_t, gamma)\n",
    "    ce_loss = -np.log(np.clip(p_t, EPSILON, 1 - EPSILON))\n",
    "    loss = focal_weight * ce_loss\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return np.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        return np.sum(loss)\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "def hinge_loss(y_true: np.ndarray, raw_scores: np.ndarray, \n",
    "               margin: float = 1.0, reduction: str = 'mean') -> float:\n",
    "    \"\"\"\n",
    "    Hinge Loss (SVM-style)\n",
    "    \n",
    "    üìö MATHEMATICAL FOUNDATION:\n",
    "    L = max(0, margin - y * f(x))\n",
    "    \n",
    "    Where:\n",
    "    ‚Ä¢ y ‚àà {-1, +1} (NOT {0, 1}!)\n",
    "    ‚Ä¢ f(x) = raw score (NOT probability)\n",
    "    ‚Ä¢ margin = desired separation (typically 1.0)\n",
    "    \n",
    "    üîç GRADIENT:\n",
    "    ‚àÇL/‚àÇf = {\n",
    "        -y  if y * f(x) < margin\n",
    "        0   otherwise\n",
    "    }\n",
    "    \n",
    "    ‚ö†Ô∏è PROPERTIES:\n",
    "    ‚Ä¢ Non-probabilistic (raw scores)\n",
    "    ‚Ä¢ Sparse gradients (many zeros)\n",
    "    ‚Ä¢ Promotes large margin separation\n",
    "    ‚Ä¢ Robust to outliers\n",
    "    \n",
    "    üéØ WHEN TO USE:\n",
    "    ‚Ä¢ SVM-style classification\n",
    "    ‚Ä¢ When you want margin maximization\n",
    "    ‚Ä¢ Binary classification with clear separation\n",
    "    \n",
    "    Args:\n",
    "        y_true: Labels in {-1, +1} format\n",
    "        raw_scores: Raw model outputs (NOT probabilities)\n",
    "        margin: Desired margin (typically 1.0)\n",
    "    \"\"\"\n",
    "    # Ensure labels are in {-1, +1} format\n",
    "    if np.any((y_true != -1) & (y_true != 1)):\n",
    "        raise ValueError(\"Hinge loss requires labels in {-1, +1} format\")\n",
    "    \n",
    "    # Hinge loss: max(0, margin - y * f(x))\n",
    "    margin_violations = margin - y_true * raw_scores\n",
    "    loss = np.maximum(0, margin_violations)\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return np.mean(loss)\n",
    "    elif reduction == 'sum':\n",
    "        return np.sum(loss)\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "def hinge_gradient(y_true: np.ndarray, raw_scores: np.ndarray, margin: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"Gradient of hinge loss w.r.t. raw scores\"\"\"\n",
    "    margin_violations = margin - y_true * raw_scores\n",
    "    # Gradient is -y where margin is violated, 0 otherwise\n",
    "    return np.where(margin_violations > 0, -y_true, 0)\n",
    "\n",
    "# Add methods to ClassificationLosses\n",
    "ClassificationLosses.focal_loss = staticmethod(focal_loss)\n",
    "ClassificationLosses.hinge_loss = staticmethod(hinge_loss)\n",
    "ClassificationLosses.hinge_gradient = staticmethod(hinge_gradient)\n",
    "\n",
    "# Demonstrate focal loss vs BCE\n",
    "print(\"üéØ FOCAL LOSS vs BINARY CROSS-ENTROPY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create imbalanced dataset scenario\n",
    "easy_examples = np.array([1, 1, 1, 1, 1])  # Easy positive examples\n",
    "hard_examples = np.array([1, 1, 1, 1, 1])  # Hard positive examples\n",
    "\n",
    "easy_logits = np.array([5, 4, 3, 4, 6])    # High confidence (easy)\n",
    "hard_logits = np.array([0.1, -0.2, 0.3, -0.1, 0.2])  # Low confidence (hard)\n",
    "\n",
    "print(\"EASY EXAMPLES (high confidence):\")\n",
    "print(f\"Labels: {easy_examples}\")\n",
    "print(f\"Logits: {easy_logits}\")\n",
    "\n",
    "easy_bce = cls_losses.binary_crossentropy_with_logits(easy_examples, easy_logits)\n",
    "easy_focal = cls_losses.focal_loss(easy_examples, easy_logits, alpha=0.25, gamma=2.0)\n",
    "\n",
    "print(f\"BCE loss:   {easy_bce:.6f}\")\n",
    "print(f\"Focal loss: {easy_focal:.6f}\")\n",
    "print(f\"Focal reduction: {(easy_bce - easy_focal) / easy_bce * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nHARD EXAMPLES (low confidence):\")\n",
    "print(f\"Labels: {hard_examples}\")\n",
    "print(f\"Logits: {hard_logits}\")\n",
    "\n",
    "hard_bce = cls_losses.binary_crossentropy_with_logits(hard_examples, hard_logits)\n",
    "hard_focal = cls_losses.focal_loss(hard_examples, hard_logits, alpha=0.25, gamma=2.0)\n",
    "\n",
    "print(f\"BCE loss:   {hard_bce:.6f}\")\n",
    "print(f\"Focal loss: {hard_focal:.6f}\")\n",
    "print(f\"Focal reduction: {(hard_bce - hard_focal) / hard_bce * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nüí° FOCAL LOSS INSIGHT:\")\n",
    "print(f\"   Easy examples: {easy_focal/easy_bce:.3f}x of original BCE\")\n",
    "print(f\"   Hard examples: {hard_focal/hard_bce:.3f}x of original BCE\")\n",
    "print(f\"   ‚Üí Focal loss focuses on hard examples!\")\n",
    "\n",
    "# Demonstrate hinge loss\n",
    "print(f\"\\n‚öîÔ∏è HINGE LOSS DEMONSTRATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Convert binary labels {0,1} to {-1,+1} for hinge loss\n",
    "y_hinge = np.array([-1, 1, 1, -1, 1])  # Hinge format\n",
    "raw_scores = np.array([-2, 3, 0.5, -0.5, 1.5])\n",
    "\n",
    "print(f\"Labels (hinge format): {y_hinge}\")\n",
    "print(f\"Raw scores: {raw_scores}\")\n",
    "\n",
    "hinge_losses = cls_losses.hinge_loss(y_hinge, raw_scores, reduction='none')\n",
    "hinge_grads = cls_losses.hinge_gradient(y_hinge, raw_scores)\n",
    "\n",
    "print(f\"Individual hinge losses: {hinge_losses}\")\n",
    "print(f\"Hinge gradients: {hinge_grads}\")\n",
    "print(f\"Mean hinge loss: {np.mean(hinge_losses):.4f}\")\n",
    "\n",
    "# Show margin violations\n",
    "margins = y_hinge * raw_scores\n",
    "print(f\"\\nMargin analysis (y * f(x)):\")\n",
    "for i, (y, score, margin) in enumerate(zip(y_hinge, raw_scores, margins)):\n",
    "    status = \"VIOLATES\" if margin < 1.0 else \"OK\"\n",
    "    print(f\"  Sample {i}: y={y:2d}, f(x)={score:5.1f}, margin={margin:5.1f} [{status}]\")\n",
    "\n",
    "print(f\"\\nüéØ HINGE LOSS INSIGHT:\")\n",
    "print(f\"   Only penalizes examples with margin < 1.0\")\n",
    "print(f\"   Promotes large margin separation (SVM-style)\")\n",
    "print(f\"   Sparse gradients ‚Üí many parameters don't update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c20e8",
   "metadata": {},
   "source": [
    "## 4. Advanced Loss Functions\n",
    "\n",
    "### üéØ **Specialized Loss Functions for Modern Applications**\n",
    "\n",
    "Beyond standard regression and classification, modern ML systems require specialized losses:\n",
    "\n",
    "| **Loss Function** | **Application** | **Key Innovation** | **Amazon Use Cases** |\n",
    "|-------------------|-----------------|-------------------|----------------------|\n",
    "| **Triplet Loss** | Metric Learning | Embedding spaces | Product similarity, face recognition |\n",
    "| **Contrastive Loss** | Siamese Networks | Pairwise similarity | Duplicate detection, recommendation |\n",
    "| **Wasserstein Loss** | GANs | Distribution matching | Data generation, style transfer |\n",
    "| **KL Divergence** | Probabilistic | Distribution comparison | VAEs, knowledge distillation |\n",
    "| **Label Smoothing** | Regularization | Confidence calibration | Overconfident model prevention |\n",
    "\n",
    "### üö® **Advanced Challenges:**\n",
    "1. **Hard negative mining**: Selecting informative training examples\n",
    "2. **Margin tuning**: Balancing similarity/dissimilarity\n",
    "3. **Batch composition**: Ensuring diverse training batches\n",
    "4. **Computational efficiency**: Scaling to large embedding spaces\n",
    "\n",
    "### üéØ **Interview Deep Dive:**\n",
    "- When standard losses fail and why\n",
    "- How to design custom losses for business problems\n",
    "- Numerical challenges in metric learning\n",
    "- Connection between loss choice and model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9891819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedLosses:\n",
    "    \"\"\"\n",
    "    Advanced loss functions for specialized applications.\n",
    "    \n",
    "    üéØ INTERVIEW FOCUS: Metric learning and distribution matching\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def triplet_loss(anchor: np.ndarray, positive: np.ndarray, negative: np.ndarray,\n",
    "                     margin: float = 1.0, distance_metric: str = 'l2', reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Triplet Loss for Metric Learning\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = max(0, d(a,p) - d(a,n) + margin)\n",
    "        \n",
    "        Where:\n",
    "        ‚Ä¢ d(a,p) = distance(anchor, positive)\n",
    "        ‚Ä¢ d(a,n) = distance(anchor, negative)\n",
    "        ‚Ä¢ margin = minimum separation desired\n",
    "        \n",
    "        üîç GRADIENT (L2 distance):\n",
    "        ‚àÇL/‚àÇa = 2(a-p) - 2(a-n) = 2(p-n)  [if loss > 0]\n",
    "        ‚àÇL/‚àÇp = 2(p-a)                    [if loss > 0]\n",
    "        ‚àÇL/‚àÇn = 2(n-a)                    [if loss > 0]\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Forces d(a,p) + margin < d(a,n)\n",
    "        ‚Ä¢ Creates embedding space with semantic structure\n",
    "        ‚Ä¢ Requires careful triplet mining\n",
    "        ‚Ä¢ Sensitive to margin choice\n",
    "        \n",
    "        üéØ AMAZON APPLICATIONS:\n",
    "        ‚Ä¢ Product similarity (same product, different sellers)\n",
    "        ‚Ä¢ Customer embeddings for recommendation\n",
    "        ‚Ä¢ Content-based similarity matching\n",
    "        \n",
    "        Args:\n",
    "            anchor: Reference embeddings\n",
    "            positive: Similar embeddings\n",
    "            negative: Dissimilar embeddings\n",
    "            margin: Minimum separation between pos/neg\n",
    "            distance_metric: 'l2' or 'cosine'\n",
    "        \"\"\"\n",
    "        if distance_metric == 'l2':\n",
    "            # L2 (Euclidean) distance\n",
    "            pos_dist = np.sum((anchor - positive)**2, axis=-1)\n",
    "            neg_dist = np.sum((anchor - negative)**2, axis=-1)\n",
    "        elif distance_metric == 'cosine':\n",
    "            # Cosine distance = 1 - cosine_similarity\n",
    "            def cosine_distance(x, y):\n",
    "                dot_product = np.sum(x * y, axis=-1)\n",
    "                norm_x = np.linalg.norm(x, axis=-1)\n",
    "                norm_y = np.linalg.norm(y, axis=-1)\n",
    "                cosine_sim = dot_product / (norm_x * norm_y + EPSILON)\n",
    "                return 1 - cosine_sim\n",
    "            \n",
    "            pos_dist = cosine_distance(anchor, positive)\n",
    "            neg_dist = cosine_distance(anchor, negative)\n",
    "        else:\n",
    "            raise ValueError(\"distance_metric must be 'l2' or 'cosine'\")\n",
    "        \n",
    "        # Triplet loss: max(0, d_pos - d_neg + margin)\n",
    "        loss = np.maximum(0, pos_dist - neg_dist + margin)\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def contrastive_loss(embedding1: np.ndarray, embedding2: np.ndarray, labels: np.ndarray,\n",
    "                        margin: float = 1.0, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Contrastive Loss for Siamese Networks\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        L = (1-y) * ¬Ωd¬≤ + y * ¬Ωmax(0, margin - d)¬≤\n",
    "        \n",
    "        Where:\n",
    "        ‚Ä¢ y = 1 if dissimilar pair, 0 if similar pair\n",
    "        ‚Ä¢ d = ||f(x1) - f(x2)||‚ÇÇ (L2 distance)\n",
    "        \n",
    "        üîç GRADIENT:\n",
    "        ‚àÇL/‚àÇf(x1) = {\n",
    "            (1-y) * (f(x1) - f(x2))           [similar pairs]\n",
    "            y * max(0, 1 - d/margin) * (f(x1) - f(x2))/d    [dissimilar pairs]\n",
    "        }\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Pulls similar pairs together (minimize distance)\n",
    "        ‚Ä¢ Pushes dissimilar pairs apart (up to margin)\n",
    "        ‚Ä¢ Simpler than triplet loss (only pairs needed)\n",
    "        ‚Ä¢ Good for verification tasks\n",
    "        \n",
    "        üéØ AMAZON APPLICATIONS:\n",
    "        ‚Ä¢ Duplicate product detection\n",
    "        ‚Ä¢ User account verification\n",
    "        ‚Ä¢ Content similarity matching\n",
    "        \n",
    "        Args:\n",
    "            embedding1: First set of embeddings\n",
    "            embedding2: Second set of embeddings\n",
    "            labels: 0 for similar pairs, 1 for dissimilar pairs\n",
    "            margin: Maximum distance for dissimilar pairs\n",
    "        \"\"\"\n",
    "        # Compute L2 distance\n",
    "        distances = np.linalg.norm(embedding1 - embedding2, axis=-1)\n",
    "        \n",
    "        # Similar pairs: minimize distance\n",
    "        similar_loss = (1 - labels) * 0.5 * distances**2\n",
    "        \n",
    "        # Dissimilar pairs: maximize distance up to margin\n",
    "        dissimilar_loss = labels * 0.5 * np.maximum(0, margin - distances)**2\n",
    "        \n",
    "        loss = similar_loss + dissimilar_loss\n",
    "        \n",
    "        if reduction == 'mean':\n",
    "            return np.mean(loss)\n",
    "        elif reduction == 'sum':\n",
    "            return np.sum(loss)\n",
    "        else:\n",
    "            return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_divergence(p: np.ndarray, q: np.ndarray, epsilon: float = EPSILON) -> float:\n",
    "        \"\"\"\n",
    "        Kullback-Leibler Divergence\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        KL(P||Q) = Œ£ p(x) log(p(x)/q(x))\n",
    "        \n",
    "        üîç PROPERTIES:\n",
    "        ‚Ä¢ Measures how P differs from Q\n",
    "        ‚Ä¢ Non-symmetric: KL(P||Q) ‚â† KL(Q||P)\n",
    "        ‚Ä¢ Always non-negative\n",
    "        ‚Ä¢ Zero iff P = Q\n",
    "        \n",
    "        ‚ö†Ô∏è NUMERICAL ISSUES:\n",
    "        ‚Ä¢ log(0) = -‚àû ‚Üí clip probabilities\n",
    "        ‚Ä¢ p(x) > 0 but q(x) = 0 ‚Üí infinite divergence\n",
    "        \n",
    "        üéØ APPLICATIONS:\n",
    "        ‚Ä¢ VAE loss (regularization term)\n",
    "        ‚Ä¢ Knowledge distillation (teacher-student)\n",
    "        ‚Ä¢ Distribution matching\n",
    "        ‚Ä¢ Information theory measures\n",
    "        \n",
    "        Args:\n",
    "            p: True distribution (probabilities)\n",
    "            q: Approximate distribution (probabilities)\n",
    "        \"\"\"\n",
    "        # Clip to prevent log(0)\n",
    "        p_clipped = np.clip(p, epsilon, 1)\n",
    "        q_clipped = np.clip(q, epsilon, 1)\n",
    "        \n",
    "        # KL divergence formula\n",
    "        return np.sum(p_clipped * np.log(p_clipped / q_clipped))\n",
    "    \n",
    "    @staticmethod\n",
    "    def jensen_shannon_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Jensen-Shannon Divergence (Symmetric version of KL)\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        JS(P||Q) = ¬ΩKL(P||M) + ¬ΩKL(Q||M)\n",
    "        Where M = ¬Ω(P + Q)\n",
    "        \n",
    "        ‚ö†Ô∏è PROPERTIES:\n",
    "        ‚Ä¢ Symmetric: JS(P||Q) = JS(Q||P)\n",
    "        ‚Ä¢ Bounded: 0 ‚â§ JS ‚â§ log(2)\n",
    "        ‚Ä¢ Smooth and well-behaved\n",
    "        \"\"\"\n",
    "        # Compute mixture distribution\n",
    "        m = 0.5 * (p + q)\n",
    "        \n",
    "        # Symmetric KL divergence\n",
    "        return 0.5 * AdvancedLosses.kl_divergence(p, m) + 0.5 * AdvancedLosses.kl_divergence(q, m)\n",
    "    \n",
    "    @staticmethod\n",
    "    def label_smoothing_crossentropy(y_true: np.ndarray, logits: np.ndarray, \n",
    "                                   smoothing: float = 0.1, reduction: str = 'mean') -> float:\n",
    "        \"\"\"\n",
    "        Label Smoothing Cross-Entropy\n",
    "        \n",
    "        üìö MATHEMATICAL FOUNDATION:\n",
    "        Instead of hard labels [0, 0, 1, 0], use soft labels:\n",
    "        y_smooth = (1 - Œµ) * y_true + Œµ / K\n",
    "        \n",
    "        Where:\n",
    "        ‚Ä¢ Œµ = smoothing parameter\n",
    "        ‚Ä¢ K = number of classes\n",
    "        \n",
    "        üîç BENEFITS:\n",
    "        ‚Ä¢ Prevents overconfident predictions\n",
    "        ‚Ä¢ Improves model calibration\n",
    "        ‚Ä¢ Acts as regularization\n",
    "        ‚Ä¢ Better generalization\n",
    "        \n",
    "        üéØ WHEN TO USE:\n",
    "        ‚Ä¢ Large models prone to overfitting\n",
    "        ‚Ä¢ When prediction confidence matters\n",
    "        ‚Ä¢ Classification with many classes\n",
    "        \n",
    "        Args:\n",
    "            smoothing: Amount of label smoothing (typically 0.1)\n",
    "        \"\"\"\n",
    "        num_classes = logits.shape[-1]\n",
    "        \n",
    "        # Create smoothed labels\n",
    "        smoothed_labels = y_true * (1 - smoothing) + smoothing / num_classes\n",
    "        \n",
    "        # Use standard categorical CE with smoothed labels\n",
    "        return ClassificationLosses.categorical_crossentropy_with_logits(\n",
    "            smoothed_labels, logits, reduction=reduction)\n",
    "\n",
    "# Test advanced losses\n",
    "print(\"üî¨ TESTING ADVANCED LOSS FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "adv_losses = AdvancedLosses()\n",
    "\n",
    "# Generate test embeddings for triplet loss\n",
    "np.random.seed(42)\n",
    "embedding_dim = 128\n",
    "anchor = np.random.randn(5, embedding_dim)\n",
    "positive = anchor + 0.1 * np.random.randn(5, embedding_dim)  # Similar to anchor\n",
    "negative = np.random.randn(5, embedding_dim)  # Random (likely dissimilar)\n",
    "\n",
    "# Test triplet loss\n",
    "triplet_l2 = adv_losses.triplet_loss(anchor, positive, negative, margin=1.0, distance_metric='l2')\n",
    "triplet_cosine = adv_losses.triplet_loss(anchor, positive, negative, margin=0.2, distance_metric='cosine')\n",
    "\n",
    "print(\"TRIPLET LOSS TEST:\")\n",
    "print(f\"L2 distance triplet loss:     {triplet_l2:.6f}\")\n",
    "print(f\"Cosine distance triplet loss: {triplet_cosine:.6f}\")\n",
    "\n",
    "# Compute individual distances for analysis\n",
    "pos_dists_l2 = np.sum((anchor - positive)**2, axis=1)\n",
    "neg_dists_l2 = np.sum((anchor - negative)**2, axis=1)\n",
    "\n",
    "print(f\"\\nDistance analysis (L2):\")\n",
    "print(f\"Positive distances (should be small): {pos_dists_l2}\")\n",
    "print(f\"Negative distances (should be large): {neg_dists_l2}\")\n",
    "print(f\"Margins (neg - pos): {neg_dists_l2 - pos_dists_l2}\")\n",
    "\n",
    "# Test contrastive loss\n",
    "labels_contrastive = np.array([0, 0, 1, 1, 0])  # 0=similar, 1=dissimilar\n",
    "contrastive_loss_val = adv_losses.contrastive_loss(\n",
    "    anchor, positive, labels_contrastive, margin=2.0)\n",
    "\n",
    "print(f\"\\nCONTRASTIVE LOSS TEST:\")\n",
    "print(f\"Contrastive loss: {contrastive_loss_val:.6f}\")\n",
    "print(f\"Labels (0=similar, 1=dissimilar): {labels_contrastive}\")\n",
    "\n",
    "# Test KL divergence\n",
    "p_dist = np.array([0.7, 0.2, 0.1])  # True distribution\n",
    "q_dist = np.array([0.6, 0.3, 0.1])  # Approximate distribution\n",
    "\n",
    "kl_pq = adv_losses.kl_divergence(p_dist, q_dist)\n",
    "kl_qp = adv_losses.kl_divergence(q_dist, p_dist)\n",
    "js_div = adv_losses.jensen_shannon_divergence(p_dist, q_dist)\n",
    "\n",
    "print(f\"\\nDIVERGENCE TEST:\")\n",
    "print(f\"P distribution: {p_dist}\")\n",
    "print(f\"Q distribution: {q_dist}\")\n",
    "print(f\"KL(P||Q): {kl_pq:.6f}\")\n",
    "print(f\"KL(Q||P): {kl_qp:.6f}\")\n",
    "print(f\"JS(P||Q): {js_div:.6f} (symmetric)\")\n",
    "\n",
    "# Test label smoothing\n",
    "y_one_hot = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "test_logits = np.random.randn(3, 3)\n",
    "\n",
    "normal_ce = ClassificationLosses.categorical_crossentropy_with_logits(y_one_hot, test_logits)\n",
    "smoothed_ce = adv_losses.label_smoothing_crossentropy(y_one_hot, test_logits, smoothing=0.1)\n",
    "\n",
    "print(f\"\\nLABEL SMOOTHING TEST:\")\n",
    "print(f\"Normal CE loss:     {normal_ce:.6f}\")\n",
    "print(f\"Label smoothed CE:  {smoothed_ce:.6f}\")\n",
    "print(f\"Smoothing effect:   {(smoothed_ce - normal_ce):.6f}\")\n",
    "\n",
    "print(f\"\\nüéØ ADVANCED LOSS INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Triplet loss learns embedding spaces with semantic structure\")\n",
    "print(f\"‚Ä¢ Contrastive loss is simpler but requires good pair mining\")\n",
    "print(f\"‚Ä¢ KL divergence is fundamental to many modern architectures\")\n",
    "print(f\"‚Ä¢ Label smoothing prevents overconfident predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Loss Function Visualization and Comparison\n",
    "def create_comprehensive_loss_visualization():\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations comparing all loss functions\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    \n",
    "    # 1. Regression Loss Comparison\n",
    "    ax = axes[0, 0]\n",
    "    errors = np.linspace(-3, 3, 1000)\n",
    "    \n",
    "    reg_losses = RegressionLosses()\n",
    "    mse_vals = [reg_losses.mse_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in errors]\n",
    "    mae_vals = [reg_losses.mae_loss(np.array([0]), np.array([e]), reduction='none')[0] for e in errors]\n",
    "    huber_vals = [reg_losses.huber_loss(np.array([0]), np.array([e]), delta=1.0, reduction='none')[0] for e in errors]\n",
    "    \n",
    "    ax.plot(errors, mse_vals, 'b-', linewidth=2, label='MSE (L2)')\n",
    "    ax.plot(errors, mae_vals, 'r-', linewidth=2, label='MAE (L1)')\n",
    "    ax.plot(errors, huber_vals, 'g-', linewidth=2, label='Huber')\n",
    "    ax.set_xlabel('Error (≈∑ - y)')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Regression Losses', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Classification Loss Comparison\n",
    "    ax = axes[0, 1]\n",
    "    probs = np.linspace(0.001, 0.999, 1000)\n",
    "    \n",
    "    # Binary CE for positive class (y=1)\n",
    "    bce_pos = -np.log(probs)\n",
    "    bce_neg = -np.log(1 - probs)\n",
    "    \n",
    "    ax.plot(probs, bce_pos, 'b-', linewidth=2, label='BCE (y=1)')\n",
    "    ax.plot(probs, bce_neg, 'r-', linewidth=2, label='BCE (y=0)')\n",
    "    ax.set_xlabel('Predicted Probability')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Binary Cross-Entropy', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # 3. Focal Loss vs BCE\n",
    "    ax = axes[0, 2]\n",
    "    # Focal loss for different gamma values\n",
    "    for gamma in [0, 1, 2, 5]:\n",
    "        focal_vals = -(1 - probs)**gamma * np.log(probs)\n",
    "        label = f'Focal (Œ≥={gamma})' if gamma > 0 else 'BCE (Œ≥=0)'\n",
    "        ax.plot(probs, focal_vals, linewidth=2, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Probability (y=1)')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Focal Loss Effect', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # 4. Hinge Loss\n",
    "    ax = axes[1, 0]\n",
    "    scores = np.linspace(-3, 3, 1000)\n",
    "    hinge_pos = np.maximum(0, 1 - scores)  # y = +1\n",
    "    hinge_neg = np.maximum(0, 1 + scores)  # y = -1\n",
    "    \n",
    "    ax.plot(scores, hinge_pos, 'b-', linewidth=2, label='Hinge (y=+1)')\n",
    "    ax.plot(scores, hinge_neg, 'r-', linewidth=2, label='Hinge (y=-1)')\n",
    "    ax.axvline(x=1, color='b', linestyle='--', alpha=0.7, label='Margin (y=+1)')\n",
    "    ax.axvline(x=-1, color='r', linestyle='--', alpha=0.7, label='Margin (y=-1)')\n",
    "    ax.set_xlabel('Raw Score f(x)')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Hinge Loss (SVM)', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Quantile Loss\n",
    "    ax = axes[1, 1]\n",
    "    for tau in [0.1, 0.5, 0.9]:\n",
    "        quantile_vals = []\n",
    "        for e in errors:\n",
    "            if e >= 0:  # Under-prediction\n",
    "                loss = tau * e\n",
    "            else:  # Over-prediction\n",
    "                loss = (tau - 1) * e\n",
    "            quantile_vals.append(loss)\n",
    "        ax.plot(errors, quantile_vals, linewidth=2, label=f'œÑ={tau}')\n",
    "    \n",
    "    ax.set_xlabel('Error (y - ≈∑)')\n",
    "    ax.set_ylabel('Loss Value')\n",
    "    ax.set_title('Quantile Loss', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Triplet Loss Illustration\n",
    "    ax = axes[1, 2]\n",
    "    # Simulate triplet loss landscape\n",
    "    distances = np.linspace(0, 3, 100)\n",
    "    margin = 1.0\n",
    "    \n",
    "    # For different d(a,n) values\n",
    "    for d_an in [0.5, 1.0, 1.5, 2.0]:\n",
    "        triplet_vals = np.maximum(0, distances - d_an + margin)\n",
    "        ax.plot(distances, triplet_vals, linewidth=2, label=f'd(a,n)={d_an}')\n",
    "    \n",
    "    ax.set_xlabel('d(a,p) - Distance to Positive')\n",
    "    ax.set_ylabel('Triplet Loss')\n",
    "    ax.set_title('Triplet Loss Landscape', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. KL Divergence\n",
    "    ax = axes[2, 0]\n",
    "    p_true = 0.7  # True probability\n",
    "    q_range = np.linspace(0.001, 0.999, 1000)\n",
    "    \n",
    "    kl_vals = p_true * np.log(p_true / q_range) + (1 - p_true) * np.log((1 - p_true) / (1 - q_range))\n",
    "    \n",
    "    ax.plot(q_range, kl_vals, 'purple', linewidth=2)\n",
    "    ax.axvline(x=p_true, color='red', linestyle='--', linewidth=2, label=f'True p={p_true}')\n",
    "    ax.set_xlabel('Predicted Probability q')\n",
    "    ax.set_ylabel('KL(p||q)')\n",
    "    ax.set_title('KL Divergence', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    # 8. Gradient Comparison\n",
    "    ax = axes[2, 1]\n",
    "    # Compare gradients of different loss functions\n",
    "    mse_grads = errors  # MSE gradient is linear\n",
    "    mae_grads = np.sign(errors)  # MAE gradient is constant\n",
    "    huber_grads = np.where(np.abs(errors) <= 1, errors, np.sign(errors))  # Huber gradient\n",
    "    \n",
    "    ax.plot(errors, mse_grads, 'b-', linewidth=2, label='MSE Gradient')\n",
    "    ax.plot(errors, mae_grads, 'r-', linewidth=2, label='MAE Gradient')\n",
    "    ax.plot(errors, huber_grads, 'g-', linewidth=2, label='Huber Gradient')\n",
    "    ax.set_xlabel('Error (≈∑ - y)')\n",
    "    ax.set_ylabel('Gradient Value')\n",
    "    ax.set_title('Loss Function Gradients', fontweight='bold', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Business Impact Summary\n",
    "    ax = axes[2, 2]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    business_text = '''\n",
    "    BUSINESS IMPACT OF LOSS CHOICE\n",
    "    \n",
    "    üìä Regression:\n",
    "    ‚Ä¢ MSE: Minimizes variance (Gaussian noise)\n",
    "    ‚Ä¢ MAE: Minimizes median error (robust)\n",
    "    ‚Ä¢ Huber: Balanced approach\n",
    "    ‚Ä¢ Quantile: Risk assessment, SLA targets\n",
    "    \n",
    "    üéØ Classification:\n",
    "    ‚Ä¢ BCE: Standard binary classification\n",
    "    ‚Ä¢ Focal: Addresses class imbalance\n",
    "    ‚Ä¢ Hinge: Maximum margin separation\n",
    "    \n",
    "    üî¨ Advanced:\n",
    "    ‚Ä¢ Triplet: Learning similarity metrics\n",
    "    ‚Ä¢ KL: Distribution matching, VAEs\n",
    "    ‚Ä¢ Label Smoothing: Calibration\n",
    "    \n",
    "    ‚ö†Ô∏è KEY INSIGHT:\n",
    "    Loss function choice directly impacts:\n",
    "    ‚Ä¢ Model behavior and robustness\n",
    "    ‚Ä¢ Training dynamics and convergence\n",
    "    ‚Ä¢ Business metrics alignment\n",
    "    ‚Ä¢ Prediction confidence and calibration\n",
    "    '''\n",
    "    \n",
    "    ax.text(0.05, 0.95, business_text, transform=ax.transAxes, fontsize=11,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Complete Loss Function Reference', fontsize=16, fontweight='bold', y=0.98)\n",
    "    plt.show()\n",
    "\n",
    "# Create comprehensive visualization\n",
    "create_comprehensive_loss_visualization()\n",
    "\n",
    "# Final performance comparison\n",
    "def benchmark_loss_computations():\n",
    "    \"\"\"Benchmark computational performance of different losses\"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(\"‚ö° COMPUTATIONAL PERFORMANCE BENCHMARK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Generate test data\n",
    "    n_samples = 10000\n",
    "    n_classes = 100\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    y_true_reg = np.random.randn(n_samples)\n",
    "    y_pred_reg = np.random.randn(n_samples)\n",
    "    \n",
    "    y_true_clf = np.random.randint(0, 2, n_samples)\n",
    "    logits_clf = np.random.randn(n_samples)\n",
    "    \n",
    "    y_true_multi = np.eye(n_classes)[np.random.randint(0, n_classes, n_samples)]\n",
    "    logits_multi = np.random.randn(n_samples, n_classes)\n",
    "    \n",
    "    # Benchmark regression losses\n",
    "    reg_losses = RegressionLosses()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        reg_losses.mse_loss(y_true_reg, y_pred_reg)\n",
    "    mse_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        reg_losses.mae_loss(y_true_reg, y_pred_reg)\n",
    "    mae_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        reg_losses.huber_loss(y_true_reg, y_pred_reg)\n",
    "    huber_time = time.time() - start_time\n",
    "    \n",
    "    # Benchmark classification losses\n",
    "    cls_losses = ClassificationLosses()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        cls_losses.binary_crossentropy_with_logits(y_true_clf, logits_clf)\n",
    "    bce_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for _ in range(100):\n",
    "        cls_losses.categorical_crossentropy_with_logits(y_true_multi, logits_multi)\n",
    "    ce_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"REGRESSION LOSSES (100 iterations, {n_samples} samples):\")\n",
    "    print(f\"  MSE:   {mse_time:.4f}s\")\n",
    "    print(f\"  MAE:   {mae_time:.4f}s\")\n",
    "    print(f\"  Huber: {huber_time:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nCLASSIFICATION LOSSES (100 iterations):\")\n",
    "    print(f\"  Binary CE:     {bce_time:.4f}s\")\n",
    "    print(f\"  Categorical CE: {ce_time:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nüí° PERFORMANCE INSIGHTS:\")\n",
    "    print(f\"‚Ä¢ MSE is fastest (simple arithmetic)\")\n",
    "    print(f\"‚Ä¢ MAE has conditional logic overhead\")\n",
    "    print(f\"‚Ä¢ Huber combines both ‚Üí moderate speed\")\n",
    "    print(f\"‚Ä¢ Logit-based losses avoid sigmoid computation\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_loss_computations()\n",
    "\n",
    "print(\"\\nüéì LOSS FUNCTION MASTERY COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Mathematical foundations understood\")\n",
    "print(\"‚úÖ Custom implementations working\")\n",
    "print(\"‚úÖ Numerical stability handled\")\n",
    "print(\"‚úÖ Gradient derivations covered\")\n",
    "print(\"‚úÖ Business applications clear\")\n",
    "print(\"‚úÖ Performance characteristics known\")\n",
    "print(\"\\nüöÄ You're ready for Amazon Applied Scientist interviews!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489168cb",
   "metadata": {},
   "source": [
    "## 5. Amazon Interview Preparation Summary\n",
    "\n",
    "### üéØ **Quick Reference for Interviews**\n",
    "\n",
    "#### **Essential Loss Function Facts:**\n",
    "\n",
    "| **Loss** | **Use Case** | **Key Property** | **Gradient** | **When It Fails** |\n",
    "|----------|--------------|------------------|--------------|-------------------|\n",
    "| **MSE** | Regression | Quadratic penalty | Linear growth | Outliers present |\n",
    "| **MAE** | Robust regression | Linear penalty | Constant | Need smooth gradients |\n",
    "| **Huber** | Balanced regression | Hybrid L1/L2 | Clipped linear | Need pure L1/L2 |\n",
    "| **BCE** | Binary classification | Probabilistic | p - y | Class imbalance |\n",
    "| **Focal** | Imbalanced classification | Down-weights easy | Complex | Balanced datasets |\n",
    "| **Hinge** | SVM-style | Large margin | Sparse | Need probabilities |\n",
    "| **Triplet** | Metric learning | Embedding structure | Distance-based | Poor triplet mining |\n",
    "\n",
    "### üö® **Critical Interview Topics**\n",
    "\n",
    "#### **1. Mathematical Derivations** (Must Know)\n",
    "```python\n",
    "# Be ready to derive these on whiteboard:\n",
    "\n",
    "# MSE gradient:\n",
    "# L = ¬Ω(y - ≈∑)¬≤\n",
    "# ‚àÇL/‚àÇ≈∑ = -(y - ≈∑) = (≈∑ - y)\n",
    "\n",
    "# BCE gradient (with sigmoid):\n",
    "# L = -[y log œÉ(z) + (1-y) log(1-œÉ(z))]\n",
    "# ‚àÇL/‚àÇz = œÉ(z) - y\n",
    "\n",
    "# Cross-entropy gradient (with softmax):\n",
    "# L = -Œ£ y_i log(softmax_i(z))\n",
    "# ‚àÇL/‚àÇz_i = softmax_i(z) - y_i\n",
    "```\n",
    "\n",
    "#### **2. Numerical Stability** (Critical)\n",
    "```python\n",
    "# Always mention these in interviews:\n",
    "\n",
    "# 1. Sigmoid overflow prevention\n",
    "def safe_sigmoid(z):\n",
    "    return np.where(z >= 0, 1/(1 + np.exp(-z)), np.exp(z)/(1 + np.exp(z)))\n",
    "\n",
    "# 2. Log-sum-exp trick\n",
    "def stable_softmax(logits):\n",
    "    shifted = logits - np.max(logits, axis=-1, keepdims=True)\n",
    "    return np.exp(shifted) / np.sum(np.exp(shifted), axis=-1, keepdims=True)\n",
    "\n",
    "# 3. Probability clipping\n",
    "def safe_log(p, epsilon=1e-15):\n",
    "    return np.log(np.clip(p, epsilon, 1 - epsilon))\n",
    "```\n",
    "\n",
    "#### **3. Business Alignment** (Amazon Specific)\n",
    "- **Customer obsession**: How does loss choice impact user experience?\n",
    "- **Ownership**: Why did you choose this loss over alternatives?\n",
    "- **Dive deep**: What are the mathematical trade-offs?\n",
    "- **Bias for action**: Quick prototyping vs. optimal solution\n",
    "\n",
    "### üéØ **Sample Interview Questions & Answers**\n",
    "\n",
    "#### **Q: \"Why would you use Huber loss instead of MSE?\"**\n",
    "**A:** \"Huber loss provides the best of both worlds. For small errors (‚â§Œ¥), it behaves like MSE with smooth gradients for stable optimization. For large errors (>Œ¥), it behaves like MAE with linear growth, making it robust to outliers. This is crucial in production systems where data quality isn't guaranteed. I'd choose Œ¥ based on the expected noise level in the data.\"\n",
    "\n",
    "#### **Q: \"Implement focal loss and explain when to use it.\"**\n",
    "**A:** \"Focal loss addresses the class imbalance problem that's common in real-world applications like fraud detection or medical diagnosis. The key insight is down-weighting easy examples with the term (1-p_t)^Œ≥. When Œ≥=0, it reduces to standard BCE. As Œ≥ increases, the model focuses more on hard examples. I typically start with Œ≥=2 and Œ±=0.25, then tune based on validation metrics that matter for the business problem.\"\n",
    "\n",
    "#### **Q: \"How do you ensure numerical stability in loss computations?\"**\n",
    "**A:** \"Three main strategies: 1) Work in log-space when possible (e.g., log-softmax instead of softmax), 2) Use the log-sum-exp trick to prevent overflow, 3) Clip probabilities to [Œµ, 1-Œµ] to prevent log(0). For sigmoid, I use the stable implementation that separates positive and negative logits. These aren't just theoretical concerns - in production with extreme inputs, naive implementations will produce NaN gradients and crash training.\"\n",
    "\n",
    "### üîß **Debugging Loss Functions**\n",
    "\n",
    "#### **Common Issues & Solutions:**\n",
    "1. **NaN losses**: Check for log(0), clip probabilities\n",
    "2. **Exploding gradients**: Verify numerical stability, check learning rate\n",
    "3. **Vanishing gradients**: Consider focal loss, check activation saturation\n",
    "4. **Slow convergence**: Examine loss landscape, try different formulations\n",
    "5. **Poor calibration**: Consider label smoothing or temperature scaling\n",
    "\n",
    "### üöÄ **Advanced Topics for Senior Roles**\n",
    "\n",
    "1. **Custom loss design**: How to create domain-specific losses\n",
    "2. **Loss landscape analysis**: Understanding optimization dynamics\n",
    "3. **Multi-task learning**: Balancing multiple loss components\n",
    "4. **Adversarial robustness**: Losses that promote model stability\n",
    "5. **Fairness constraints**: Incorporating bias mitigation into loss functions\n",
    "\n",
    "### üìö **Key Takeaways for Amazon**\n",
    "\n",
    "1. **Customer impact**: Always connect loss choice to business metrics\n",
    "2. **Scalability**: Consider computational cost at Amazon scale\n",
    "3. **Robustness**: Production systems need stable, predictable behavior\n",
    "4. **Interpretability**: Stakeholders need to understand model decisions\n",
    "5. **Continuous improvement**: Loss functions evolve with business needs\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Final Interview Tip**: Practice explaining loss functions in simple terms to non-technical stakeholders. Amazon values leaders who can communicate complex concepts clearly and connect technical decisions to customer outcomes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
