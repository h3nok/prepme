{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60043577",
   "metadata": {},
   "source": [
    "# Linear Regression Interview Preparation\n",
    "\n",
    "## Comprehensive Implementation and Evaluation Guide\n",
    "\n",
    "This notebook demonstrates a complete understanding of linear regression concepts for technical interviews, including:\n",
    "\n",
    "- **Mathematical Foundation**: Linear model Y = Xw + b and MSE loss function\n",
    "- **Implementation from Scratch**: Both gradient descent and analytical solutions\n",
    "- **Optimization Techniques**: Parameter updates and convergence criteria\n",
    "- **Model Evaluation**: MSE, R¬≤, and visualization techniques\n",
    "- **Industry Standards**: Comparison with scikit-learn implementation\n",
    "\n",
    "### Key Interview Topics Covered:\n",
    "1. ‚úÖ Linear regression model formulation\n",
    "2. ‚úÖ Mean Squared Error (MSE) derivation and calculation\n",
    "3. ‚úÖ Gradient descent optimization algorithm\n",
    "4. ‚úÖ Normal equation analytical solution\n",
    "5. ‚úÖ Model evaluation metrics and interpretation\n",
    "6. ‚úÖ Implementation best practices and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ed72a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our comprehensive linear regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Ready for linear regression implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384913f6",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundation\n",
    "\n",
    "### Linear Regression Model\n",
    "The linear regression model is defined as:\n",
    "\n",
    "**Y = Xw + b**\n",
    "\n",
    "Where:\n",
    "- **Y**: Target values (n_samples,)\n",
    "- **X**: Feature matrix (n_samples, n_features)  \n",
    "- **w**: Weight vector (n_features,)\n",
    "- **b**: Bias term (scalar)\n",
    "\n",
    "### Mean Squared Error (MSE) Loss Function\n",
    "The cost function we want to minimize:\n",
    "\n",
    "**MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤**\n",
    "\n",
    "Where:\n",
    "- **n**: Number of samples\n",
    "- **y·µ¢**: True target value for sample i\n",
    "- **≈∑·µ¢**: Predicted value for sample i\n",
    "\n",
    "### Gradient Descent Updates\n",
    "To minimize MSE, we compute gradients and update parameters:\n",
    "\n",
    "**‚àÇMSE/‚àÇw = -(2/n) √ó X·µÄ √ó (y - ≈∑)**  \n",
    "**‚àÇMSE/‚àÇb = -(2/n) √ó Œ£(y - ≈∑)**\n",
    "\n",
    "**Parameter Updates:**  \n",
    "**w = w - Œ± √ó ‚àÇMSE/‚àÇw**  \n",
    "**b = b - Œ± √ó ‚àÇMSE/‚àÇb**\n",
    "\n",
    "Where **Œ±** is the learning rate.\n",
    "\n",
    "### Normal Equation (Analytical Solution)\n",
    "For the optimal solution directly:\n",
    "\n",
    "**Œ∏ = (X·µÄX)‚Åª¬πX·µÄy**\n",
    "\n",
    "Where **Œ∏ = [b, w‚ÇÅ, w‚ÇÇ, ...]** contains bias and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e700c0f",
   "metadata": {},
   "source": [
    "## 3. LinearRegression Class Implementation\n",
    "\n",
    "Let's implement our linear regression class from scratch with both gradient descent and analytical solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f165dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation with gradient descent and analytical solutions.\n",
    "    \n",
    "    Model: Y = Xw + b\n",
    "    Loss: MSE = (1/n) * Œ£(yi - ≈∑i)¬≤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000, tolerance: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Step size for gradient descent\n",
    "            max_iterations: Maximum number of training iterations\n",
    "            tolerance: Convergence threshold for early stopping\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using the linear model: Y = Xw + b\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted values of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def mean_squared_error(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Squared Error: MSE = (1/n) * Œ£(yi - ≈∑i)¬≤\n",
    "        \n",
    "        Args:\n",
    "            y_true: True target values\n",
    "            y_pred: Predicted values\n",
    "            \n",
    "        Returns:\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def r_squared(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate R¬≤ coefficient of determination\n",
    "        \n",
    "        R¬≤ = 1 - (SS_res / SS_tot)\n",
    "        where SS_res = Œ£(yi - ≈∑i)¬≤ and SS_tot = Œ£(yi - »≥)¬≤\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"LinearRegression class defined successfully!\")\n",
    "print(\"Key methods: fit_gradient_descent, fit_analytical, predict, mean_squared_error, r_squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36f543",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Training Method\n",
    "\n",
    "Now let's implement the gradient descent algorithm to optimize our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_descent(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent optimization.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "        \n",
    "    Returns:\n",
    "        self: Fitted model\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    self.weights = np.random.normal(0, 0.01, n_features)\n",
    "    self.bias = 0\n",
    "    self.cost_history = []\n",
    "    \n",
    "    print(f\"Starting gradient descent with {n_samples} samples, {n_features} features\")\n",
    "    print(f\"Learning rate: {self.learning_rate}, Max iterations: {self.max_iterations}\")\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(self.max_iterations):\n",
    "        # Forward pass: Y = Xw + b\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Calculate MSE: (1/n) * Œ£(yi - ≈∑i)¬≤\n",
    "        mse = self.mean_squared_error(y, y_pred)\n",
    "        self.cost_history.append(mse)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        # ‚àÇMSE/‚àÇw = -(2/n) * X^T * (y - ≈∑)\n",
    "        dw = (-2/n_samples) * X.T.dot(y - y_pred)\n",
    "        # ‚àÇMSE/‚àÇb = -(2/n) * Œ£(y - ≈∑)\n",
    "        db = (-2/n_samples) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "        \n",
    "        # Check for convergence\n",
    "        if i > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < self.tolerance:\n",
    "            print(f\"‚úÖ Converged after {i+1} iterations\")\n",
    "            break\n",
    "            \n",
    "        # Print progress every 100 iterations\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1}: MSE = {mse:.6f}\")\n",
    "            \n",
    "    return self\n",
    "\n",
    "# Add the method to our LinearRegression class\n",
    "LinearRegression.fit_gradient_descent = fit_gradient_descent\n",
    "print(\"‚úÖ Gradient descent method added to LinearRegression class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f74e1a",
   "metadata": {},
   "source": [
    "## 5. Analytical Solution Method (Normal Equation)\n",
    "\n",
    "The analytical solution provides the optimal weights directly without iterative optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_analytical(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "    \"\"\"\n",
    "    Fit the model using the analytical solution (Normal Equation).\n",
    "    \n",
    "    For Y = Xw + b, we solve: Œ∏ = (X^T X)^(-1) X^T y\n",
    "    where Œ∏ = [b, w1, w2, ...] contains bias and weights\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "        \n",
    "    Returns:\n",
    "        self: Fitted model\n",
    "    \"\"\"\n",
    "    print(\"Computing analytical solution using Normal Equation...\")\n",
    "    \n",
    "    # Add bias column to X (column of ones)\n",
    "    X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    # Normal equation: Œ∏ = (X^T X)^(-1) X^T y\n",
    "    # Using pseudoinverse for numerical stability\n",
    "    try:\n",
    "        theta = np.linalg.pinv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
    "        print(\"‚úÖ Analytical solution computed successfully!\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"‚ö†Ô∏è Matrix inversion failed, using pseudoinverse\")\n",
    "        theta = np.linalg.pinv(X_with_bias).dot(y)\n",
    "    \n",
    "    # Extract bias and weights\n",
    "    self.bias = theta[0]\n",
    "    self.weights = theta[1:]\n",
    "    \n",
    "    print(f\"Optimal bias: {self.bias:.4f}\")\n",
    "    print(f\"Optimal weights: {self.weights}\")\n",
    "    \n",
    "    return self\n",
    "\n",
    "# Add the method to our LinearRegression class\n",
    "LinearRegression.fit_analytical = fit_analytical\n",
    "print(\"‚úÖ Analytical solution method added to LinearRegression class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7b032",
   "metadata": {},
   "source": [
    "## 6. Data Generation and Preprocessing\n",
    "\n",
    "Let's generate synthetic data with known parameters to test our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627df7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_samples: int = 200, n_features: int = 2, noise_std: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sample data for regression demonstration with known parameters.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of data points to generate\n",
    "        n_features: Number of input features\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "        \n",
    "    Returns:\n",
    "        X: Feature matrix, y: Target vector\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Generate random features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Define true parameters (these are what we want our model to learn)\n",
    "    true_weights = np.array([3.5, -2.1])  # True weights\n",
    "    true_bias = 1.2                       # True bias\n",
    "    \n",
    "    # Generate targets: Y = Xw + b + noise\n",
    "    y = X.dot(true_weights) + true_bias + np.random.normal(0, noise_std, n_samples)\n",
    "    \n",
    "    return X, y, true_weights, true_bias\n",
    "\n",
    "# Generate our dataset\n",
    "print(\"üéØ Generating synthetic dataset...\")\n",
    "X, y, true_weights, true_bias = generate_sample_data(n_samples=200, noise_std=0.5)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias:.4f}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Standardize features (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"‚úÖ Data generated and preprocessed successfully!\")\n",
    "print(f\"Feature means after scaling: {np.mean(X_train_scaled, axis=0)}\")\n",
    "print(f\"Feature stds after scaling: {np.std(X_train_scaled, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b1d46",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation\n",
    "\n",
    "Now let's train our models using both methods and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train using Gradient Descent\n",
    "print(\"=\" * 50)\n",
    "print(\"üöÄ TRAINING WITH GRADIENT DESCENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_gd = LinearRegression(learning_rate=0.01, max_iterations=1000, tolerance=1e-6)\n",
    "model_gd.fit_gradient_descent(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gd = model_gd.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_gd = model_gd.mean_squared_error(y_test, y_pred_gd)\n",
    "r2_gd = model_gd.r_squared(y_test, y_pred_gd)\n",
    "\n",
    "print(f\"\\nüìä GRADIENT DESCENT RESULTS:\")\n",
    "print(f\"Learned weights: {model_gd.weights}\")\n",
    "print(f\"Learned bias: {model_gd.bias:.4f}\")\n",
    "print(f\"Test MSE: {mse_gd:.4f}\")\n",
    "print(f\"Test R¬≤: {r2_gd:.4f}\")\n",
    "print(f\"Training iterations: {len(model_gd.cost_history)}\")\n",
    "print(f\"Final cost: {model_gd.cost_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train using Analytical Solution\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéØ TRAINING WITH ANALYTICAL SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_analytical = LinearRegression()\n",
    "model_analytical.fit_analytical(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_analytical = model_analytical.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_analytical = model_analytical.mean_squared_error(y_test, y_pred_analytical)\n",
    "r2_analytical = model_analytical.r_squared(y_test, y_pred_analytical)\n",
    "\n",
    "print(f\"\\nüìä ANALYTICAL SOLUTION RESULTS:\")\n",
    "print(f\"Optimal weights: {model_analytical.weights}\")\n",
    "print(f\"Optimal bias: {model_analytical.bias:.4f}\")\n",
    "print(f\"Test MSE: {mse_analytical:.4f}\")\n",
    "print(f\"Test R¬≤: {r2_analytical:.4f}\")\n",
    "\n",
    "# Compare the two methods\n",
    "print(f\"\\nüîç COMPARISON:\")\n",
    "print(f\"Weight difference: {np.abs(model_gd.weights - model_analytical.weights)}\")\n",
    "print(f\"Bias difference: {abs(model_gd.bias - model_analytical.bias):.6f}\")\n",
    "print(f\"MSE difference: {abs(mse_gd - mse_analytical):.6f}\")\n",
    "\n",
    "# Check how close we are to true parameters\n",
    "print(f\"\\nüéØ COMPARISON WITH TRUE PARAMETERS:\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "print(f\"Note: Scaling affects the learned parameters, but predictions should be accurate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d35fa3",
   "metadata": {},
   "source": [
    "## 8. Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Cost function convergence (Gradient Descent)\n",
    "axes[0, 0].plot(model_gd.cost_history, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Cost Function Convergence\\n(Gradient Descent)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('MSE')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')  # Log scale to see convergence better\n",
    "\n",
    "# Plot 2: Predictions vs Actual Values\n",
    "axes[0, 1].scatter(y_test, y_pred_gd, alpha=0.6, color='blue', label='Gradient Descent')\n",
    "axes[0, 1].scatter(y_test, y_pred_analytical, alpha=0.6, color='red', label='Analytical')\n",
    "# Perfect prediction line\n",
    "min_val, max_val = y_test.min(), y_test.max()\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Values')\n",
    "axes[0, 1].set_ylabel('Predicted Values')\n",
    "axes[0, 1].set_title('Predictions vs Actual Values', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals (Gradient Descent)\n",
    "residuals_gd = y_test - y_pred_gd\n",
    "axes[1, 0].scatter(y_pred_gd, residuals_gd, alpha=0.6, color='blue')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residual Plot\\n(Gradient Descent)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning curve comparison\n",
    "axes[1, 1].bar(['Gradient Descent', 'Analytical', 'True MSE'], \n",
    "               [mse_gd, mse_analytical, np.var(np.random.normal(0, 0.5, 1000))], \n",
    "               color=['blue', 'red', 'green'], alpha=0.7)\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title('Method Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"üìà VISUALIZATION ANALYSIS:\")\n",
    "print(f\"1. Cost Function: {'Converged' if len(model_gd.cost_history) < 1000 else 'Reached max iterations'}\")\n",
    "print(f\"2. Prediction Quality: R¬≤ = {r2_gd:.4f} (closer to 1.0 is better)\")\n",
    "print(f\"3. Residuals: Should be randomly scattered around 0\")\n",
    "print(f\"4. Method Agreement: Both methods should give similar results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e50623",
   "metadata": {},
   "source": [
    "## 9. Scikit-learn Comparison\n",
    "\n",
    "Let's compare our implementation with the industry standard scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab135c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scikit-learn model for comparison\n",
    "print(\"üî¨ SCIKIT-LEARN COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sklearn_model = SklearnLR()\n",
    "sklearn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_sklearn = sklearn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn weights: {sklearn_model.coef_}\")\n",
    "print(f\"Scikit-learn bias: {sklearn_model.intercept_:.4f}\")\n",
    "print(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n",
    "print(f\"Scikit-learn R¬≤: {r2_sklearn:.4f}\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Our Gradient Descent', 'Our Analytical', 'Scikit-learn'],\n",
    "    'MSE': [mse_gd, mse_analytical, mse_sklearn],\n",
    "    'R¬≤': [r2_gd, r2_analytical, r2_sklearn],\n",
    "    'Weight 1': [model_gd.weights[0], model_analytical.weights[0], sklearn_model.coef_[0]],\n",
    "    'Weight 2': [model_gd.weights[1], model_analytical.weights[1], sklearn_model.coef_[1]],\n",
    "    'Bias': [model_gd.bias, model_analytical.bias, sklearn_model.intercept_]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä DETAILED COMPARISON:\")\n",
    "print(comparison_df.round(6))\n",
    "\n",
    "# Verify our implementation matches sklearn\n",
    "weight_diff = np.max(np.abs(model_analytical.weights - sklearn_model.coef_))\n",
    "bias_diff = abs(model_analytical.bias - sklearn_model.intercept_)\n",
    "\n",
    "print(f\"\\n‚úÖ VALIDATION:\")\n",
    "print(f\"Max weight difference with sklearn: {weight_diff:.8f}\")\n",
    "print(f\"Bias difference with sklearn: {bias_diff:.8f}\")\n",
    "print(f\"Implementation correct: {weight_diff < 1e-6 and bias_diff < 1e-6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e57c42",
   "metadata": {},
   "source": [
    "## 10. Interview Preparation Summary\n",
    "\n",
    "### üéØ Key Concepts Demonstrated\n",
    "\n",
    "This notebook covers all essential linear regression concepts for technical interviews:\n",
    "\n",
    "#### ‚úÖ **Mathematical Foundation**\n",
    "- **Linear Model**: Y = Xw + b\n",
    "- **MSE Loss**: MSE = (1/n) √ó Œ£(y·µ¢ - ≈∑·µ¢)¬≤\n",
    "- **Gradient Computation**: ‚àÇMSE/‚àÇw and ‚àÇMSE/‚àÇb\n",
    "- **Normal Equation**: Œ∏ = (X·µÄX)‚Åª¬πX·µÄy\n",
    "\n",
    "#### ‚úÖ **Implementation Skills**\n",
    "- From-scratch implementation without external ML libraries\n",
    "- Gradient descent optimization with convergence criteria\n",
    "- Analytical solution using matrix operations\n",
    "- Proper error handling and numerical stability\n",
    "\n",
    "#### ‚úÖ **Model Evaluation**\n",
    "- Mean Squared Error (MSE) for loss measurement\n",
    "- R¬≤ coefficient for explained variance\n",
    "- Residual analysis for model diagnostics\n",
    "- Comparison with industry standards\n",
    "\n",
    "#### ‚úÖ **Best Practices**\n",
    "- Feature standardization for gradient descent\n",
    "- Train/test split for unbiased evaluation\n",
    "- Convergence monitoring and early stopping\n",
    "- Numerical stability considerations\n",
    "\n",
    "### üöÄ **Interview Talking Points**\n",
    "\n",
    "1. **\"Why minimize MSE?\"**\n",
    "   - Convex loss function with unique global minimum\n",
    "   - Penalizes large errors more than small ones\n",
    "   - Mathematically tractable with closed-form solution\n",
    "\n",
    "2. **\"Gradient Descent vs Analytical Solution?\"**\n",
    "   - GD: Iterative, scales to large datasets, works with regularization\n",
    "   - Analytical: Direct solution, exact result, requires matrix inversion\n",
    "\n",
    "3. **\"When does gradient descent fail?\"**\n",
    "   - Poor learning rate choice (too large: divergence, too small: slow)\n",
    "   - Features not standardized\n",
    "   - Non-convex loss functions (not applicable here)\n",
    "\n",
    "4. **\"How to detect overfitting?\"**\n",
    "   - Training MSE << Test MSE\n",
    "   - Monitor validation loss during training\n",
    "   - Use regularization techniques (Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation - run a quick test to ensure everything works\n",
    "print(\"üéâ FINAL VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test our complete implementation\n",
    "test_model = LinearRegression(learning_rate=0.01, max_iterations=100)\n",
    "test_X = np.random.randn(50, 2)\n",
    "test_y = test_X.dot([1.5, -0.8]) + 0.5 + np.random.normal(0, 0.1, 50)\n",
    "\n",
    "# Test both methods\n",
    "test_model.fit_gradient_descent(test_X, test_y)\n",
    "print(f\"‚úÖ Gradient descent works: MSE = {test_model.mean_squared_error(test_y, test_model.predict(test_X)):.4f}\")\n",
    "\n",
    "test_model.fit_analytical(test_X, test_y)\n",
    "print(f\"‚úÖ Analytical solution works: MSE = {test_model.mean_squared_error(test_y, test_model.predict(test_X)):.4f}\")\n",
    "\n",
    "print(\"\\nüéØ INTERVIEW READINESS CHECKLIST:\")\n",
    "checklist = [\n",
    "    \"‚úÖ Can explain linear regression model equation\",\n",
    "    \"‚úÖ Can derive and implement MSE loss function\", \n",
    "    \"‚úÖ Can implement gradient descent from scratch\",\n",
    "    \"‚úÖ Can solve using Normal Equation\",\n",
    "    \"‚úÖ Can evaluate model performance with metrics\",\n",
    "    \"‚úÖ Can create visualizations for analysis\",\n",
    "    \"‚úÖ Can compare with industry standard implementations\",\n",
    "    \"‚úÖ Can discuss optimization trade-offs\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)\n",
    "\n",
    "print(f\"\\nüöÄ You're ready to demonstrate linear regression expertise in your interview!\")\n",
    "print(f\"üìö Study tip: Practice explaining each concept without looking at code first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
