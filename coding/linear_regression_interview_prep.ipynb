{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60043577",
   "metadata": {},
   "source": [
    "# Linear Regression Interview Preparation\n",
    "\n",
    "## Comprehensive Implementation and Evaluation Guide\n",
    "\n",
    "This notebook demonstrates a complete understanding of linear regression concepts for technical interviews, including:\n",
    "\n",
    "- **Mathematical Foundation**: Linear model Y = Xw + b and MSE loss function\n",
    "- **Implementation from Scratch**: Both gradient descent and analytical solutions\n",
    "- **Optimization Techniques**: Parameter updates and convergence criteria\n",
    "- **Model Evaluation**: MSE, R², and visualization techniques\n",
    "- **Industry Standards**: Comparison with scikit-learn implementation\n",
    "\n",
    "### Key Interview Topics Covered:\n",
    "1. ✅ Linear regression model formulation\n",
    "2. ✅ Mean Squared Error (MSE) derivation and calculation\n",
    "3. ✅ Gradient descent optimization algorithm\n",
    "4. ✅ Normal equation analytical solution\n",
    "5. ✅ Model evaluation metrics and interpretation\n",
    "6. ✅ Implementation best practices and debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ed72a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our comprehensive linear regression implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd0c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, Optional\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Ready for linear regression implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384913f6",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundation\n",
    "\n",
    "### Linear Regression Model\n",
    "The linear regression model is defined as:\n",
    "\n",
    "**Y = Xw + b**\n",
    "\n",
    "Where:\n",
    "- **Y**: Target values (n_samples,)\n",
    "- **X**: Feature matrix (n_samples, n_features)  \n",
    "- **w**: Weight vector (n_features,)\n",
    "- **b**: Bias term (scalar)\n",
    "\n",
    "### Mean Squared Error (MSE) Loss Function\n",
    "The cost function we want to minimize:\n",
    "\n",
    "**MSE = (1/n) × Σ(yᵢ - ŷᵢ)²**\n",
    "\n",
    "Where:\n",
    "- **n**: Number of samples\n",
    "- **yᵢ**: True target value for sample i\n",
    "- **ŷᵢ**: Predicted value for sample i\n",
    "\n",
    "### Gradient Descent Updates\n",
    "To minimize MSE, we compute gradients and update parameters:\n",
    "\n",
    "**∂MSE/∂w = -(2/n) × Xᵀ × (y - ŷ)**  \n",
    "**∂MSE/∂b = -(2/n) × Σ(y - ŷ)**\n",
    "\n",
    "**Parameter Updates:**  \n",
    "**w = w - α × ∂MSE/∂w**  \n",
    "**b = b - α × ∂MSE/∂b**\n",
    "\n",
    "Where **α** is the learning rate.\n",
    "\n",
    "### Normal Equation (Analytical Solution)\n",
    "For the optimal solution directly:\n",
    "\n",
    "**θ = (XᵀX)⁻¹Xᵀy**\n",
    "\n",
    "Where **θ = [b, w₁, w₂, ...]** contains bias and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e700c0f",
   "metadata": {},
   "source": [
    "## 3. LinearRegression Class Implementation\n",
    "\n",
    "Let's implement our linear regression class from scratch with both gradient descent and analytical solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f165dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation with gradient descent and analytical solutions.\n",
    "    \n",
    "    Model: Y = Xw + b\n",
    "    Loss: MSE = (1/n) * Σ(yi - ŷi)²\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000, tolerance: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Step size for gradient descent\n",
    "            max_iterations: Maximum number of training iterations\n",
    "            tolerance: Convergence threshold for early stopping\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions using the linear model: Y = Xw + b\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted values of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model has not been fitted yet\")\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def mean_squared_error(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Mean Squared Error: MSE = (1/n) * Σ(yi - ŷi)²\n",
    "        \n",
    "        Args:\n",
    "            y_true: True target values\n",
    "            y_pred: Predicted values\n",
    "            \n",
    "        Returns:\n",
    "            Mean squared error\n",
    "        \"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def r_squared(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate R² coefficient of determination\n",
    "        \n",
    "        R² = 1 - (SS_res / SS_tot)\n",
    "        where SS_res = Σ(yi - ŷi)² and SS_tot = Σ(yi - ȳ)²\n",
    "        \"\"\"\n",
    "        ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(\"LinearRegression class defined successfully!\")\n",
    "print(\"Key methods: fit_gradient_descent, fit_analytical, predict, mean_squared_error, r_squared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36f543",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent Training Method\n",
    "\n",
    "Now let's implement the gradient descent algorithm to optimize our model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gradient_descent(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "    \"\"\"\n",
    "    Fit the model using gradient descent optimization.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "        \n",
    "    Returns:\n",
    "        self: Fitted model\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    self.weights = np.random.normal(0, 0.01, n_features)\n",
    "    self.bias = 0\n",
    "    self.cost_history = []\n",
    "    \n",
    "    print(f\"Starting gradient descent with {n_samples} samples, {n_features} features\")\n",
    "    print(f\"Learning rate: {self.learning_rate}, Max iterations: {self.max_iterations}\")\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(self.max_iterations):\n",
    "        # Forward pass: Y = Xw + b\n",
    "        y_pred = self.predict(X)\n",
    "        \n",
    "        # Calculate MSE: (1/n) * Σ(yi - ŷi)²\n",
    "        mse = self.mean_squared_error(y, y_pred)\n",
    "        self.cost_history.append(mse)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        # ∂MSE/∂w = -(2/n) * X^T * (y - ŷ)\n",
    "        dw = (-2/n_samples) * X.T.dot(y - y_pred)\n",
    "        # ∂MSE/∂b = -(2/n) * Σ(y - ŷ)\n",
    "        db = (-2/n_samples) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update parameters\n",
    "        self.weights -= self.learning_rate * dw\n",
    "        self.bias -= self.learning_rate * db\n",
    "        \n",
    "        # Check for convergence\n",
    "        if i > 0 and abs(self.cost_history[-2] - self.cost_history[-1]) < self.tolerance:\n",
    "            print(f\"✅ Converged after {i+1} iterations\")\n",
    "            break\n",
    "            \n",
    "        # Print progress every 100 iterations\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Iteration {i+1}: MSE = {mse:.6f}\")\n",
    "            \n",
    "    return self\n",
    "\n",
    "# Add the method to our LinearRegression class\n",
    "LinearRegression.fit_gradient_descent = fit_gradient_descent\n",
    "print(\"✅ Gradient descent method added to LinearRegression class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f74e1a",
   "metadata": {},
   "source": [
    "## 5. Analytical Solution Method (Normal Equation)\n",
    "\n",
    "The analytical solution provides the optimal weights directly without iterative optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1fb2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_analytical(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':\n",
    "    \"\"\"\n",
    "    Fit the model using the analytical solution (Normal Equation).\n",
    "    \n",
    "    For Y = Xw + b, we solve: θ = (X^T X)^(-1) X^T y\n",
    "    where θ = [b, w1, w2, ...] contains bias and weights\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix of shape (n_samples, n_features)\n",
    "        y: Target vector of shape (n_samples,)\n",
    "        \n",
    "    Returns:\n",
    "        self: Fitted model\n",
    "    \"\"\"\n",
    "    print(\"Computing analytical solution using Normal Equation...\")\n",
    "    \n",
    "    # Add bias column to X (column of ones)\n",
    "    X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    # Normal equation: θ = (X^T X)^(-1) X^T y\n",
    "    # Using pseudoinverse for numerical stability\n",
    "    try:\n",
    "        theta = np.linalg.pinv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
    "        print(\"✅ Analytical solution computed successfully!\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"⚠️ Matrix inversion failed, using pseudoinverse\")\n",
    "        theta = np.linalg.pinv(X_with_bias).dot(y)\n",
    "    \n",
    "    # Extract bias and weights\n",
    "    self.bias = theta[0]\n",
    "    self.weights = theta[1:]\n",
    "    \n",
    "    print(f\"Optimal bias: {self.bias:.4f}\")\n",
    "    print(f\"Optimal weights: {self.weights}\")\n",
    "    \n",
    "    return self\n",
    "\n",
    "# Add the method to our LinearRegression class\n",
    "LinearRegression.fit_analytical = fit_analytical\n",
    "print(\"✅ Analytical solution method added to LinearRegression class!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7b032",
   "metadata": {},
   "source": [
    "## 6. Data Generation and Preprocessing\n",
    "\n",
    "Let's generate synthetic data with known parameters to test our implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627df7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_data(n_samples: int = 200, n_features: int = 2, noise_std: float = 0.5) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate sample data for regression demonstration with known parameters.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of data points to generate\n",
    "        n_features: Number of input features\n",
    "        noise_std: Standard deviation of Gaussian noise\n",
    "        \n",
    "    Returns:\n",
    "        X: Feature matrix, y: Target vector\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Generate random features\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    # Define true parameters (these are what we want our model to learn)\n",
    "    true_weights = np.array([3.5, -2.1])  # True weights\n",
    "    true_bias = 1.2                       # True bias\n",
    "    \n",
    "    # Generate targets: Y = Xw + b + noise\n",
    "    y = X.dot(true_weights) + true_bias + np.random.normal(0, noise_std, n_samples)\n",
    "    \n",
    "    return X, y, true_weights, true_bias\n",
    "\n",
    "# Generate our dataset\n",
    "print(\"🎯 Generating synthetic dataset...\")\n",
    "X, y, true_weights, true_bias = generate_sample_data(n_samples=200, noise_std=0.5)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias:.4f}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Standardize features (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"✅ Data generated and preprocessed successfully!\")\n",
    "print(f\"Feature means after scaling: {np.mean(X_train_scaled, axis=0)}\")\n",
    "print(f\"Feature stds after scaling: {np.std(X_train_scaled, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b1d46",
   "metadata": {},
   "source": [
    "## 7. Model Training and Evaluation\n",
    "\n",
    "Now let's train our models using both methods and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dae2274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train using Gradient Descent\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 TRAINING WITH GRADIENT DESCENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_gd = LinearRegression(learning_rate=0.01, max_iterations=1000, tolerance=1e-6)\n",
    "model_gd.fit_gradient_descent(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gd = model_gd.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_gd = model_gd.mean_squared_error(y_test, y_pred_gd)\n",
    "r2_gd = model_gd.r_squared(y_test, y_pred_gd)\n",
    "\n",
    "print(f\"\\n📊 GRADIENT DESCENT RESULTS:\")\n",
    "print(f\"Learned weights: {model_gd.weights}\")\n",
    "print(f\"Learned bias: {model_gd.bias:.4f}\")\n",
    "print(f\"Test MSE: {mse_gd:.4f}\")\n",
    "print(f\"Test R²: {r2_gd:.4f}\")\n",
    "print(f\"Training iterations: {len(model_gd.cost_history)}\")\n",
    "print(f\"Final cost: {model_gd.cost_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train using Analytical Solution\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"🎯 TRAINING WITH ANALYTICAL SOLUTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model_analytical = LinearRegression()\n",
    "model_analytical.fit_analytical(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_analytical = model_analytical.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_analytical = model_analytical.mean_squared_error(y_test, y_pred_analytical)\n",
    "r2_analytical = model_analytical.r_squared(y_test, y_pred_analytical)\n",
    "\n",
    "print(f\"\\n📊 ANALYTICAL SOLUTION RESULTS:\")\n",
    "print(f\"Optimal weights: {model_analytical.weights}\")\n",
    "print(f\"Optimal bias: {model_analytical.bias:.4f}\")\n",
    "print(f\"Test MSE: {mse_analytical:.4f}\")\n",
    "print(f\"Test R²: {r2_analytical:.4f}\")\n",
    "\n",
    "# Compare the two methods\n",
    "print(f\"\\n🔍 COMPARISON:\")\n",
    "print(f\"Weight difference: {np.abs(model_gd.weights - model_analytical.weights)}\")\n",
    "print(f\"Bias difference: {abs(model_gd.bias - model_analytical.bias):.6f}\")\n",
    "print(f\"MSE difference: {abs(mse_gd - mse_analytical):.6f}\")\n",
    "\n",
    "# Check how close we are to true parameters\n",
    "print(f\"\\n🎯 COMPARISON WITH TRUE PARAMETERS:\")\n",
    "print(f\"True weights: {true_weights}\")\n",
    "print(f\"True bias: {true_bias}\")\n",
    "print(f\"Note: Scaling affects the learned parameters, but predictions should be accurate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d35fa3",
   "metadata": {},
   "source": [
    "## 8. Visualization and Analysis\n",
    "\n",
    "Let's create comprehensive visualizations to understand our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Cost function convergence (Gradient Descent)\n",
    "axes[0, 0].plot(model_gd.cost_history, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Cost Function Convergence\\n(Gradient Descent)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('MSE')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_yscale('log')  # Log scale to see convergence better\n",
    "\n",
    "# Plot 2: Predictions vs Actual Values\n",
    "axes[0, 1].scatter(y_test, y_pred_gd, alpha=0.6, color='blue', label='Gradient Descent')\n",
    "axes[0, 1].scatter(y_test, y_pred_analytical, alpha=0.6, color='red', label='Analytical')\n",
    "# Perfect prediction line\n",
    "min_val, max_val = y_test.min(), y_test.max()\n",
    "axes[0, 1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Perfect Prediction')\n",
    "axes[0, 1].set_xlabel('Actual Values')\n",
    "axes[0, 1].set_ylabel('Predicted Values')\n",
    "axes[0, 1].set_title('Predictions vs Actual Values', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals (Gradient Descent)\n",
    "residuals_gd = y_test - y_pred_gd\n",
    "axes[1, 0].scatter(y_pred_gd, residuals_gd, alpha=0.6, color='blue')\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Values')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residual Plot\\n(Gradient Descent)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Learning curve comparison\n",
    "axes[1, 1].bar(['Gradient Descent', 'Analytical', 'True MSE'], \n",
    "               [mse_gd, mse_analytical, np.var(np.random.normal(0, 0.5, 1000))], \n",
    "               color=['blue', 'red', 'green'], alpha=0.7)\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title('Method Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed analysis\n",
    "print(\"📈 VISUALIZATION ANALYSIS:\")\n",
    "print(f\"1. Cost Function: {'Converged' if len(model_gd.cost_history) < 1000 else 'Reached max iterations'}\")\n",
    "print(f\"2. Prediction Quality: R² = {r2_gd:.4f} (closer to 1.0 is better)\")\n",
    "print(f\"3. Residuals: Should be randomly scattered around 0\")\n",
    "print(f\"4. Method Agreement: Both methods should give similar results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e50623",
   "metadata": {},
   "source": [
    "## 9. Scikit-learn Comparison\n",
    "\n",
    "Let's compare our implementation with the industry standard scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab135c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train scikit-learn model for comparison\n",
    "print(\"🔬 SCIKIT-LEARN COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "sklearn_model = SklearnLR()\n",
    "sklearn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_sklearn = sklearn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"Scikit-learn weights: {sklearn_model.coef_}\")\n",
    "print(f\"Scikit-learn bias: {sklearn_model.intercept_:.4f}\")\n",
    "print(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n",
    "print(f\"Scikit-learn R²: {r2_sklearn:.4f}\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Our Gradient Descent', 'Our Analytical', 'Scikit-learn'],\n",
    "    'MSE': [mse_gd, mse_analytical, mse_sklearn],\n",
    "    'R²': [r2_gd, r2_analytical, r2_sklearn],\n",
    "    'Weight 1': [model_gd.weights[0], model_analytical.weights[0], sklearn_model.coef_[0]],\n",
    "    'Weight 2': [model_gd.weights[1], model_analytical.weights[1], sklearn_model.coef_[1]],\n",
    "    'Bias': [model_gd.bias, model_analytical.bias, sklearn_model.intercept_]\n",
    "})\n",
    "\n",
    "print(\"\\n📊 DETAILED COMPARISON:\")\n",
    "print(comparison_df.round(6))\n",
    "\n",
    "# Verify our implementation matches sklearn\n",
    "weight_diff = np.max(np.abs(model_analytical.weights - sklearn_model.coef_))\n",
    "bias_diff = abs(model_analytical.bias - sklearn_model.intercept_)\n",
    "\n",
    "print(f\"\\n✅ VALIDATION:\")\n",
    "print(f\"Max weight difference with sklearn: {weight_diff:.8f}\")\n",
    "print(f\"Bias difference with sklearn: {bias_diff:.8f}\")\n",
    "print(f\"Implementation correct: {weight_diff < 1e-6 and bias_diff < 1e-6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e57c42",
   "metadata": {},
   "source": [
    "## 10. Interview Preparation Summary\n",
    "\n",
    "### 🎯 Key Concepts Demonstrated\n",
    "\n",
    "This notebook covers all essential linear regression concepts for technical interviews:\n",
    "\n",
    "#### ✅ **Mathematical Foundation**\n",
    "- **Linear Model**: Y = Xw + b\n",
    "- **MSE Loss**: MSE = (1/n) × Σ(yᵢ - ŷᵢ)²\n",
    "- **Gradient Computation**: ∂MSE/∂w and ∂MSE/∂b\n",
    "- **Normal Equation**: θ = (XᵀX)⁻¹Xᵀy\n",
    "\n",
    "#### ✅ **Implementation Skills**\n",
    "- From-scratch implementation without external ML libraries\n",
    "- Gradient descent optimization with convergence criteria\n",
    "- Analytical solution using matrix operations\n",
    "- Proper error handling and numerical stability\n",
    "\n",
    "#### ✅ **Model Evaluation**\n",
    "- Mean Squared Error (MSE) for loss measurement\n",
    "- R² coefficient for explained variance\n",
    "- Residual analysis for model diagnostics\n",
    "- Comparison with industry standards\n",
    "\n",
    "#### ✅ **Best Practices**\n",
    "- Feature standardization for gradient descent\n",
    "- Train/test split for unbiased evaluation\n",
    "- Convergence monitoring and early stopping\n",
    "- Numerical stability considerations\n",
    "\n",
    "### 🚀 **Interview Talking Points**\n",
    "\n",
    "1. **\"Why minimize MSE?\"**\n",
    "   - Convex loss function with unique global minimum\n",
    "   - Penalizes large errors more than small ones\n",
    "   - Mathematically tractable with closed-form solution\n",
    "\n",
    "2. **\"Gradient Descent vs Analytical Solution?\"**\n",
    "   - GD: Iterative, scales to large datasets, works with regularization\n",
    "   - Analytical: Direct solution, exact result, requires matrix inversion\n",
    "\n",
    "3. **\"When does gradient descent fail?\"**\n",
    "   - Poor learning rate choice (too large: divergence, too small: slow)\n",
    "   - Features not standardized\n",
    "   - Non-convex loss functions (not applicable here)\n",
    "\n",
    "4. **\"How to detect overfitting?\"**\n",
    "   - Training MSE << Test MSE\n",
    "   - Monitor validation loss during training\n",
    "   - Use regularization techniques (Ridge, Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final validation - run a quick test to ensure everything works\n",
    "print(\"🎉 FINAL VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test our complete implementation\n",
    "test_model = LinearRegression(learning_rate=0.01, max_iterations=100)\n",
    "test_X = np.random.randn(50, 2)\n",
    "test_y = test_X.dot([1.5, -0.8]) + 0.5 + np.random.normal(0, 0.1, 50)\n",
    "\n",
    "# Test both methods\n",
    "test_model.fit_gradient_descent(test_X, test_y)\n",
    "print(f\"✅ Gradient descent works: MSE = {test_model.mean_squared_error(test_y, test_model.predict(test_X)):.4f}\")\n",
    "\n",
    "test_model.fit_analytical(test_X, test_y)\n",
    "print(f\"✅ Analytical solution works: MSE = {test_model.mean_squared_error(test_y, test_model.predict(test_X)):.4f}\")\n",
    "\n",
    "print(\"\\n🎯 INTERVIEW READINESS CHECKLIST:\")\n",
    "checklist = [\n",
    "    \"✅ Can explain linear regression model equation\",\n",
    "    \"✅ Can derive and implement MSE loss function\", \n",
    "    \"✅ Can implement gradient descent from scratch\",\n",
    "    \"✅ Can solve using Normal Equation\",\n",
    "    \"✅ Can evaluate model performance with metrics\",\n",
    "    \"✅ Can create visualizations for analysis\",\n",
    "    \"✅ Can compare with industry standard implementations\",\n",
    "    \"✅ Can discuss optimization trade-offs\"\n",
    "]\n",
    "\n",
    "for item in checklist:\n",
    "    print(item)\n",
    "\n",
    "print(f\"\\n🚀 You're ready to demonstrate linear regression expertise in your interview!\")\n",
    "print(f\"📚 Study tip: Practice explaining each concept without looking at code first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
