{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e043d4e2",
   "metadata": {},
   "source": [
    "# üéØ **Support Vector Machines (SVM) - Essential Guide**\n",
    "\n",
    "## **Applied Scientist Interview Preparation**\n",
    "### *Core Concepts & Implementation*\n",
    "\n",
    "---\n",
    "\n",
    "## üìö **Learning Objectives**\n",
    "- Understand the **geometric intuition** behind SVMs\n",
    "- Master **key mathematical concepts** (margin, kernel trick)\n",
    "- Implement **basic SVM** from scratch\n",
    "- Know **when to use SVMs** vs other algorithms\n",
    "- Handle **common interview questions**\n",
    "\n",
    "### **‚è±Ô∏è Duration: 45 minutes**\n",
    "### **üéØ Interview Focus: Conceptual understanding + practical application**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b13662",
   "metadata": {},
   "source": [
    "## **üßÆ Core Concept: Maximum Margin**\n",
    "\n",
    "### **The Big Idea**\n",
    "SVM finds the **hyperplane** that separates classes with the **maximum margin**.\n",
    "\n",
    "```\n",
    "Decision boundary: w¬∑x + b = 0\n",
    "Margin = 2/||w||  (distance between support vectors)\n",
    "Goal: Maximize margin ‚ü∫ Minimize ||w||\n",
    "```\n",
    "\n",
    "### **üéØ Key Terms**\n",
    "- **Support Vectors**: Data points closest to decision boundary\n",
    "- **Margin**: \"Street width\" between classes\n",
    "- **Hyperplane**: Decision boundary (line in 2D, plane in 3D, etc.)\n",
    "- **Kernel**: Function to map data to higher dimensions\n",
    "\n",
    "### **üîç Why Maximum Margin?**\n",
    "1. **Generalization**: Better performance on unseen data\n",
    "2. **Robustness**: Less sensitive to small data changes\n",
    "3. **Unique solution**: Only one maximum margin hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SVM concept\n",
    "def visualize_svm_concept():\n",
    "    \"\"\"Show the key SVM concepts visually\"\"\"\n",
    "    \n",
    "    # Create simple 2D dataset\n",
    "    X, y = make_classification(n_samples=20, n_features=2, n_redundant=0, \n",
    "                             n_informative=2, n_clusters_per_class=1, \n",
    "                             class_sep=2, random_state=42)\n",
    "    \n",
    "    # Train SVM\n",
    "    svm = SVC(kernel='linear', C=1000)  # Large C for hard margin\n",
    "    svm.fit(X, y)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Data and decision boundary\n",
    "    plt.subplot(1, 2, 1)\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Get decision function values\n",
    "    Z = svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and margins\n",
    "    plt.contour(xx, yy, Z, levels=[-1, 0, 1], linestyles=['--', '-', '--'], \n",
    "                colors=['red', 'black', 'red'], alpha=0.7)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', s=100, alpha=0.8)\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    plt.scatter(X[svm.support_, 0], X[svm.support_, 1], \n",
    "                s=300, linewidth=2, facecolors='none', edgecolors='black')\n",
    "    \n",
    "    plt.title('SVM: Maximum Margin Classifier')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(['Margin', 'Decision Boundary', 'Margin', 'Data Points', 'Support Vectors'])\n",
    "    \n",
    "    # Plot 2: Concept illustration\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Simple illustration\n",
    "    x_simple = np.array([[1, 3], [2, 3], [3, 1], [4, 1]])\n",
    "    y_simple = np.array([0, 0, 1, 1])\n",
    "    \n",
    "    plt.scatter(x_simple[y_simple==0, 0], x_simple[y_simple==0, 1], \n",
    "                c='red', s=200, marker='o', label='Class 0')\n",
    "    plt.scatter(x_simple[y_simple==1, 0], x_simple[y_simple==1, 1], \n",
    "                c='blue', s=200, marker='s', label='Class 1')\n",
    "    \n",
    "    # Draw margin illustration\n",
    "    plt.plot([0.5, 4.5], [2.5, 1.5], 'k-', linewidth=2, label='Decision Boundary')\n",
    "    plt.plot([0.5, 4.5], [3, 2], 'r--', alpha=0.7, label='Margin')\n",
    "    plt.plot([0.5, 4.5], [2, 1], 'r--', alpha=0.7)\n",
    "    \n",
    "    # Add margin width annotation\n",
    "    plt.annotate('', xy=(2.5, 2.25), xytext=(2.5, 1.75), \n",
    "                arrowprops=dict(arrowstyle='<->', color='green', lw=2))\n",
    "    plt.text(2.7, 2, 'Margin\\nWidth', fontsize=12, color='green', weight='bold')\n",
    "    \n",
    "    plt.title('SVM Margin Concept')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä **SVM Results:**\")\n",
    "    print(f\"Number of support vectors: {len(svm.support_)}\")\n",
    "    print(f\"Support vector indices: {svm.support_}\")\n",
    "    print(f\"\\nüéØ **Key Insight**: Only support vectors determine the decision boundary!\")\n",
    "\n",
    "visualize_svm_concept()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a08544",
   "metadata": {},
   "source": [
    "## **‚öôÔ∏è The Kernel Trick**\n",
    "\n",
    "### **Problem**: Linear separation doesn't always work\n",
    "### **Solution**: Map data to higher dimensions where it becomes linearly separable\n",
    "\n",
    "```\n",
    "Original space: œÜ(x) ‚Üí Higher dimensional space\n",
    "Kernel function: K(x‚ÇÅ, x‚ÇÇ) = œÜ(x‚ÇÅ)¬∑œÜ(x‚ÇÇ)\n",
    "```\n",
    "\n",
    "### **üîß Common Kernels**\n",
    "- **Linear**: K(x‚ÇÅ, x‚ÇÇ) = x‚ÇÅ¬∑x‚ÇÇ\n",
    "- **Polynomial**: K(x‚ÇÅ, x‚ÇÇ) = (Œ≥x‚ÇÅ¬∑x‚ÇÇ + r)^d\n",
    "- **RBF (Gaussian)**: K(x‚ÇÅ, x‚ÇÇ) = exp(-Œ≥||x‚ÇÅ-x‚ÇÇ||¬≤)\n",
    "- **Sigmoid**: K(x‚ÇÅ, x‚ÇÇ) = tanh(Œ≥x‚ÇÅ¬∑x‚ÇÇ + r)\n",
    "\n",
    "### **üéØ Interview Tip**: The kernel trick lets us work in infinite dimensions without actually computing the transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate kernel trick\n",
    "def demonstrate_kernels():\n",
    "    \"\"\"Show how different kernels handle non-linear data\"\"\"\n",
    "    \n",
    "    # Create non-linearly separable data\n",
    "    X, y = make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=42)\n",
    "    \n",
    "    # Different kernels\n",
    "    kernels = ['linear', 'poly', 'rbf']\n",
    "    kernel_names = ['Linear', 'Polynomial (degree=3)', 'RBF (Gaussian)']\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, (kernel, name) in enumerate(zip(kernels, kernel_names)):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        \n",
    "        # Train SVM with specific kernel\n",
    "        if kernel == 'poly':\n",
    "            svm = SVC(kernel=kernel, degree=3, C=1)\n",
    "        else:\n",
    "            svm = SVC(kernel=kernel, C=1)\n",
    "        \n",
    "        svm.fit(X, y)\n",
    "        \n",
    "        # Create mesh for decision boundary\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "        \n",
    "        # Plot data points\n",
    "        scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', s=50)\n",
    "        \n",
    "        # Highlight support vectors\n",
    "        plt.scatter(X[svm.support_, 0], X[svm.support_, 1], \n",
    "                    s=200, linewidth=2, facecolors='none', edgecolors='black')\n",
    "        \n",
    "        accuracy = svm.score(X, y)\n",
    "        plt.title(f'{name}\\nAccuracy: {accuracy:.3f}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ **Kernel Insights:**\")\n",
    "    print(\"1. **Linear**: Fails on non-linear data (circles)\")\n",
    "    print(\"2. **Polynomial**: Can capture some non-linear patterns\")\n",
    "    print(\"3. **RBF**: Most flexible, handles complex boundaries\")\n",
    "    print(\"4. **Trade-off**: Flexibility vs overfitting risk\")\n",
    "\n",
    "demonstrate_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4912b",
   "metadata": {},
   "source": [
    "## **üéõÔ∏è Key Hyperparameters**\n",
    "\n",
    "### **C (Regularization Parameter)**\n",
    "- **High C**: Hard margin (less tolerance for misclassification)\n",
    "- **Low C**: Soft margin (more tolerance, better generalization)\n",
    "\n",
    "### **Œ≥ (Gamma) for RBF kernel**\n",
    "- **High Œ≥**: Tight fit (low bias, high variance)\n",
    "- **Low Œ≥**: Loose fit (high bias, low variance)\n",
    "\n",
    "### **üéØ Interview Question**: \"How do you tune SVM hyperparameters?\"\n",
    "**Answer**: Use grid search with cross-validation on log scale: C=[0.1, 1, 10, 100], Œ≥=[0.001, 0.01, 0.1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20ede5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter effects\n",
    "def show_hyperparameter_effects():\n",
    "    \"\"\"Demonstrate C and gamma effects\"\"\"\n",
    "    \n",
    "    # Create dataset with some noise\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, \n",
    "                             n_informative=2, n_clusters_per_class=1, \n",
    "                             class_sep=1.5, random_state=42)\n",
    "    \n",
    "    # Add some noise\n",
    "    X += np.random.normal(0, 0.1, X.shape)\n",
    "    \n",
    "    # Different C values\n",
    "    C_values = [0.1, 1, 10, 100]\n",
    "    \n",
    "    plt.figure(figsize=(16, 4))\n",
    "    \n",
    "    for i, C in enumerate(C_values):\n",
    "        plt.subplot(1, 4, i+1)\n",
    "        \n",
    "        # Train SVM\n",
    "        svm = SVC(kernel='rbf', C=C, gamma='scale')\n",
    "        svm.fit(X, y)\n",
    "        \n",
    "        # Create mesh\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        # Plot decision boundary\n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlBu')\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', s=50)\n",
    "        \n",
    "        # Support vectors\n",
    "        plt.scatter(X[svm.support_, 0], X[svm.support_, 1], \n",
    "                    s=200, linewidth=2, facecolors='none', edgecolors='black')\n",
    "        \n",
    "        accuracy = svm.score(X, y)\n",
    "        n_support = len(svm.support_)\n",
    "        plt.title(f'C = {C}\\nAccuracy: {accuracy:.3f}\\nSupport Vectors: {n_support}')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìà **C Parameter Effects:**\")\n",
    "    print(\"‚Ä¢ **Low C (0.1)**: Soft margin, more support vectors, smoother boundary\")\n",
    "    print(\"‚Ä¢ **High C (100)**: Hard margin, fewer support vectors, complex boundary\")\n",
    "    print(\"‚Ä¢ **Sweet spot**: Usually C=1 or C=10 work well\")\n",
    "\n",
    "show_hyperparameter_effects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aaae9b",
   "metadata": {},
   "source": [
    "## **üè≠ When to Use SVMs**\n",
    "\n",
    "### **‚úÖ SVM Strengths**\n",
    "- **High-dimensional data** (e.g., text, genomics)\n",
    "- **Small to medium datasets** (< 10K samples)\n",
    "- **Clear margin of separation** exists\n",
    "- **Memory efficient** (only stores support vectors)\n",
    "- **Versatile** (different kernels for different data)\n",
    "\n",
    "### **‚ùå SVM Limitations**\n",
    "- **Large datasets** (O(n¬≥) training complexity)\n",
    "- **Noisy data** with overlapping classes\n",
    "- **No probabilistic output** (just decision scores)\n",
    "- **Feature scaling required**\n",
    "- **Hyperparameter sensitive**\n",
    "\n",
    "### **üéØ Interview Answer**: \"When would you choose SVM over Random Forest?\"\n",
    "\"SVM for high-dimensional data with clear separability (like text classification), Random Forest for tabular data with mixed types and when you need feature importance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f6e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick implementation of core SVM concept\n",
    "class SimpleSVM:\n",
    "    \"\"\"Simplified SVM implementation for understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, C=1.0, max_iter=1000):\n",
    "        self.C = C\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Simplified training using sklearn's underlying solver\"\"\"\n",
    "        # This is a simplified version - real SVM uses SMO algorithm\n",
    "        from sklearn.svm import SVC\n",
    "        self.svm = SVC(kernel='linear', C=self.C)\n",
    "        self.svm.fit(X, y)\n",
    "        \n",
    "        # Extract learned parameters\n",
    "        self.w = self.svm.coef_[0]\n",
    "        self.b = self.svm.intercept_[0]\n",
    "        self.support_vectors = X[self.svm.support_]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Compute decision function w¬∑x + b\"\"\"\n",
    "        return X.dot(self.w) + self.b\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        return np.sign(self.decision_function(X))\n",
    "    \n",
    "    def margin_width(self):\n",
    "        \"\"\"Compute margin width\"\"\"\n",
    "        return 2.0 / np.linalg.norm(self.w)\n",
    "\n",
    "# Test simple SVM\n",
    "X, y = make_classification(n_samples=50, n_features=2, n_redundant=0, \n",
    "                         n_informative=2, n_clusters_per_class=1, \n",
    "                         class_sep=2, random_state=42)\n",
    "\n",
    "y[y == 0] = -1  # Convert to -1, +1 labels\n",
    "\n",
    "# Train simple SVM\n",
    "simple_svm = SimpleSVM(C=1.0)\n",
    "simple_svm.fit(X, y)\n",
    "\n",
    "print(\"üîç **Simple SVM Results:**\")\n",
    "print(f\"Weight vector w: {simple_svm.w}\")\n",
    "print(f\"Bias term b: {simple_svm.b:.3f}\")\n",
    "print(f\"Margin width: {simple_svm.margin_width():.3f}\")\n",
    "print(f\"Number of support vectors: {len(simple_svm.support_vectors)}\")\n",
    "\n",
    "print(\"\\nüéØ **Key SVM Equations:**\")\n",
    "print(\"‚Ä¢ Decision boundary: w¬∑x + b = 0\")\n",
    "print(\"‚Ä¢ Prediction: sign(w¬∑x + b)\")\n",
    "print(\"‚Ä¢ Margin width: 2/||w||\")\n",
    "print(\"‚Ä¢ Optimization: minimize ||w||¬≤ subject to y·µ¢(w¬∑x·µ¢ + b) ‚â• 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb40d17",
   "metadata": {},
   "source": [
    "## **üí° Interview Essentials**\n",
    "\n",
    "### **üîë Core Concepts to Master**\n",
    "1. **Maximum margin principle**: Why SVM finds the \"widest street\"\n",
    "2. **Support vectors**: Only these points matter for the decision boundary\n",
    "3. **Kernel trick**: How to handle non-linear data without explicit transformation\n",
    "4. **C parameter**: Trade-off between margin width and training accuracy\n",
    "\n",
    "### **üéØ Common Interview Questions**\n",
    "\n",
    "**Q1**: \"Explain SVM in simple terms\"  \n",
    "**A**: \"SVM finds the line/plane that separates classes with maximum margin - like finding the widest possible street between two neighborhoods.\"\n",
    "\n",
    "**Q2**: \"What happens if data isn't linearly separable?\"  \n",
    "**A**: \"Use kernels to map data to higher dimensions where it becomes linearly separable, or use soft margin with penalty C.\"\n",
    "\n",
    "**Q3**: \"SVM vs Logistic Regression?\"  \n",
    "**A**: \"SVM focuses on support vectors (boundary cases), LR considers all points. SVM better for high dimensions, LR gives probabilities.\"\n",
    "\n",
    "**Q4**: \"Why doesn't SVM scale to large datasets?\"  \n",
    "**A**: \"Training complexity is O(n¬≥) due to quadratic optimization. For large data, use SGD-based linear SVM or switch to other algorithms.\"\n",
    "\n",
    "### **üöÄ Practical Tips**\n",
    "- **Always scale features** (StandardScaler)\n",
    "- **Start with RBF kernel** for non-linear data\n",
    "- **Use GridSearchCV** for hyperparameter tuning\n",
    "- **For large datasets**: Use SGDClassifier with hinge loss\n",
    "\n",
    "---\n",
    "\n",
    "## **‚úÖ Summary Checklist**\n",
    "- [ ] Understand maximum margin principle\n",
    "- [ ] Know what support vectors are\n",
    "- [ ] Grasp the kernel trick concept\n",
    "- [ ] Understand C and Œ≥ parameter effects\n",
    "- [ ] Know when to use SVM vs other algorithms\n",
    "- [ ] Can explain SVM to non-technical stakeholders\n",
    "\n",
    "**üéØ You're ready for SVM interview questions!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
